

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2019 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2019 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/mixed_precision.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2019-The-TensorFlow-Authors.">
<h1>Copyright 2019 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2019-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Mixed-precision">
<h2>Mixed precision<a class="headerlink" href="#Mixed-precision" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>216fb8a684674524bab808605a827cb2|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>16e0e62a17b9455da3b841674e6eef7b|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>1ed818100a2b4e6ca80f5ce5fa455b41|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>fdfccf8b19ee488c8710a4f6cd087e7a|Download notebook</p>
</td></table><div class="section" id="Overview">
<h3>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. By keeping certain parts of the model in the 32-bit types for numeric stability, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy. This guide describes how to use the Keras mixed precision API to speed up your models. Using this API can improve performance by more than 3 times on modern
GPUs and 60% on TPUs.</p>
<p>Today, most models use the float32 dtype, which takes 32 bits of memory. However, there are two lower-precision dtypes, float16 and bfloat16, each which take 16 bits of memory instead. Modern accelerators can run operations faster in the 16-bit dtypes, as they have specialized hardware to run 16-bit computations and 16-bit dtypes can be read from memory faster.</p>
<p>NVIDIA GPUs can run operations in float16 faster than in float32, and TPUs can run operations in bfloat16 faster than float32. Therefore, these lower-precision dtypes should be used whenever possible on those devices. However, variables and a few computations should still be in float32 for numeric reasons so that the model trains to the same quality. The Keras mixed precision API allows you to use a mix of either float16 or bfloat16 with float32, to get the performance benefits from
float16/bfloat16 and the numeric stability benefits from float32.</p>
<p>Note: In this guide, the term “numeric stability” refers to how a model’s quality is affected by the use of a lower-precision dtype instead of a higher precision dtype. We say an operation is “numerically unstable” in float16 or bfloat16 if running it in one of those dtypes causes the model to have worse evaluation accuracy or other metrics compared to running the operation in float32.</p>
</div>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import mixed_precision
</pre></div>
</div>
</div>
</div>
<div class="section" id="Supported-hardware">
<h3>Supported hardware<a class="headerlink" href="#Supported-hardware" title="Enlazar permanentemente con este título">¶</a></h3>
<p>While mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs and Cloud TPUs. NVIDIA GPUs support using a mix of float16 and float32, while TPUs support a mix of bfloat16 and float32.</p>
<p>Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. Older GPUs offer no math performance benefit for using mixed precision, however memory and bandwidth savings can enable some speedups. You can look up the compute capability for your GPU at NVIDIA’s <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">CUDA GPU web
page</a>. Examples of GPUs that will benefit most from mixed precision include RTX GPUs, the V100, and the A100.</p>
<p>Note: If running this guide in Google Colab, the GPU runtime typically has a P100 connected. The P100 has compute capability 6.0 and is not expected to show a significant speedup.</p>
<p>You can check your GPU type with the following. The command only exists if the NVIDIA drivers are installed, so the following will raise an error otherwise.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!nvidia-smi -L
</pre></div>
</div>
</div>
<p>All Cloud TPUs support bfloat16.</p>
<p>Even on CPUs and older GPUs, where no speedup is expected, mixed precision APIs can still be used for unit testing, debugging, or just to try out the API. On CPUs, mixed precision will run significantly slower, however.</p>
</div>
<div class="section" id="Setting-the-dtype-policy">
<h3>Setting the dtype policy<a class="headerlink" href="#Setting-the-dtype-policy" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To use mixed precision in Keras, you need to create a <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision.Policy</span></code>, typically referred to as a <em>dtype policy</em>. Dtype policies specify the dtypes layers will run in. In this guide, you will construct a policy from the string <code class="docutils literal notranslate"><span class="pre">'mixed_float16'</span></code> and set it as the global policy. This will cause subsequently created layers to use mixed precision with a mix of float16 and float32.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>policy = mixed_precision.Policy(&#39;mixed_float16&#39;)
mixed_precision.set_global_policy(policy)
</pre></div>
</div>
</div>
<p>For short, you can directly pass a string to <code class="docutils literal notranslate"><span class="pre">set_global_policy</span></code>, which is typically done in practice.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Equivalent to the two lines above
mixed_precision.set_global_policy(&#39;mixed_float16&#39;)
</pre></div>
</div>
</div>
<p>The policy specifies two important aspects of a layer: the dtype the layer’s computations are done in, and the dtype of a layer’s variables. Above, you created a <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> policy (i.e., a <code class="docutils literal notranslate"><span class="pre">mixed_precision.Policy</span></code> created by passing the string <code class="docutils literal notranslate"><span class="pre">'mixed_float16'</span></code> to its constructor). With this policy, layers use float16 computations and float32 variables. Computations are done in float16 for performance, but variables must be kept in float32 for numeric stability. You can directly query
these properties of the policy.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&#39;Compute dtype: %s&#39; % policy.compute_dtype)
print(&#39;Variable dtype: %s&#39; % policy.variable_dtype)
</pre></div>
</div>
</div>
<p>As mentioned before, the <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> policy will most significantly improve performance on NVIDIA GPUs with compute capability of at least 7.0. The policy will run on other GPUs and CPUs but may not improve performance. For TPUs, the <code class="docutils literal notranslate"><span class="pre">mixed_bfloat16</span></code> policy should be used instead.</p>
</div>
<div class="section" id="Building-the-model">
<h3>Building the model<a class="headerlink" href="#Building-the-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next, let’s start building a simple model. Very small toy models typically do not benefit from mixed precision, because overhead from the TensorFlow runtime typically dominates the execution time, making any performance improvement on the GPU negligible. Therefore, let’s build two large <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers with 4096 units each if a GPU is used.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>inputs = keras.Input(shape=(784,), name=&#39;digits&#39;)
if tf.config.list_physical_devices(&#39;GPU&#39;):
  print(&#39;The model will run with 4096 units on a GPU&#39;)
  num_units = 4096
else:
  # Use fewer units on CPUs so the model finishes in a reasonable amount of time
  print(&#39;The model will run with 64 units on a CPU&#39;)
  num_units = 64
dense1 = layers.Dense(num_units, activation=&#39;relu&#39;, name=&#39;dense_1&#39;)
x = dense1(inputs)
dense2 = layers.Dense(num_units, activation=&#39;relu&#39;, name=&#39;dense_2&#39;)
x = dense2(x)
</pre></div>
</div>
</div>
<p>Each layer has a policy and uses the global policy by default. Each of the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers therefore have the <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> policy because you set the global policy to <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> previously. This will cause the dense layers to do float16 computations and have float32 variables. They cast their inputs to float16 in order to do float16 computations, which causes their outputs to be float16 as a result. Their variables are float32 and will be cast to float16 when the layers are called
to avoid errors from dtype mismatches.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(dense1.dtype_policy)
print(&#39;x.dtype: %s&#39; % x.dtype.name)
# &#39;kernel&#39; is dense1&#39;s variable
print(&#39;dense1.kernel.dtype: %s&#39; % dense1.kernel.dtype.name)
</pre></div>
</div>
</div>
<p>Next, create the output predictions. Normally, you can create the output predictions as follows, but this is not always numerically stable with float16.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># INCORRECT: softmax and model output will be float16, when it should be float32
outputs = layers.Dense(10, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x)
print(&#39;Outputs dtype: %s&#39; % outputs.dtype.name)
</pre></div>
</div>
</div>
<p>A softmax activation at the end of the model should be float32. Because the dtype policy is <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code>, the softmax activation would normally have a float16 compute dtype and output a float16 tensors.</p>
<p>This can be fixed by separating the Dense and softmax layers, and by passing <code class="docutils literal notranslate"><span class="pre">dtype='float32'</span></code> to the softmax layer</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># CORRECT: softmax and model output are float32
x = layers.Dense(10, name=&#39;dense_logits&#39;)(x)
outputs = layers.Activation(&#39;softmax&#39;, dtype=&#39;float32&#39;, name=&#39;predictions&#39;)(x)
print(&#39;Outputs dtype: %s&#39; % outputs.dtype.name)
</pre></div>
</div>
</div>
<p>Passing <code class="docutils literal notranslate"><span class="pre">dtype='float32'</span></code> to the softmax layer constructor overrides the layer’s dtype policy to be the <code class="docutils literal notranslate"><span class="pre">float32</span></code> policy, which does computations and keeps variables in float32. Equivalently, we could have instead passed <code class="docutils literal notranslate"><span class="pre">dtype=mixed_precision.Policy('float32')</span></code>; layers always convert the dtype argument to a policy. Because the <code class="docutils literal notranslate"><span class="pre">Activation</span></code> layer has no variables, the policy’s variable dtype is ignored, but the policy’s compute dtype of float32 causes softmax and the model output to be
float32.</p>
<p>Adding a float16 softmax in the middle of a model is fine, but a softmax at the end of the model should be in float32. The reason is that if the intermediate tensor flowing from the softmax to the loss is float16 or bfloat16, numeric issues may occur.</p>
<p>You can override the dtype of any layer to be float32 by passing <code class="docutils literal notranslate"><span class="pre">dtype='float32'</span></code> if you think it will not be numerically stable with float16 computations. But typically, this is only necessary on the last layer of the model, as most layers have sufficient precision with <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> and <code class="docutils literal notranslate"><span class="pre">mixed_bfloat16</span></code>.</p>
<p>Even if the model does not end in a softmax, the outputs should still be float32. While unnecessary for this specific model, the model outputs can be cast to float32 with the following:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># The linear activation is an identity function. So this simply casts &#39;outputs&#39;
# to float32. In this particular case, &#39;outputs&#39; is already float32 so this is a
# no-op.
outputs = layers.Activation(&#39;linear&#39;, dtype=&#39;float32&#39;)(outputs)
</pre></div>
</div>
</div>
<p>Next, finish and compile the model, and generate input data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,
              optimizer=keras.optimizers.RMSprop(),
              metrics=[&#39;accuracy&#39;])

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype(&#39;float32&#39;) / 255
x_test = x_test.reshape(10000, 784).astype(&#39;float32&#39;) / 255
</pre></div>
</div>
</div>
<p>This example cast the input data from int8 to float32. We don’t cast to float16 since the division by 255 is on the CPU, which runs float16 operations slower than float32 operations. In this case, the performance difference in negligible, but in general you should run input processing math in float32 if it runs on the CPU. The first layer of the model will cast the inputs to float16, as each layer casts floating-point inputs to its compute dtype.</p>
<p>The initial weights of the model are retrieved. This will allow training from scratch again by loading the weights.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>initial_weights = model.get_weights()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training-the-model-with-Model.fit">
<h3>Training the model with Model.fit<a class="headerlink" href="#Training-the-model-with-Model.fit" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next, train the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>history = model.fit(x_train, y_train,
                    batch_size=8192,
                    epochs=5,
                    validation_split=0.2)
test_scores = model.evaluate(x_test, y_test, verbose=2)
print(&#39;Test loss:&#39;, test_scores[0])
print(&#39;Test accuracy:&#39;, test_scores[1])

</pre></div>
</div>
</div>
<p>Notice the model prints the time per step in the logs: for example, “25ms/step”. The first epoch may be slower as TensorFlow spends some time optimizing the model, but afterwards the time per step should stabilize.</p>
<p>If you are running this guide in Colab, you can compare the performance of mixed precision with float32. To do so, change the policy from <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> to <code class="docutils literal notranslate"><span class="pre">float32</span></code> in the “Setting the dtype policy” section, then rerun all the cells up to this point. On GPUs with compute capability 7.X, you should see the time per step significantly increase, indicating mixed precision sped up the model. Make sure to change the policy back to <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> and rerun the cells before continuing with the
guide.</p>
<p>On GPUs with compute capability of at least 8.0 (Ampere GPUs and above), you likely will see no performance improvement in the toy model in this guide when using mixed precision compared to float32. This is due to the use of <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_tensor_float_32_execution">TensorFloat-32</a>, which automatically uses lower precision math in certain float32 ops such as <code class="docutils literal notranslate"><span class="pre">tf.linalg.matmul</span></code>. TensorFloat-32 gives some of the performance advantages
of mixed precision when using float32. However, in real-world models, you will still typically see significantly performance improvements from mixed precision due to memory bandwidth savings and ops which TensorFloat-32 does not support.</p>
<p>If running mixed precision on a TPU, you will not see as much of a performance gain compared to running mixed precision on GPUs, especially pre-Ampere GPUs. This is because TPUs do certain ops in bfloat16 under the hood even with the default dtype policy of float32. This is similar to how Ampere GPUs use TensorFloat-32 by default. Compared to Ampere GPUs, TPUs typically see less performance gains with mixed precision on real-world models.</p>
<p>For many real-world models, mixed precision also allows you to double the batch size without running out of memory, as float16 tensors take half the memory. This does not apply however to this toy model, as you can likely run the model in any dtype where each batch consists of the entire MNIST dataset of 60,000 images.</p>
</div>
<div class="section" id="Loss-scaling">
<h3>Loss scaling<a class="headerlink" href="#Loss-scaling" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Loss scaling is a technique which <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit</span></code> automatically performs with the <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> policy to avoid numeric underflow. This section describes what loss scaling is and the next section describes how to use it with a custom training loop.</p>
<div class="section" id="Underflow-and-Overflow">
<h4>Underflow and Overflow<a class="headerlink" href="#Underflow-and-Overflow" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The float16 data type has a narrow dynamic range compared to float32. This means values above <span class="math notranslate nohighlight">\(65504\)</span> will overflow to infinity and values below <span class="math notranslate nohighlight">\(6.0 \times 10^{-8}\)</span> will underflow to zero. float32 and bfloat16 have a much higher dynamic range so that overflow and underflow are not a problem.</p>
<p>For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant(256, dtype=&#39;float16&#39;)
(x ** 2).numpy()  # Overflow
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant(1e-5, dtype=&#39;float16&#39;)
(x ** 2).numpy()  # Underflow
</pre></div>
</div>
</div>
<p>In practice, overflow with float16 rarely occurs. Additionally, underflow also rarely occurs during the forward pass. However, during the backward pass, gradients can underflow to zero. Loss scaling is a technique to prevent this underflow.</p>
</div>
<div class="section" id="Loss-scaling-overview">
<h4>Loss scaling overview<a class="headerlink" href="#Loss-scaling-overview" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The basic concept of loss scaling is simple: simply multiply the loss by some large number, say <span class="math notranslate nohighlight">\(1024\)</span>. We call this number the <em>loss scale</em>. This will cause the gradients to scale by <span class="math notranslate nohighlight">\(1024\)</span> as well, greatly reducing the chance of underflow. Once the final gradients are computed, divide them by <span class="math notranslate nohighlight">\(1024\)</span> to bring them back to their correct values.</p>
<p>The pseudocode for this process is:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>loss_scale = 1024
loss = model(inputs)
loss *= loss_scale
# We assume `grads` are float32. We do not want to divide float16 gradients
grads = compute_gradient(loss, model.trainable_variables)
grads /= loss_scale
</pre></div>
</div>
<p>Choosing a loss scale can be tricky. If the loss scale is too low, gradients may still underflow to zero. If too high, the opposite the problem occurs: the gradients may overflow to infinity.</p>
<p>To solve this, TensorFlow dynamically determines the loss scale so you do not have to choose one manually. If you use <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit</span></code>, loss scaling is done for you so you do not have to do any extra work. If you use a custom training loop, you must explicitly use the special optimizer wrapper <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision.LossScaleOptimizer</span></code> in order to use loss scaling. This is described in the next section.</p>
</div>
</div>
<div class="section" id="Training-the-model-with-a-custom-training-loop">
<h3>Training the model with a custom training loop<a class="headerlink" href="#Training-the-model-with-a-custom-training-loop" title="Enlazar permanentemente con este título">¶</a></h3>
<p>So far, you trained a Keras model with mixed precision using <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit</span></code>. Next, you will use mixed precision with a custom training loop. If you do not already know what a custom training loop is, please read <a class="reference external" href="../tutorials/customization/custom_training_walkthrough.ipynb">the Custom training guide</a> first.</p>
<p>Running a custom training loop with mixed precision requires two changes over running it in float32: 1. Build the model with mixed precision (you already did this) 2. Explicitly use loss scaling if <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code> is used.</p>
<p>For step (2), you will use the <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision.LossScaleOptimizer</span></code> class, which wraps an optimizer and applies loss scaling. By default, it dynamically determines the loss scale so you do not have to choose one. Construct a <code class="docutils literal notranslate"><span class="pre">LossScaleOptimizer</span></code> as follows.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>optimizer = keras.optimizers.RMSprop()
optimizer = mixed_precision.LossScaleOptimizer(optimizer)
</pre></div>
</div>
</div>
<p>If you want, it is possible choose an explicit loss scale or otherwise customize the loss scaling behavior, but it is highly recommended to keep the default loss scaling behavior, as it has been found to work well on all known models. See the <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision.LossScaleOptimizer</span></code> documention if you want to customize the loss scaling behavior.</p>
<p>Next, define the loss object and the <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>s.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train))
                 .shuffle(10000).batch(8192))
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(8192)
</pre></div>
</div>
</div>
<p>Next, define the training step function. Two new methods from the loss scale optimizer are used in order to scale the loss and unscale the gradients: * <code class="docutils literal notranslate"><span class="pre">get_scaled_loss(loss)</span></code>: Multiplies the loss by the loss scale * <code class="docutils literal notranslate"><span class="pre">get_unscaled_gradients(gradients)</span></code>: Takes in a list of scaled gradients as inputs, and divides each one by the loss scale to unscale them</p>
<p>These functions must be used in order to prevent underflow in the gradients. <code class="docutils literal notranslate"><span class="pre">LossScaleOptimizer.apply_gradients</span></code> will then apply gradients if none of them have Infs or NaNs. It will also update the loss scale, halving it if the gradients had Infs or NaNs and potentially increasing it otherwise.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def train_step(x, y):
  with tf.GradientTape() as tape:
    predictions = model(x)
    loss = loss_object(y, predictions)
    scaled_loss = optimizer.get_scaled_loss(loss)
  scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)
  gradients = optimizer.get_unscaled_gradients(scaled_gradients)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return loss
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">LossScaleOptimizer</span></code> will likely skip the first few steps at the start of training. The loss scale starts out high so that the optimal loss scale can quickly be determined. After a few steps, the loss scale will stabilize and very few steps will be skipped. This process happens automatically and does not affect training quality.</p>
<p>Now define the test step.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def test_step(x):
  return model(x, training=False)
</pre></div>
</div>
</div>
<p>Load the initial weights of the model, so you can retrain from scratch.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model.set_weights(initial_weights)
</pre></div>
</div>
</div>
<p>Finally, run the custom training loop.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for epoch in range(5):
  epoch_loss_avg = tf.keras.metrics.Mean()
  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      name=&#39;test_accuracy&#39;)
  for x, y in train_dataset:
    loss = train_step(x, y)
    epoch_loss_avg(loss)
  for x, y in test_dataset:
    predictions = test_step(x)
    test_accuracy.update_state(y, predictions)
  print(&#39;Epoch {}: loss={}, test accuracy={}&#39;.format(epoch, epoch_loss_avg.result(), test_accuracy.result()))
</pre></div>
</div>
</div>
</div>
<div class="section" id="GPU-performance-tips">
<h3>GPU performance tips<a class="headerlink" href="#GPU-performance-tips" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Here are some performance tips when using mixed precision on GPUs.</p>
<div class="section" id="Increasing-your-batch-size">
<h4>Increasing your batch size<a class="headerlink" href="#Increasing-your-batch-size" title="Enlazar permanentemente con este título">¶</a></h4>
<p>If it doesn’t affect model quality, try running with double the batch size when using mixed precision. As float16 tensors use half the memory, this often allows you to double your batch size without running out of memory. Increasing batch size typically increases training throughput, i.e. the training elements per second your model can run on.</p>
</div>
<div class="section" id="Ensuring-GPU-Tensor-Cores-are-used">
<h4>Ensuring GPU Tensor Cores are used<a class="headerlink" href="#Ensuring-GPU-Tensor-Cores-are-used" title="Enlazar permanentemente con este título">¶</a></h4>
<p>As mentioned previously, modern NVIDIA GPUs use a special hardware unit called Tensor Cores that can multiply float16 matrices very quickly. However, Tensor Cores requires certain dimensions of tensors to be a multiple of 8. In the examples below, an argument is bold if and only if it needs to be a multiple of 8 for Tensor Cores to be used.</p>
<ul class="simple">
<li><p>tf.keras.layers.Dense(<strong>units=64</strong>)</p></li>
<li><p>tf.keras.layers.Conv2d(<strong>filters=48</strong>, kernel_size=7, stride=3)</p>
<ul>
<li><p>And similarly for other convolutional layers, such as tf.keras.layers.Conv3d</p></li>
</ul>
</li>
<li><p>tf.keras.layers.LSTM(<strong>units=64</strong>)</p>
<ul>
<li><p>And similar for other RNNs, such as tf.keras.layers.GRU</p></li>
</ul>
</li>
<li><p>tf.keras.Model.fit(epochs=2, <strong>batch_size=128</strong>)</p></li>
</ul>
<p>You should try to use Tensor Cores when possible. If you want to learn more <a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html">NVIDIA deep learning performance guide</a> describes the exact requirements for using Tensor Cores as well as other Tensor Core-related performance information.</p>
</div>
<div class="section" id="XLA">
<h4>XLA<a class="headerlink" href="#XLA" title="Enlazar permanentemente con este título">¶</a></h4>
<p>XLA is a compiler that can further increase mixed precision performance, as well as float32 performance to a lesser extent. See the <a class="reference external" href="https://www.tensorflow.org/xla">XLA guide</a> for details.</p>
</div>
</div>
<div class="section" id="Cloud-TPU-performance-tips">
<h3>Cloud TPU performance tips<a class="headerlink" href="#Cloud-TPU-performance-tips" title="Enlazar permanentemente con este título">¶</a></h3>
<p>As on GPUs, you should try doubling your batch size, as bfloat16 tensors use half the memory. Doubling batch size may increase training throughput.</p>
<p>TPUs do not require any other mixed precision-specific tuning to get optimal performance. TPUs already require the use of XLA. They benefit from having certain dimensions being multiples of <span class="math notranslate nohighlight">\(128\)</span>, but this applies equally to float32 as it does for mixed precision. See the <a class="reference external" href="https://cloud.google.com/tpu/docs/performance-guide">Cloud TPU Performance Guide</a> for general TPU performance tips, which apply to mixed precision as well as float32.</p>
</div>
<div class="section" id="Summary">
<h3>Summary<a class="headerlink" href="#Summary" title="Enlazar permanentemente con este título">¶</a></h3>
<ul>
<li><p>You should use mixed precision if you use TPUs or NVIDIA GPUs with at least compute capability 7.0, as it will improve performance by up to 3x.</p></li>
<li><p>You can use mixed precision with the following lines:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># On TPUs, use &#39;mixed_bfloat16&#39; instead
mixed_precision.set_global_policy(&#39;mixed_float16&#39;)
</pre></div>
</div>
</li>
<li><p>If your model ends in softmax, make sure it is float32. And regardless of what your model ends in, make sure the output is float32.</p></li>
<li><p>If you use a custom training loop with <code class="docutils literal notranslate"><span class="pre">mixed_float16</span></code>, in addition to the above lines, you need to wrap your optimizer with a <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision.LossScaleOptimizer</span></code>. Then call <code class="docutils literal notranslate"><span class="pre">optimizer.get_scaled_loss</span></code> to scale the loss, and <code class="docutils literal notranslate"><span class="pre">optimizer.get_unscaled_gradients</span></code> to unscale the gradients.</p></li>
<li><p>Double the training batch size if it does not reduce evaluation accuracy</p></li>
<li><p>On GPUs, ensure most tensor dimensions are a multiple of <span class="math notranslate nohighlight">\(8\)</span> to maximize performance</p></li>
</ul>
<p>For more examples of mixed precision using the <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision</span></code> API, see the <a class="reference external" href="https://github.com/tensorflow/models/tree/master/official">official models repository</a>. Most official models, such as <a class="reference external" href="https://github.com/tensorflow/models/tree/master/official/vision/image_classification">ResNet</a> and <a class="reference external" href="https://github.com/tensorflow/models/blob/master/official/nlp/transformer">Transformer</a> will run using mixed precision by passing <code class="docutils literal notranslate"><span class="pre">--dtype=fp16</span></code>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>