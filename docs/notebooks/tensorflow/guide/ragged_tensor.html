

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2018 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2018 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/ragged_tensor.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2018-The-TensorFlow-Authors.">
<h1>Copyright 2018 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2018-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Ragged-tensors">
<h2>Ragged tensors<a class="headerlink" href="#Ragged-tensors" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>6d02137ff03743569c362aee171a682d|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>eb54e53ead6943e6b425e6cfc08b11c6|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>e38a376523dc49ec818bafe368bf3648|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>bcb3698bc0d049eeb28f7540a929f8a7|Download notebook</p>
</td></table><p><strong>API Documentation:</strong> <code class="docutils literal notranslate"><span class="pre">`tf.RaggedTensor</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/RaggedTensor">https://www.tensorflow.org/api_docs/python/tf/RaggedTensor</a>&gt;`__ <code class="docutils literal notranslate"><span class="pre">`tf.ragged</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/ragged">https://www.tensorflow.org/api_docs/python/tf/ragged</a>&gt;`__</p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import math
import tensorflow as tf
</pre></div>
</div>
</div>
</div>
<div class="section" id="Overview">
<h3>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Your data comes in many shapes; your tensors should too. <em>Ragged tensors</em> are the TensorFlow equivalent of nested variable-length lists. They make it easy to store and process data with non-uniform shapes, including:</p>
<ul class="simple">
<li><p>Variable-length features, such as the set of actors in a movie.</p></li>
<li><p>Batches of variable-length sequential inputs, such as sentences or video clips.</p></li>
<li><p>Hierarchical inputs, such as text documents that are subdivided into sections, paragraphs, sentences, and words.</p></li>
<li><p>Individual fields in structured inputs, such as protocol buffers.</p></li>
</ul>
<div class="section" id="What-you-can-do-with-a-ragged-tensor">
<h4>What you can do with a ragged tensor<a class="headerlink" href="#What-you-can-do-with-a-ragged-tensor" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Ragged tensors are supported by more than a hundred TensorFlow operations, including math operations (such as <code class="docutils literal notranslate"><span class="pre">tf.add</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.reduce_mean</span></code>), array operations (such as <code class="docutils literal notranslate"><span class="pre">tf.concat</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.tile</span></code>), string manipulation ops (such as <code class="docutils literal notranslate"><span class="pre">tf.substr</span></code>), control flow operations (such as <code class="docutils literal notranslate"><span class="pre">tf.while_loop</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.map_fn</span></code>), and many others:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>digits = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])
words = tf.ragged.constant([[&quot;So&quot;, &quot;long&quot;], [&quot;thanks&quot;, &quot;for&quot;, &quot;all&quot;, &quot;the&quot;, &quot;fish&quot;]])
print(tf.add(digits, 3))
print(tf.reduce_mean(digits, axis=1))
print(tf.concat([digits, [[5, 3]]], axis=0))
print(tf.tile(digits, [1, 2]))
print(tf.strings.substr(words, 0, 2))
print(tf.map_fn(tf.math.square, digits))
</pre></div>
</div>
</div>
<p>There are also a number of methods and operations that are specific to ragged tensors, including factory methods, conversion methods, and value-mapping operations. For a list of supported ops, see the <strong>``tf.ragged`` package documentation</strong>.</p>
<p>Ragged tensors are supported by many TensorFlow APIs, including <a class="reference external" href="https://www.tensorflow.org/guide/keras">Keras</a>, <a class="reference external" href="https://www.tensorflow.org/guide/data">Datasets</a>, <a class="reference external" href="https://www.tensorflow.org/guide/function">tf.function</a>, <a class="reference external" href="https://www.tensorflow.org/guide/saved_model">SavedModels</a>, and <a class="reference external" href="https://www.tensorflow.org/tutorials/load_data/tfrecord">tf.Example</a>. For more information, see the section on <strong>TensorFlow APIs</strong> below.</p>
<p>As with normal tensors, you can use Python-style indexing to access specific slices of a ragged tensor. For more information, see the section on <strong>Indexing</strong> below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(digits[0])       # First row
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(digits[:, :2])   # First two values in each row.
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(digits[:, -2:])  # Last two values in each row.
</pre></div>
</div>
</div>
<p>And just like normal tensors, you can use Python arithmetic and comparison operators to perform elementwise operations. For more information, see the section on <strong>Overloaded Operators</strong> below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(digits + 3)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(digits + tf.ragged.constant([[1, 2, 3, 4], [], [5, 6, 7], [8], []]))
</pre></div>
</div>
</div>
<p>If you need to perform an elementwise transformation to the values of a <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code>, you can use <code class="docutils literal notranslate"><span class="pre">tf.ragged.map_flat_values</span></code>, which takes a function plus one or more arguments, and applies the function to transform the <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code>’s values.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>times_two_plus_one = lambda x: x * 2 + 1
print(tf.ragged.map_flat_values(times_two_plus_one, digits))
</pre></div>
</div>
</div>
<p>Ragged tensors can be converted to nested Python <code class="docutils literal notranslate"><span class="pre">list</span></code>s and numpy <code class="docutils literal notranslate"><span class="pre">array</span></code>s:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>digits.to_list()
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>digits.numpy()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Constructing-a-ragged-tensor">
<h4>Constructing a ragged tensor<a class="headerlink" href="#Constructing-a-ragged-tensor" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The simplest way to construct a ragged tensor is using <code class="docutils literal notranslate"><span class="pre">tf.ragged.constant</span></code>, which builds the <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> corresponding to a given nested Python <code class="docutils literal notranslate"><span class="pre">list</span></code> or numpy <code class="docutils literal notranslate"><span class="pre">array</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sentences = tf.ragged.constant([
    [&quot;Let&#39;s&quot;, &quot;build&quot;, &quot;some&quot;, &quot;ragged&quot;, &quot;tensors&quot;, &quot;!&quot;],
    [&quot;We&quot;, &quot;can&quot;, &quot;use&quot;, &quot;tf.ragged.constant&quot;, &quot;.&quot;]])
print(sentences)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>paragraphs = tf.ragged.constant([
    [[&#39;I&#39;, &#39;have&#39;, &#39;a&#39;, &#39;cat&#39;], [&#39;His&#39;, &#39;name&#39;, &#39;is&#39;, &#39;Mat&#39;]],
    [[&#39;Do&#39;, &#39;you&#39;, &#39;want&#39;, &#39;to&#39;, &#39;come&#39;, &#39;visit&#39;], [&quot;I&#39;m&quot;, &#39;free&#39;, &#39;tomorrow&#39;]],
])
print(paragraphs)
</pre></div>
</div>
</div>
<p>Ragged tensors can also be constructed by pairing flat <em>values</em> tensors with <em>row-partitioning</em> tensors indicating how those values should be divided into rows, using factory classmethods such as <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_value_rowids</span></code>, <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_row_lengths</span></code>, and <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_row_splits</span></code>.</p>
<div class="section" id="tf.RaggedTensor.from_value_rowids">
<h5><code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_value_rowids</span></code><a class="headerlink" href="#tf.RaggedTensor.from_value_rowids" title="Enlazar permanentemente con este título">¶</a></h5>
<p>If you know which row each value belongs in, then you can build a <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> using a <code class="docutils literal notranslate"><span class="pre">value_rowids</span></code> row-partitioning tensor:</p>
<p><img alt="value_rowids" src="https://www.tensorflow.org/images/ragged_tensors/value_rowids.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.RaggedTensor.from_value_rowids(
    values=[3, 1, 4, 1, 5, 9, 2],
    value_rowids=[0, 0, 0, 0, 2, 2, 3]))
</pre></div>
</div>
</div>
</div>
<div class="section" id="tf.RaggedTensor.from_row_lengths">
<h5><code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_row_lengths</span></code><a class="headerlink" href="#tf.RaggedTensor.from_row_lengths" title="Enlazar permanentemente con este título">¶</a></h5>
<p>If you know how long each row is, then you can use a <code class="docutils literal notranslate"><span class="pre">row_lengths</span></code> row-partitioning tensor:</p>
<p><img alt="row_lengths" src="https://www.tensorflow.org/images/ragged_tensors/row_lengths.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.RaggedTensor.from_row_lengths(
    values=[3, 1, 4, 1, 5, 9, 2],
    row_lengths=[4, 0, 2, 1]))
</pre></div>
</div>
</div>
</div>
<div class="section" id="tf.RaggedTensor.from_row_splits">
<h5><code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_row_splits</span></code><a class="headerlink" href="#tf.RaggedTensor.from_row_splits" title="Enlazar permanentemente con este título">¶</a></h5>
<p>If you know the index where each row starts and ends, then you can use a <code class="docutils literal notranslate"><span class="pre">row_splits</span></code> row-partitioning tensor:</p>
<p><img alt="row_splits" src="https://www.tensorflow.org/images/ragged_tensors/row_splits.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.RaggedTensor.from_row_splits(
    values=[3, 1, 4, 1, 5, 9, 2],
    row_splits=[0, 4, 4, 6, 7]))
</pre></div>
</div>
</div>
<p>See the <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> class documentation for a full list of factory methods.</p>
<p>Note: By default, these factory methods add assertions that the row partition tensor is well-formed and consistent with the number of values. The <code class="docutils literal notranslate"><span class="pre">validate=False</span></code> parameter can be used to skip these checks if you can guarantee that the inputs are well-formed and consistent.</p>
</div>
</div>
<div class="section" id="What-you-can-store-in-a-ragged-tensor">
<h4>What you can store in a ragged tensor<a class="headerlink" href="#What-you-can-store-in-a-ragged-tensor" title="Enlazar permanentemente con este título">¶</a></h4>
<p>As with normal <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s, the values in a <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> must all have the same type; and the values must all be at the same nesting depth (the <em>rank</em> of the tensor):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.ragged.constant([[&quot;Hi&quot;], [&quot;How&quot;, &quot;are&quot;, &quot;you&quot;]]))  # ok: type=string, rank=2
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.ragged.constant([[[1, 2], [3]], [[4, 5]]]))        # ok: type=int32, rank=3
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>try:
  tf.ragged.constant([[&quot;one&quot;, &quot;two&quot;], [3, 4]])              # bad: multiple types
except ValueError as exception:
  print(exception)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>try:
  tf.ragged.constant([&quot;A&quot;, [&quot;B&quot;, &quot;C&quot;]])                     # bad: multiple nesting depths
except ValueError as exception:
  print(exception)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Example-use-case">
<h3>Example use case<a class="headerlink" href="#Example-use-case" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The following example demonstrates how <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code>s can be used to construct and combine unigram and bigram embeddings for a batch of variable-length queries, using special markers for the beginning and end of each sentence. For more details on the ops used in this example, see the <code class="docutils literal notranslate"><span class="pre">tf.ragged</span></code> package documentation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>queries = tf.ragged.constant([[&#39;Who&#39;, &#39;is&#39;, &#39;Dan&#39;, &#39;Smith&#39;],
                              [&#39;Pause&#39;],
                              [&#39;Will&#39;, &#39;it&#39;, &#39;rain&#39;, &#39;later&#39;, &#39;today&#39;]])

# Create an embedding table.
num_buckets = 1024
embedding_size = 4
embedding_table = tf.Variable(
    tf.random.truncated_normal([num_buckets, embedding_size],
                       stddev=1.0 / math.sqrt(embedding_size)))

# Look up the embedding for each word.
word_buckets = tf.strings.to_hash_bucket_fast(queries, num_buckets)
word_embeddings = tf.nn.embedding_lookup(embedding_table, word_buckets)     # ①

# Add markers to the beginning and end of each sentence.
marker = tf.fill([queries.nrows(), 1], &#39;#&#39;)
padded = tf.concat([marker, queries, marker], axis=1)                       # ②

# Build word bigrams &amp; look up embeddings.
bigrams = tf.strings.join([padded[:, :-1], padded[:, 1:]], separator=&#39;+&#39;)   # ③

bigram_buckets = tf.strings.to_hash_bucket_fast(bigrams, num_buckets)
bigram_embeddings = tf.nn.embedding_lookup(embedding_table, bigram_buckets) # ④

# Find the average embedding for each sentence
all_embeddings = tf.concat([word_embeddings, bigram_embeddings], axis=1)    # ⑤
avg_embedding = tf.reduce_mean(all_embeddings, axis=1)                      # ⑥
print(avg_embedding)
</pre></div>
</div>
</div>
<p><img alt="ragged_example" src="https://www.tensorflow.org/images/ragged_tensors/ragged_example.png" /></p>
</div>
<div class="section" id="Ragged-and-uniform-dimensions">
<h3>Ragged and uniform dimensions<a class="headerlink" href="#Ragged-and-uniform-dimensions" title="Enlazar permanentemente con este título">¶</a></h3>
<p>A <strong>ragged dimension</strong> is a dimension whose slices may have different lengths. For example, the inner (column) dimension of <code class="docutils literal notranslate"><span class="pre">rt=[[3,</span> <span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">1],</span> <span class="pre">[],</span> <span class="pre">[5,</span> <span class="pre">9,</span> <span class="pre">2],</span> <span class="pre">[6],</span> <span class="pre">[]]</span></code> is ragged, since the column slices (<code class="docutils literal notranslate"><span class="pre">rt[0,</span> <span class="pre">:]</span></code>, …, <code class="docutils literal notranslate"><span class="pre">rt[4,</span> <span class="pre">:]</span></code>) have different lengths. Dimensions whose slices all have the same length are called <em>uniform dimensions</em>.</p>
<p>The outermost dimension of a ragged tensor is always uniform, since it consists of a single slice (and so there is no possibility for differing slice lengths). The remaining dimensions may be either ragged or uniform. For example, we might store the word embeddings for each word in a batch of sentences using a ragged tensor with shape <code class="docutils literal notranslate"><span class="pre">[num_sentences,</span> <span class="pre">(num_words),</span> <span class="pre">embedding_size]</span></code>, where the parentheses around <code class="docutils literal notranslate"><span class="pre">(num_words)</span></code> indicate that the dimension is ragged.</p>
<p><img alt="sent_word_embed" src="https://www.tensorflow.org/images/ragged_tensors/sent_word_embed.png" /></p>
<p>Ragged tensors may have multiple ragged dimensions. For example, we could store a batch of structured text documents using a tensor with shape <code class="docutils literal notranslate"><span class="pre">[num_documents,</span> <span class="pre">(num_paragraphs),</span> <span class="pre">(num_sentences),</span> <span class="pre">(num_words)]</span></code> (where again parentheses are used to indicate ragged dimensions).</p>
<p>As with <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>, the <strong>rank</strong> of a ragged tensor is its total number of dimensions (including both ragged and uniform dimensions). A <strong>potentially ragged tensor</strong> is a value that might be either a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code>.</p>
<p>When describing the shape of a RaggedTensor, ragged dimensions are conventionally indicated by enclosing them in parentheses. For example, as we saw above, the shape of a 3-D RaggedTensor that stores word embeddings for each word in a batch of sentences can be written as <code class="docutils literal notranslate"><span class="pre">[num_sentences,</span> <span class="pre">(num_words),</span> <span class="pre">embedding_size]</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">RaggedTensor.shape</span></code> attribute returns a <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> for a ragged tensor, where ragged dimensions have size <code class="docutils literal notranslate"><span class="pre">None</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tf.ragged.constant([[&quot;Hi&quot;], [&quot;How&quot;, &quot;are&quot;, &quot;you&quot;]]).shape
</pre></div>
</div>
</div>
<p>The method <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.bounding_shape</span></code> can be used to find a tight bounding shape for a given <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.ragged.constant([[&quot;Hi&quot;], [&quot;How&quot;, &quot;are&quot;, &quot;you&quot;]]).bounding_shape())
</pre></div>
</div>
</div>
</div>
<div class="section" id="Ragged-vs. sparse">
<h3>Ragged vs. sparse<a class="headerlink" href="#Ragged-vs. sparse" title="Enlazar permanentemente con este título">¶</a></h3>
<p>A ragged tensor should <em>not</em> be thought of as a type of sparse tensor. In particular, sparse tensors are <em>efficient encodings for tf.Tensor</em>, that model the same data in a compact format; but ragged tensor is an <em>extension to tf.Tensor</em>, that models an expanded class of data. This difference is crucial when defining operations:</p>
<ul class="simple">
<li><p>Applying an op to a sparse or dense tensor should always give the same result.</p></li>
<li><p>Applying an op to a ragged or sparse tensor may give different results.</p></li>
</ul>
<p>As an illustrative example, consider how array operations such as <code class="docutils literal notranslate"><span class="pre">concat</span></code>, <code class="docutils literal notranslate"><span class="pre">stack</span></code>, and <code class="docutils literal notranslate"><span class="pre">tile</span></code> are defined for ragged vs. sparse tensors. Concatenating ragged tensors joins each row to form a single row with the combined length:</p>
<p><img alt="ragged_concat" src="https://www.tensorflow.org/images/ragged_tensors/ragged_concat.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ragged_x = tf.ragged.constant([[&quot;John&quot;], [&quot;a&quot;, &quot;big&quot;, &quot;dog&quot;], [&quot;my&quot;, &quot;cat&quot;]])
ragged_y = tf.ragged.constant([[&quot;fell&quot;, &quot;asleep&quot;], [&quot;barked&quot;], [&quot;is&quot;, &quot;fuzzy&quot;]])
print(tf.concat([ragged_x, ragged_y], axis=1))
</pre></div>
</div>
</div>
<p>But concatenating sparse tensors is equivalent to concatenating the corresponding dense tensors, as illustrated by the following example (where Ø indicates missing values):</p>
<p><img alt="sparse_concat" src="https://www.tensorflow.org/images/ragged_tensors/sparse_concat.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sparse_x = ragged_x.to_sparse()
sparse_y = ragged_y.to_sparse()
sparse_result = tf.sparse.concat(sp_inputs=[sparse_x, sparse_y], axis=1)
print(tf.sparse.to_dense(sparse_result, &#39;&#39;))
</pre></div>
</div>
</div>
<p>For another example of why this distinction is important, consider the definition of “the mean value of each row” for an op such as <code class="docutils literal notranslate"><span class="pre">tf.reduce_mean</span></code>. For a ragged tensor, the mean value for a row is the sum of the row’s values divided by the row’s width. But for a sparse tensor, the mean value for a row is the sum of the row’s values divided by the sparse tensor’s overall width (which is greater than or equal to the width of the longest row).</p>
</div>
<div class="section" id="TensorFlow-APIs">
<h3>TensorFlow APIs<a class="headerlink" href="#TensorFlow-APIs" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="section" id="Keras">
<h4>Keras<a class="headerlink" href="#Keras" title="Enlazar permanentemente con este título">¶</a></h4>
<p><a class="reference external" href="https://www.tensorflow.org/guide/keras">tf.keras</a> is TensorFlow’s high-level API for building and training deep learning models. Ragged tensors may be passed as inputs to a Keras model by setting <code class="docutils literal notranslate"><span class="pre">ragged=True</span></code> on <code class="docutils literal notranslate"><span class="pre">tf.keras.Input</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.InputLayer</span></code>. Ragged tensors may also be passed between Keras layers, and returned by Keras models. The following example shows a toy LSTM model that is trained using ragged tensors.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Task: predict whether each sentence is a question or not.
sentences = tf.constant(
    [&#39;What makes you think she is a witch?&#39;,
     &#39;She turned me into a newt.&#39;,
     &#39;A newt?&#39;,
     &#39;Well, I got better.&#39;])
is_question = tf.constant([True, False, True, False])

# Preprocess the input strings.
hash_buckets = 1000
words = tf.strings.split(sentences, &#39; &#39;)
hashed_words = tf.strings.to_hash_bucket_fast(words, hash_buckets)

# Build the Keras model.
keras_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=[None], dtype=tf.int64, ragged=True),
    tf.keras.layers.Embedding(hash_buckets, 16),
    tf.keras.layers.LSTM(32, use_bias=False),
    tf.keras.layers.Dense(32),
    tf.keras.layers.Activation(tf.nn.relu),
    tf.keras.layers.Dense(1)
])

keras_model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)
keras_model.fit(hashed_words, is_question, epochs=5)
print(keras_model.predict(hashed_words))
</pre></div>
</div>
</div>
</div>
<div class="section" id="tf.Example">
<h4>tf.Example<a class="headerlink" href="#tf.Example" title="Enlazar permanentemente con este título">¶</a></h4>
<p><a class="reference external" href="https://www.tensorflow.org/tutorials/load_data/tfrecord">tf.Example</a> is a standard <a class="reference external" href="https://developers.google.com/protocol-buffers/">protobuf</a> encoding for TensorFlow data. Data encoded with <code class="docutils literal notranslate"><span class="pre">tf.Example</span></code>s often includes variable-length features. For example, the following code defines a batch of four <code class="docutils literal notranslate"><span class="pre">tf.Example</span></code> messages with different feature lengths:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import google.protobuf.text_format as pbtext

def build_tf_example(s):
  return pbtext.Merge(s, tf.train.Example()).SerializeToString()

example_batch = [
  build_tf_example(r&#39;&#39;&#39;
    features {
      feature {key: &quot;colors&quot; value {bytes_list {value: [&quot;red&quot;, &quot;blue&quot;]} } }
      feature {key: &quot;lengths&quot; value {int64_list {value: [7]} } } }&#39;&#39;&#39;),
  build_tf_example(r&#39;&#39;&#39;
    features {
      feature {key: &quot;colors&quot; value {bytes_list {value: [&quot;orange&quot;]} } }
      feature {key: &quot;lengths&quot; value {int64_list {value: []} } } }&#39;&#39;&#39;),
  build_tf_example(r&#39;&#39;&#39;
    features {
      feature {key: &quot;colors&quot; value {bytes_list {value: [&quot;black&quot;, &quot;yellow&quot;]} } }
      feature {key: &quot;lengths&quot; value {int64_list {value: [1, 3]} } } }&#39;&#39;&#39;),
  build_tf_example(r&#39;&#39;&#39;
    features {
      feature {key: &quot;colors&quot; value {bytes_list {value: [&quot;green&quot;]} } }
      feature {key: &quot;lengths&quot; value {int64_list {value: [3, 5, 2]} } } }&#39;&#39;&#39;)]
</pre></div>
</div>
</div>
<p>We can parse this encoded data using <code class="docutils literal notranslate"><span class="pre">tf.io.parse_example</span></code>, which takes a tensor of serialized strings and a feature specification dictionary, and returns a dictionary mapping feature names to tensors. To read the variable-length features into ragged tensors, we simply use <code class="docutils literal notranslate"><span class="pre">tf.io.RaggedFeature</span></code> in the feature specification dictionary:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>feature_specification = {
    &#39;colors&#39;: tf.io.RaggedFeature(tf.string),
    &#39;lengths&#39;: tf.io.RaggedFeature(tf.int64),
}
feature_tensors = tf.io.parse_example(example_batch, feature_specification)
for name, value in feature_tensors.items():
  print(&quot;{}={}&quot;.format(name, value))
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.io.RaggedFeature</span></code> can also be used to read features with multiple ragged dimensions. For details, see the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature">API documentation</a>.</p>
</div>
<div class="section" id="Datasets">
<h4>Datasets<a class="headerlink" href="#Datasets" title="Enlazar permanentemente con este título">¶</a></h4>
<p><a class="reference external" href="https://www.tensorflow.org/guide/data">tf.data</a> is an API that enables you to build complex input pipelines from simple, reusable pieces. Its core data structure is <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>, which represents a sequence of elements, in which each element consists of one or more components.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Helper function used to print datasets in the examples below.
def print_dictionary_dataset(dataset):
  for i, element in enumerate(dataset):
    print(&quot;Element {}:&quot;.format(i))
    for (feature_name, feature_value) in element.items():
      print(&#39;{:&gt;14} = {}&#39;.format(feature_name, feature_value))
</pre></div>
</div>
</div>
<div class="section" id="Building-Datasets-with-ragged-tensors">
<h5>Building Datasets with ragged tensors<a class="headerlink" href="#Building-Datasets-with-ragged-tensors" title="Enlazar permanentemente con este título">¶</a></h5>
<p>Datasets can be built from ragged tensors using the same methods that are used to build them from <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>s or numpy <code class="docutils literal notranslate"><span class="pre">array</span></code>s, such as <code class="docutils literal notranslate"><span class="pre">Dataset.from_tensor_slices</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>dataset = tf.data.Dataset.from_tensor_slices(feature_tensors)
print_dictionary_dataset(dataset)
</pre></div>
</div>
</div>
<p>Note: <code class="docutils literal notranslate"><span class="pre">Dataset.from_generator</span></code> does not support ragged tensors yet, but support will be added soon.</p>
</div>
<div class="section" id="Batching-and-unbatching-Datasets-with-ragged-tensors">
<h5>Batching and unbatching Datasets with ragged tensors<a class="headerlink" href="#Batching-and-unbatching-Datasets-with-ragged-tensors" title="Enlazar permanentemente con este título">¶</a></h5>
<p>Datasets with ragged tensors can be batched (which combines <em>n</em> consecutive elements into a single elements) using the <code class="docutils literal notranslate"><span class="pre">Dataset.batch</span></code> method.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>batched_dataset = dataset.batch(2)
print_dictionary_dataset(batched_dataset)
</pre></div>
</div>
</div>
<p>Conversely, a batched dataset can be transformed into a flat dataset using <code class="docutils literal notranslate"><span class="pre">Dataset.unbatch</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>unbatched_dataset = batched_dataset.unbatch()
print_dictionary_dataset(unbatched_dataset)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Batching-Datasets-with-variable-length-non-ragged-tensors">
<h5>Batching Datasets with variable-length non-ragged tensors<a class="headerlink" href="#Batching-Datasets-with-variable-length-non-ragged-tensors" title="Enlazar permanentemente con este título">¶</a></h5>
<p>If you have a Dataset that contains non-ragged tensors, and tensor lengths vary across elements, then you can batch those non-ragged tensors into ragged tensors by applying the <code class="docutils literal notranslate"><span class="pre">dense_to_ragged_batch</span></code> transformation:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>non_ragged_dataset = tf.data.Dataset.from_tensor_slices([1, 5, 3, 2, 8])
non_ragged_dataset = non_ragged_dataset.map(tf.range)
batched_non_ragged_dataset = non_ragged_dataset.apply(
    tf.data.experimental.dense_to_ragged_batch(2))
for element in batched_non_ragged_dataset:
  print(element)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Transforming-Datasets-with-ragged-tensors">
<h5>Transforming Datasets with ragged tensors<a class="headerlink" href="#Transforming-Datasets-with-ragged-tensors" title="Enlazar permanentemente con este título">¶</a></h5>
<p>Ragged tensors in Datasets can also be created or transformed using <code class="docutils literal notranslate"><span class="pre">Dataset.map</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def transform_lengths(features):
  return {
      &#39;mean_length&#39;: tf.math.reduce_mean(features[&#39;lengths&#39;]),
      &#39;length_ranges&#39;: tf.ragged.range(features[&#39;lengths&#39;])}
transformed_dataset = dataset.map(transform_lengths)
print_dictionary_dataset(transformed_dataset)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tf.function">
<h4>tf.function<a class="headerlink" href="#tf.function" title="Enlazar permanentemente con este título">¶</a></h4>
<p><a class="reference external" href="https://www.tensorflow.org/guide/function">tf.function</a> is a decorator that precomputes TensorFlow graphs for Python functions, which can substantially improve the performance of your TensorFlow code. Ragged tensors can be used transparently with <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code>-decorated functions. For example, the following function works with both ragged and non-ragged tensors:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def make_palindrome(x, axis):
  return tf.concat([x, tf.reverse(x, [axis])], axis)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>make_palindrome(tf.constant([[1, 2], [3, 4], [5, 6]]), axis=1)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>make_palindrome(tf.ragged.constant([[1, 2], [3], [4, 5, 6]]), axis=1)
</pre></div>
</div>
</div>
<p>If you wish to explicitly specify the <code class="docutils literal notranslate"><span class="pre">input_signature</span></code> for the <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>, then you can do so using <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensorSpec</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function(
    input_signature=[tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32)])
def max_and_min(rt):
  return (tf.math.reduce_max(rt, axis=-1), tf.math.reduce_min(rt, axis=-1))

max_and_min(tf.ragged.constant([[1, 2], [3], [4, 5, 6]]))
</pre></div>
</div>
</div>
<div class="section" id="Concrete-functions">
<h5>Concrete functions<a class="headerlink" href="#Concrete-functions" title="Enlazar permanentemente con este título">¶</a></h5>
<p><a class="reference external" href="https://www.tensorflow.org/guide/function#obtaining_concrete_functions">Concrete functions</a> encapsulate individual traced graphs that are built by <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>. Ragged tensors can be used transparently with concrete functions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def increment(x):
  return x + 1

rt = tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
cf = increment.get_concrete_function(rt)
print(cf(rt))

</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="SavedModels">
<h4>SavedModels<a class="headerlink" href="#SavedModels" title="Enlazar permanentemente con este título">¶</a></h4>
<p>A <a class="reference external" href="https://www.tensorflow.org/guide/saved_model">SavedModel</a> is a serialized TensorFlow program, including both weights and computation. It can be built from a Keras model or from a custom model. In either case, ragged tensors can be used transparently with the functions and methods defined by a SavedModel.</p>
<div class="section" id="Example:-saving-a-Keras-model">
<h5>Example: saving a Keras model<a class="headerlink" href="#Example:-saving-a-Keras-model" title="Enlazar permanentemente con este título">¶</a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tempfile

keras_module_path = tempfile.mkdtemp()
tf.saved_model.save(keras_model, keras_module_path)
imported_model = tf.saved_model.load(keras_module_path)
imported_model(hashed_words)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Example:-saving-a-custom-model">
<h5>Example: saving a custom model<a class="headerlink" href="#Example:-saving-a-custom-model" title="Enlazar permanentemente con este título">¶</a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class CustomModule(tf.Module):
  def __init__(self, variable_value):
    super(CustomModule, self).__init__()
    self.v = tf.Variable(variable_value)

  @tf.function
  def grow(self, x):
    return x * self.v

module = CustomModule(100.0)

# Before saving a custom model, we must ensure that concrete functions are
# built for each input signature that we will need.
module.grow.get_concrete_function(tf.RaggedTensorSpec(shape=[None, None],
                                                      dtype=tf.float32))

custom_module_path = tempfile.mkdtemp()
tf.saved_model.save(module, custom_module_path)
imported_model = tf.saved_model.load(custom_module_path)
imported_model.grow(tf.ragged.constant([[1.0, 4.0, 3.0], [2.0]]))
</pre></div>
</div>
</div>
<p>Note: SavedModel <a class="reference external" href="https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export">signatures</a> are concrete functions. As discussed in the section on Concrete Functions above, ragged tensors are only handled correctly by concrete functions starting with TensorFlow 2.3. If you need to use SavedModel signatures in a previous version of TensorFlow, then we recommend decomposing the ragged tensor into its component tensors.</p>
</div>
</div>
</div>
<div class="section" id="Overloaded-operators">
<h3>Overloaded operators<a class="headerlink" href="#Overloaded-operators" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> class overloads the standard Python arithmetic and comparison operators, making it easy to perform basic elementwise math:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
y = tf.ragged.constant([[1, 1], [2], [3, 3, 3]])
print(x + y)
</pre></div>
</div>
</div>
<p>Since the overloaded operators perform elementwise computations, the inputs to all binary operations must have the same shape, or be broadcastable to the same shape. In the simplest broadcasting case, a single scalar is combined elementwise with each value in a ragged tensor:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
print(x + 3)
</pre></div>
</div>
</div>
<p>For a discussion of more advanced cases, see the section on <strong>Broadcasting</strong>.</p>
<p>Ragged tensors overload the same set of operators as normal <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s: the unary operators <code class="docutils literal notranslate"><span class="pre">-</span></code>, <code class="docutils literal notranslate"><span class="pre">~</span></code>, and <code class="docutils literal notranslate"><span class="pre">abs()</span></code>; and the binary operators <code class="docutils literal notranslate"><span class="pre">+</span></code>, <code class="docutils literal notranslate"><span class="pre">-</span></code>, <code class="docutils literal notranslate"><span class="pre">*</span></code>, <code class="docutils literal notranslate"><span class="pre">/</span></code>, <code class="docutils literal notranslate"><span class="pre">//</span></code>, <code class="docutils literal notranslate"><span class="pre">%</span></code>, <code class="docutils literal notranslate"><span class="pre">**</span></code>, <code class="docutils literal notranslate"><span class="pre">&amp;</span></code>, <code class="docutils literal notranslate"><span class="pre">|</span></code>, <code class="docutils literal notranslate"><span class="pre">^</span></code>, <code class="docutils literal notranslate"><span class="pre">==</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;=</span></code>, <code class="docutils literal notranslate"><span class="pre">&gt;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&gt;=</span></code>.</p>
</div>
<div class="section" id="Indexing">
<h3>Indexing<a class="headerlink" href="#Indexing" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Ragged tensors support Python-style indexing, including multidimensional indexing and slicing. The following examples demonstrate ragged tensor indexing with a 2-D and a 3-D ragged tensor.</p>
<div class="section" id="Indexing-examples:-2D-ragged-tensor">
<h4>Indexing examples: 2D ragged tensor<a class="headerlink" href="#Indexing-examples:-2D-ragged-tensor" title="Enlazar permanentemente con este título">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>queries = tf.ragged.constant(
    [[&#39;Who&#39;, &#39;is&#39;, &#39;George&#39;, &#39;Washington&#39;],
     [&#39;What&#39;, &#39;is&#39;, &#39;the&#39;, &#39;weather&#39;, &#39;tomorrow&#39;],
     [&#39;Goodnight&#39;]])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(queries[1])                   # A single query
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(queries[1, 2])                # A single word
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(queries[1:])                  # Everything but the first row
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(queries[:, :3])               # The first 3 words of each query
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(queries[:, -2:])              # The last 2 words of each query
</pre></div>
</div>
</div>
</div>
<div class="section" id="Indexing-examples-3D-ragged-tensor">
<h4>Indexing examples 3D ragged tensor<a class="headerlink" href="#Indexing-examples-3D-ragged-tensor" title="Enlazar permanentemente con este título">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.ragged.constant([[[1, 2, 3], [4]],
                         [[5], [], [6]],
                         [[7]],
                         [[8, 9], [10]]])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rt[1])                        # Second row (2-D RaggedTensor)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rt[3, 0])                     # First element of fourth row (1-D Tensor)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rt[:, 1:3])                   # Items 1-3 of each row (3-D RaggedTensor)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rt[:, -1:])                   # Last item of each row (3-D RaggedTensor)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code>s supports multidimensional indexing and slicing, with one restriction: indexing into a ragged dimension is not allowed. This case is problematic because the indicated value may exist in some rows but not others. In such cases, it’s not obvious whether we should (1) raise an <code class="docutils literal notranslate"><span class="pre">IndexError</span></code>; (2) use a default value; or (3) skip that value and return a tensor with fewer rows than we started with. Following the <a class="reference external" href="https://www.python.org/dev/peps/pep-0020/">guiding principles of
Python</a> (“In the face of ambiguity, refuse the temptation to guess” ), we currently disallow this operation.</p>
</div>
</div>
<div class="section" id="Tensor-type-conversion">
<h3>Tensor type conversion<a class="headerlink" href="#Tensor-type-conversion" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> class defines methods that can be used to convert between <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code>s and <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>s or <code class="docutils literal notranslate"><span class="pre">tf.SparseTensors</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ragged_sentences = tf.ragged.constant([
    [&#39;Hi&#39;], [&#39;Welcome&#39;, &#39;to&#39;, &#39;the&#39;, &#39;fair&#39;], [&#39;Have&#39;, &#39;fun&#39;]])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># RaggedTensor -&gt; Tensor
print(ragged_sentences.to_tensor(default_value=&#39;&#39;, shape=[None, 10]))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Tensor -&gt; RaggedTensor
x = [[1, 3, -1, -1], [2, -1, -1, -1], [4, 5, 8, 9]]
print(tf.RaggedTensor.from_tensor(x, padding=-1))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#RaggedTensor -&gt; SparseTensor
print(ragged_sentences.to_sparse())
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># SparseTensor -&gt; RaggedTensor
st = tf.SparseTensor(indices=[[0, 0], [2, 0], [2, 1]],
                     values=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;],
                     dense_shape=[3, 3])
print(tf.RaggedTensor.from_sparse(st))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Evaluating-ragged-tensors">
<h3>Evaluating ragged tensors<a class="headerlink" href="#Evaluating-ragged-tensors" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To access the values in a ragged tensor, you can:</p>
<ol class="arabic simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.to_list()</span></code> to convert the ragged tensor to a nested python list.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.numpy()</span></code> to convert the ragged tensor to a numpy array whose values are nested numpy arrays.</p></li>
<li><p>Decompose the ragged tensor into its components, using the <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.values</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.row_splits</span></code> properties, or row-paritioning methods such as <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.row_lengths()</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.value_rowids()</span></code>.</p></li>
<li><p>Use Python indexing to select values from the ragged tensor.</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.ragged.constant([[1, 2], [3, 4, 5], [6], [], [7]])
print(&quot;python list:&quot;, rt.to_list())
print(&quot;numpy array:&quot;, rt.numpy())
print(&quot;values:&quot;, rt.values.numpy())
print(&quot;splits:&quot;, rt.row_splits.numpy())
print(&quot;indexed value:&quot;, rt[1].numpy())
</pre></div>
</div>
</div>
</div>
<div class="section" id="Broadcasting">
<h3>Broadcasting<a class="headerlink" href="#Broadcasting" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Broadcasting is the process of making tensors with different shapes have compatible shapes for elementwise operations. For more background on broadcasting, see:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">Numpy: Broadcasting</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.broadcast_dynamic_shape</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.broadcast_to</span></code></p></li>
</ul>
<p>The basic steps for broadcasting two inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> to have compatible shapes are:</p>
<ol class="arabic simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> do not have the same number of dimensions, then add outer dimensions (with size 1) until they do.</p></li>
<li><p>For each dimension where <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> have different sizes:</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">x</span></code> or <code class="docutils literal notranslate"><span class="pre">y</span></code> have size <code class="docutils literal notranslate"><span class="pre">1</span></code> in dimension <code class="docutils literal notranslate"><span class="pre">d</span></code>, then repeat its values across dimension <code class="docutils literal notranslate"><span class="pre">d</span></code> to match the other input’s size.</p></li>
<li><p>Otherwise, raise an exception (<code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are not broadcast compatible).</p></li>
</ul>
</li>
</ol>
<p>Where the size of a tensor in a uniform dimension is a single number (the size of slices across that dimension); and the size of a tensor in a ragged dimension is a list of slice lengths (for all slices across that dimension).</p>
<div class="section" id="Broadcasting-examples">
<h4>Broadcasting examples<a class="headerlink" href="#Broadcasting-examples" title="Enlazar permanentemente con este título">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x       (2D ragged):  2 x (num_rows)
# y       (scalar)
# result  (2D ragged):  2 x (num_rows)
x = tf.ragged.constant([[1, 2], [3]])
y = 3
print(x + y)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x         (2d ragged):  3 x (num_rows)
# y         (2d tensor):  3 x          1
# Result    (2d ragged):  3 x (num_rows)
x = tf.ragged.constant(
   [[10, 87, 12],
    [19, 53],
    [12, 32]])
y = [[1000], [2000], [3000]]
print(x + y)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x      (3d ragged):  2 x (r1) x 2
# y      (2d ragged):         1 x 1
# Result (3d ragged):  2 x (r1) x 2
x = tf.ragged.constant(
    [[[1, 2], [3, 4], [5, 6]],
     [[7, 8]]],
    ragged_rank=1)
y = tf.constant([[10]])
print(x + y)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x      (3d ragged):  2 x (r1) x (r2) x 1
# y      (1d tensor):                    3
# Result (3d ragged):  2 x (r1) x (r2) x 3
x = tf.ragged.constant(
    [
        [
            [[1], [2]],
            [],
            [[3]],
            [[4]],
        ],
        [
            [[5], [6]],
            [[7]]
        ]
    ],
    ragged_rank=2)
y = tf.constant([10, 20, 30])
print(x + y)
</pre></div>
</div>
</div>
<p>Here are some examples of shapes that do not broadcast:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x      (2d ragged): 3 x (r1)
# y      (2d tensor): 3 x    4  # trailing dimensions do not match
x = tf.ragged.constant([[1, 2], [3, 4, 5, 6], [7]])
y = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
try:
  x + y
except tf.errors.InvalidArgumentError as exception:
  print(exception)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x      (2d ragged): 3 x (r1)
# y      (2d ragged): 3 x (r2)  # ragged dimensions do not match.
x = tf.ragged.constant([[1, 2, 3], [4], [5, 6]])
y = tf.ragged.constant([[10, 20], [30, 40], [50]])
try:
  x + y
except tf.errors.InvalidArgumentError as exception:
  print(exception)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># x      (3d ragged): 3 x (r1) x 2
# y      (3d ragged): 3 x (r1) x 3  # trailing dimensions do not match
x = tf.ragged.constant([[[1, 2], [3, 4], [5, 6]],
                        [[7, 8], [9, 10]]])
y = tf.ragged.constant([[[1, 2, 0], [3, 4, 0], [5, 6, 0]],
                        [[7, 8, 0], [9, 10, 0]]])
try:
  x + y
except tf.errors.InvalidArgumentError as exception:
  print(exception)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="RaggedTensor-encoding">
<h3>RaggedTensor encoding<a class="headerlink" href="#RaggedTensor-encoding" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Ragged tensors are encoded using the <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> class. Internally, each <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> consists of:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">values</span></code> tensor, which concatenates the variable-length rows into a flattened list.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">row_partition</span></code>, which indicates how those flattened values are divided into rows.</p></li>
</ul>
<p><img alt="ragged_encoding_2" src="https://www.tensorflow.org/images/ragged_tensors/ragged_encoding_2.png" /></p>
<p>The <code class="docutils literal notranslate"><span class="pre">row_partition</span></code> can be stored using four different encodings:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">row_splits</span></code> is an integer vector specifying the split points between rows.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">value_rowids</span></code> is an integer vector specifying the row index for each value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">row_lengths</span></code> is an integer vector specifying the length of each row.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">uniform_row_length</span></code> is an integer scalar specifying a single length for all rows.</p></li>
</ul>
<p><img alt="partition_encodings" src="https://www.tensorflow.org/images/ragged_tensors/partition_encodings.png" /></p>
<p>An integer scalar <code class="docutils literal notranslate"><span class="pre">nrows</span></code> can also be included in the <code class="docutils literal notranslate"><span class="pre">row_partition</span></code> encoding, to account for empty trailing rows with <code class="docutils literal notranslate"><span class="pre">value_rowids</span></code>, or empty rows with <code class="docutils literal notranslate"><span class="pre">uniform_row_length</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.RaggedTensor.from_row_splits(
    values=[3, 1, 4, 1, 5, 9, 2],
    row_splits=[0, 4, 4, 6, 7])
print(rt)
</pre></div>
</div>
</div>
<p>The choice of which encoding to use for row partitions is managed internally by ragged tensors, to improve efficiency in some contexts. In particular, some of the advantages and disadvantages of the different row-partitioning schemes are:</p>
<ul class="simple">
<li><p><strong>Efficient indexing</strong>: The <code class="docutils literal notranslate"><span class="pre">row_splits</span></code> encoding enables constant-time indexing and slicing into ragged tensors.</p></li>
<li><p><strong>Efficient concatenation</strong>: The <code class="docutils literal notranslate"><span class="pre">row_lengths</span></code> encoding is more efficient when concatenating ragged tensors, since row lengths do not change when two tensors are concatenated together.</p></li>
<li><p><strong>Small encoding size</strong>: The <code class="docutils literal notranslate"><span class="pre">value_rowids</span></code> encoding is more efficient when storing ragged tensors that have a large number of empty rows, since the size of the tensor depends only on the total number of values. On the other hand, the <code class="docutils literal notranslate"><span class="pre">row_splits</span></code> and <code class="docutils literal notranslate"><span class="pre">row_lengths</span></code> encodings are more efficient when storing ragged tensors with longer rows, since they require only one scalar value for each row.</p></li>
<li><p><strong>Compatibility</strong>: The <code class="docutils literal notranslate"><span class="pre">value_rowids</span></code> scheme matches the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/math#about_segmentation">segmentation</a> format used by operations such as <code class="docutils literal notranslate"><span class="pre">tf.segment_sum</span></code>. The <code class="docutils literal notranslate"><span class="pre">row_limits</span></code> scheme matches the format used by ops such as <code class="docutils literal notranslate"><span class="pre">tf.sequence_mask</span></code>.</p></li>
<li><p><strong>Uniform dimensions</strong>: As discussed below, the <code class="docutils literal notranslate"><span class="pre">uniform_row_length</span></code> encoding is used to encode ragged tensors with uniform dimensions.</p></li>
</ul>
<div class="section" id="Multiple-ragged-dimensions">
<h4>Multiple ragged dimensions<a class="headerlink" href="#Multiple-ragged-dimensions" title="Enlazar permanentemente con este título">¶</a></h4>
<p>A ragged tensor with multiple ragged dimensions is encoded by using a nested <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> for the <code class="docutils literal notranslate"><span class="pre">values</span></code> tensor. Each nested <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> adds a single ragged dimension.</p>
<p><img alt="ragged_rank_2" src="https://www.tensorflow.org/images/ragged_tensors/ragged_rank_2.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.RaggedTensor.from_row_splits(
    values=tf.RaggedTensor.from_row_splits(
        values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
        row_splits=[0, 3, 3, 5, 9, 10]),
    row_splits=[0, 1, 1, 5])
print(rt)
print(&quot;Shape: {}&quot;.format(rt.shape))
print(&quot;Number of partitioned dimensions: {}&quot;.format(rt.ragged_rank))
</pre></div>
</div>
</div>
<p>The factory function <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor.from_nested_row_splits</span></code> may be used to construct a RaggedTensor with multiple ragged dimensions directly, by providing a list of <code class="docutils literal notranslate"><span class="pre">row_splits</span></code> tensors:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.RaggedTensor.from_nested_row_splits(
    flat_values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
    nested_row_splits=([0, 1, 1, 5], [0, 3, 3, 5, 9, 10]))
print(rt)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Ragged-rank-and-flat-values">
<h4>Ragged rank and flat values<a class="headerlink" href="#Ragged-rank-and-flat-values" title="Enlazar permanentemente con este título">¶</a></h4>
<p>A ragged tensor’s <strong>ragged rank</strong> is the number of times that the underlying <code class="docutils literal notranslate"><span class="pre">values</span></code> Tensor has been partitioned (i.e., the nesting depth of <code class="docutils literal notranslate"><span class="pre">RaggedTensor</span></code> objects). The innermost <code class="docutils literal notranslate"><span class="pre">values</span></code> tensor is known as its <strong>flat_values</strong>. In the following example, <code class="docutils literal notranslate"><span class="pre">conversations</span></code> has ragged_rank=3, and its <code class="docutils literal notranslate"><span class="pre">flat_values</span></code> is a 1D <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with 24 strings:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># shape = [batch, (paragraph), (sentence), (word)]
conversations = tf.ragged.constant(
    [[[[&quot;I&quot;, &quot;like&quot;, &quot;ragged&quot;, &quot;tensors.&quot;]],
      [[&quot;Oh&quot;, &quot;yeah?&quot;], [&quot;What&quot;, &quot;can&quot;, &quot;you&quot;, &quot;use&quot;, &quot;them&quot;, &quot;for?&quot;]],
      [[&quot;Processing&quot;, &quot;variable&quot;, &quot;length&quot;, &quot;data!&quot;]]],
     [[[&quot;I&quot;, &quot;like&quot;, &quot;cheese.&quot;], [&quot;Do&quot;, &quot;you?&quot;]],
      [[&quot;Yes.&quot;], [&quot;I&quot;, &quot;do.&quot;]]]])
conversations.shape
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>assert conversations.ragged_rank == len(conversations.nested_row_splits)
conversations.ragged_rank  # Number of partitioned dimensions.
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>conversations.flat_values.numpy()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Uniform-inner-dimensions">
<h4>Uniform inner dimensions<a class="headerlink" href="#Uniform-inner-dimensions" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Ragged tensors with uniform inner dimensions are encoded by using a multidimensional <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> for the flat_values (i.e., the innermost <code class="docutils literal notranslate"><span class="pre">values</span></code>).</p>
<p><img alt="uniform_inner" src="https://www.tensorflow.org/images/ragged_tensors/uniform_inner.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.RaggedTensor.from_row_splits(
    values=[[1, 3], [0, 0], [1, 3], [5, 3], [3, 3], [1, 2]],
    row_splits=[0, 3, 4, 6])
print(rt)
print(&quot;Shape: {}&quot;.format(rt.shape))
print(&quot;Number of partitioned dimensions: {}&quot;.format(rt.ragged_rank))
print(&quot;Flat values shape: {}&quot;.format(rt.flat_values.shape))
print(&quot;Flat values:\n{}&quot;.format(rt.flat_values))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Uniform-non-inner-dimensions">
<h4>Uniform non-inner dimensions<a class="headerlink" href="#Uniform-non-inner-dimensions" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Ragged tensors with uniform non-inner dimensions are encoded by partitioning rows with <code class="docutils literal notranslate"><span class="pre">uniform_row_length</span></code>.</p>
<p><img alt="uniform_outer" src="https://www.tensorflow.org/images/ragged_tensors/uniform_outer.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rt = tf.RaggedTensor.from_uniform_row_length(
    values=tf.RaggedTensor.from_row_splits(
        values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
        row_splits=[0, 3, 5, 9, 10]),
    uniform_row_length=2)
print(rt)
print(&quot;Shape: {}&quot;.format(rt.shape))
print(&quot;Number of partitioned dimensions: {}&quot;.format(rt.ragged_rank))
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>