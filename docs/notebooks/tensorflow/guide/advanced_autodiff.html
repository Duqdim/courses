

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2020 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2020 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/advanced_autodiff.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2020-The-TensorFlow-Authors.">
<h1>Copyright 2020 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2020-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Advanced-Automatic-Differentiation">
<h2>Advanced Automatic Differentiation<a class="headerlink" href="#Advanced-Automatic-Differentiation" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>c6c860206fcc49adb9c512f8bfe36357|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>5b094df83f29443b9afaa1af1badc0ad|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>e72e7c37445a4457924d1a3984dad5b3|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>bd09006ad5b14e66bcb8e4f4cdece6a7|Download notebook</p>
</td></table><p>The <a class="reference internal" href="autodiff.html"><span class="doc">automatic differentiation guide</span></a> includes everything required to calculate gradients. This guide focuses on deeper, less common features of the <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> api.</p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow as tf

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rcParams[&#39;figure.figsize&#39;] = (8, 6)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Controlling-gradient-recording">
<h3>Controlling gradient recording<a class="headerlink" href="#Controlling-gradient-recording" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In the <a class="reference internal" href="autodiff.html"><span class="doc">automatic differentiation guide</span></a> you saw how to control which variables and tensors are watched by the tape while building the gradient calculation.</p>
<p>The tape also has methods to manipulate the recording.</p>
<p>If you wish to stop recording gradients, you can use <code class="docutils literal notranslate"><span class="pre">GradientTape.stop_recording()</span></code> to temporarily suspend recording.</p>
<p>This may be useful to reduce overhead if you do not wish to differentiate a complicated operation in the middle of your model. This could include calculating a metric or an intermediate result:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.0)
y = tf.Variable(3.0)

with tf.GradientTape() as t:
  x_sq = x * x
  with t.stop_recording():
    y_sq = y * y
  z = x_sq + y_sq

grad = t.gradient(z, {&#39;x&#39;: x, &#39;y&#39;: y})

print(&#39;dz/dx:&#39;, grad[&#39;x&#39;])  # 2*x =&gt; 4
print(&#39;dz/dy:&#39;, grad[&#39;y&#39;])
</pre></div>
</div>
</div>
<p>If you wish to start over entirely, use <code class="docutils literal notranslate"><span class="pre">reset()</span></code>. Simply exiting the gradient tape block and restarting is usually easier to read, but you can use <code class="docutils literal notranslate"><span class="pre">reset</span></code> when exiting the tape block is difficult or impossible.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.0)
y = tf.Variable(3.0)
reset = True

with tf.GradientTape() as t:
  y_sq = y * y
  if reset:
    # Throw out all the tape recorded so far
    t.reset()
  z = x * x + y_sq

grad = t.gradient(z, {&#39;x&#39;: x, &#39;y&#39;: y})

print(&#39;dz/dx:&#39;, grad[&#39;x&#39;])  # 2*x =&gt; 4
print(&#39;dz/dy:&#39;, grad[&#39;y&#39;])
</pre></div>
</div>
</div>
</div>
<div class="section" id="Stop-gradient">
<h3>Stop gradient<a class="headerlink" href="#Stop-gradient" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In contrast to the global tape controls above, the <code class="docutils literal notranslate"><span class="pre">tf.stop_gradient</span></code> function is much more precise. It can be used to stop gradients from flowing along a particular path, without needing access to the tape itself:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.0)
y = tf.Variable(3.0)

with tf.GradientTape() as t:
  y_sq = y**2
  z = x**2 + tf.stop_gradient(y_sq)

grad = t.gradient(z, {&#39;x&#39;: x, &#39;y&#39;: y})

print(&#39;dz/dx:&#39;, grad[&#39;x&#39;])  # 2*x =&gt; 4
print(&#39;dz/dy:&#39;, grad[&#39;y&#39;])
</pre></div>
</div>
</div>
</div>
<div class="section" id="Custom-gradients">
<h3>Custom gradients<a class="headerlink" href="#Custom-gradients" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In some cases, you may want to control exactly how gradients are calculated rather than using the default. These situations include:</p>
<ul class="simple">
<li><p>There is no defined gradient for a new op you are writing.</p></li>
<li><p>The default calculations are numerically unstable.</p></li>
<li><p>You wish to cache an expensive computation from the forward pass.</p></li>
<li><p>You want to modify a value (for example using: <code class="docutils literal notranslate"><span class="pre">tf.clip_by_value</span></code>, <code class="docutils literal notranslate"><span class="pre">tf.math.round</span></code>) without modifying the gradient.</p></li>
</ul>
<p>For writing a new op, you can use <code class="docutils literal notranslate"><span class="pre">tf.RegisterGradient</span></code> to set up your own. See that page for details. (Note that the gradient registry is global, so change it with caution.)</p>
<p>For the latter three cases, you can use <code class="docutils literal notranslate"><span class="pre">tf.custom_gradient</span></code>.</p>
<p>Here is an example that applies <code class="docutils literal notranslate"><span class="pre">tf.clip_by_norm</span></code> to the intermediate gradient.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Establish an identity operation, but clip during the gradient pass
@tf.custom_gradient
def clip_gradients(y):
  def backward(dy):
    return tf.clip_by_norm(dy, 0.5)
  return y, backward

v = tf.Variable(2.0)
with tf.GradientTape() as t:
  output = clip_gradients(v * v)
print(t.gradient(output, v))  # calls &quot;backward&quot;, which clips 4 to 2

</pre></div>
</div>
</div>
<p>See the <code class="docutils literal notranslate"><span class="pre">tf.custom_gradient</span></code> decorator for more details.</p>
</div>
<div class="section" id="Multiple-tapes">
<h3>Multiple tapes<a class="headerlink" href="#Multiple-tapes" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Multiple tapes interact seamlessly. For example, here each tape watches a different set of tensors:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x0 = tf.constant(0.0)
x1 = tf.constant(0.0)

with tf.GradientTape() as tape0, tf.GradientTape() as tape1:
  tape0.watch(x0)
  tape1.watch(x1)

  y0 = tf.math.sin(x0)
  y1 = tf.nn.sigmoid(x1)

  y = y0 + y1

  ys = tf.reduce_sum(y)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tape0.gradient(ys, x0).numpy()   # cos(x) =&gt; 1.0
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) =&gt; 0.25
</pre></div>
</div>
</div>
<div class="section" id="Higher-order-gradients">
<h4>Higher-order gradients<a class="headerlink" href="#Higher-order-gradients" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Operations inside of the <code class="docutils literal notranslate"><span class="pre">GradientTape</span></code> context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t2:
  with tf.GradientTape() as t1:
    y = x * x * x

  # Compute the gradient inside the outer `t2` context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t1.gradient(y, x)
d2y_dx2 = t2.gradient(dy_dx, x)

print(&#39;dy_dx:&#39;, dy_dx.numpy())  # 3 * x**2 =&gt; 3.0
print(&#39;d2y_dx2:&#39;, d2y_dx2.numpy())  # 6 * x =&gt; 6.0
</pre></div>
</div>
</div>
<p>While that does give you the second derivative of a <em>scalar</em> function, this pattern does not generalize to produce a Hessian matrix, since <code class="docutils literal notranslate"><span class="pre">GradientTape.gradient</span></code> only computes the gradient of a scalar. To construct a Hessian, see the <a class="reference external" href="#hessian">Hessian example</a> under the <a class="reference external" href="#jacobians">Jacobian section</a>.</p>
<p>“Nested calls to <code class="docutils literal notranslate"><span class="pre">GradientTape.gradient</span></code>” is a good pattern when you are calculating a scalar from a gradient, and then the resulting scalar acts as a source for a second gradient calculation, as in the following example.</p>
<div class="section" id="Example:-Input-gradient-regularization">
<h5>Example: Input gradient regularization<a class="headerlink" href="#Example:-Input-gradient-regularization" title="Enlazar permanentemente con este título">¶</a></h5>
<p>Many models are susceptible to “adversarial examples”. This collection of techniques modifies the model’s input to confuse the model’s output. The <a class="reference external" href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">simplest implementation</a> takes a single step along the gradient of the output with respect to the input; the “input gradient”.</p>
<p>One technique to increase robustness to adversarial examples is <a class="reference external" href="https://arxiv.org/abs/1905.11468">input gradient regularization</a>, which attempts to minimize the magnitude of the input gradient. If the input gradient is small, then the change in the output should be small too.</p>
<p>Below is a naive implementation of input gradient regularization. The implementation is:</p>
<ol class="arabic simple">
<li><p>Calculate the gradient of the output with respect to the input using an inner tape.</p></li>
<li><p>Calculate the magnitude of that input gradient.</p></li>
<li><p>Calculate the gradient of that magnitude with respect to the model.</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.random.normal([7, 5])

layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>with tf.GradientTape() as t2:
  # The inner tape only takes the gradient with respect to the input,
  # not the variables.
  with tf.GradientTape(watch_accessed_variables=False) as t1:
    t1.watch(x)
    y = layer(x)
    out = tf.reduce_sum(layer(x)**2)
  # 1. Calculate the input gradient.
  g1 = t1.gradient(out, x)
  # 2. Calculate the magnitude of the input gradient.
  g1_mag = tf.norm(g1)

# 3. Calculate the gradient of the magnitude with respect to the model.
dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[var.shape for var in dg1_mag]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="Jacobians">
<h3>Jacobians<a class="headerlink" href="#Jacobians" title="Enlazar permanentemente con este título">¶</a></h3>
<p>All the previous examples took the gradients of a scalar target with respect to some source tensor(s).</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian matrix</a> represents the gradients of a vector valued function. Each row contains the gradient of one of the vector’s elements.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">GradientTape.jacobian</span></code> method allows you to efficiently calculate a Jacobian matrix.</p>
<p>Note that:</p>
<ul class="simple">
<li><p>Like <code class="docutils literal notranslate"><span class="pre">gradient</span></code>: The <code class="docutils literal notranslate"><span class="pre">sources</span></code> argument can be a tensor or a container of tensors.</p></li>
<li><p>Unlike <code class="docutils literal notranslate"><span class="pre">gradient</span></code>: The <code class="docutils literal notranslate"><span class="pre">target</span></code> tensor must be a single tensor.</p></li>
</ul>
<div class="section" id="Scalar-source">
<h4>Scalar source<a class="headerlink" href="#Scalar-source" title="Enlazar permanentemente con este título">¶</a></h4>
<p>As a first example, here is the Jacobian of a vector-target with respect to a scalar-source.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.linspace(-10.0, 10.0, 200+1)
delta = tf.Variable(0.0)

with tf.GradientTape() as tape:
  y = tf.nn.sigmoid(x+delta)

dy_dx = tape.jacobian(y, delta)
</pre></div>
</div>
</div>
<p>When you take the Jacobian with respect to a scalar the result has the shape of the <strong>target</strong>, and gives the gradient of the each element with respect to the source:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(y.shape)
print(dy_dx.shape)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.plot(x.numpy(), y, label=&#39;y&#39;)
plt.plot(x.numpy(), dy_dx, label=&#39;dy/dx&#39;)
plt.legend()
_ = plt.xlabel(&#39;x&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Tensor-source">
<h4>Tensor source<a class="headerlink" href="#Tensor-source" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Whether the input is scalar or tensor, <code class="docutils literal notranslate"><span class="pre">GradientTape.jacobian</span></code> efficiently calculates the gradient of each element of the source with respect to each element of the target(s).</p>
<p>For example, the output of this layer has a shape of <code class="docutils literal notranslate"><span class="pre">(10,</span> <span class="pre">7)</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.random.normal([7, 5])
layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)

with tf.GradientTape(persistent=True) as tape:
  y = layer(x)

y.shape
</pre></div>
</div>
</div>
<p>And the layer’s kernel’s shape is <code class="docutils literal notranslate"><span class="pre">(5,</span> <span class="pre">10)</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>layer.kernel.shape
</pre></div>
</div>
</div>
<p>The shape of the Jacobian of the output with respect to the kernel is those two shapes concatenated together:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>j = tape.jacobian(y, layer.kernel)
j.shape
</pre></div>
</div>
</div>
<p>If you sum over the target’s dimensions, you’re left with the gradient of the sum that would have been calculated by <code class="docutils literal notranslate"><span class="pre">GradientTape.gradient</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>g = tape.gradient(y, layer.kernel)
print(&#39;g.shape:&#39;, g.shape)

j_sum = tf.reduce_sum(j, axis=[0, 1])
delta = tf.reduce_max(abs(g - j_sum)).numpy()
assert delta &lt; 1e-3
print(&#39;delta:&#39;, delta)
</pre></div>
</div>
</div>
<div class="section" id="Example:-Hessian">
<h5>Example: Hessian<a class="headerlink" href="#Example:-Hessian" title="Enlazar permanentemente con este título">¶</a></h5>
<p>While <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> doesn’t give an explicit method for constructing a Hessian matrix it’s possible to build one using the <code class="docutils literal notranslate"><span class="pre">GradientTape.jacobian</span></code> method.</p>
<p>Note: The Hessian matrix contains <code class="docutils literal notranslate"><span class="pre">N**2</span></code> parameters. For this and other reasons it is not practical for most models. This example is included more as a demonstration of how to use the <code class="docutils literal notranslate"><span class="pre">GradientTape.jacobian</span></code> method, and is not an endorsement of direct Hessian-based optimization. A Hessian-vector product can be <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/hvp_test.py">calculated efficiently with nested tapes</a>, and is a much more
efficient approach to second-order optimization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.random.normal([7, 5])
layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)
layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)

with tf.GradientTape() as t2:
  with tf.GradientTape() as t1:
    x = layer1(x)
    x = layer2(x)
    loss = tf.reduce_mean(x**2)

  g = t1.gradient(loss, layer1.kernel)

h = t2.jacobian(g, layer1.kernel)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(f&#39;layer.kernel.shape: {layer1.kernel.shape}&#39;)
print(f&#39;h.shape: {h.shape}&#39;)
</pre></div>
</div>
</div>
<p>To use this Hessian for a Newton’s method step, you would first flatten out its axes into a matrix, and flatten out the gradient into a vector:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>n_params = tf.reduce_prod(layer1.kernel.shape)

g_vec = tf.reshape(g, [n_params, 1])
h_mat = tf.reshape(h, [n_params, n_params])
</pre></div>
</div>
</div>
<p>The Hessian matrix should be symmetric:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def imshow_zero_center(image, **kwargs):
  lim = tf.reduce_max(abs(image))
  plt.imshow(image, vmin=-lim, vmax=lim, cmap=&#39;seismic&#39;, **kwargs)
  plt.colorbar()
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>imshow_zero_center(h_mat)
</pre></div>
</div>
</div>
<p>The Newton’s method update step is shown below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>eps = 1e-3
eye_eps = tf.eye(h_mat.shape[0])*eps
</pre></div>
</div>
</div>
<p>Note: <a class="reference external" href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">Don’t actually invert the matrix</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))
# h_mat = ∇²f(X(k))
# g_vec = ∇f(X(k))
update = tf.linalg.solve(h_mat + eye_eps, g_vec)

# Reshape the update and apply it to the variable.
_ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))
</pre></div>
</div>
</div>
<p>While this is relatively simple for a single <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>, applying this to a non-trivial model would require careful concatenation and slicing to produce a full Hessian across multiple variables.</p>
</div>
</div>
<div class="section" id="Batch-Jacobian">
<h4>Batch Jacobian<a class="headerlink" href="#Batch-Jacobian" title="Enlazar permanentemente con este título">¶</a></h4>
<p>In some cases, you want to take the Jacobian of each of a stack of targets with respect to a stack of sources, where the Jacobians for each target-source pair are independent.</p>
<p>For example, here the input <code class="docutils literal notranslate"><span class="pre">x</span></code> is shaped <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">ins)</span></code> and the output <code class="docutils literal notranslate"><span class="pre">y</span></code> is shaped <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">outs)</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.random.normal([7, 5])

layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)
layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)

with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:
  tape.watch(x)
  y = layer1(x)
  y = layer2(y)

y.shape
</pre></div>
</div>
</div>
<p>The full Jacobian of <code class="docutils literal notranslate"><span class="pre">y</span></code> with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code> has a shape of <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">ins,</span> <span class="pre">batch,</span> <span class="pre">outs)</span></code>, even if you only want <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">ins,</span> <span class="pre">outs)</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>j = tape.jacobian(y, x)
j.shape
</pre></div>
</div>
</div>
<p>If the gradients of each item in the stack are independent, then every <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">batch)</span></code> slice of this tensor is a diagonal matrix:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>imshow_zero_center(j[:, 0, :, 0])
_ = plt.title(&#39;A (batch, batch) slice&#39;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def plot_as_patches(j):
  # Reorder axes so the diagonals will each form a contiguous patch.
  j = tf.transpose(j, [1, 0, 3, 2])
  # Pad in between each patch.
  lim = tf.reduce_max(abs(j))
  j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]],
             constant_values=-lim)
  # Reshape to form a single image.
  s = j.shape
  j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]])
  imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5])

plot_as_patches(j)
_ = plt.title(&#39;All (batch, batch) slices are diagonal&#39;)
</pre></div>
</div>
</div>
<p>To get the desired result you can sum over the duplicate <code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension, or else select the diagonals using <code class="docutils literal notranslate"><span class="pre">tf.einsum</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>j_sum = tf.reduce_sum(j, axis=2)
print(j_sum.shape)
j_select = tf.einsum(&#39;bxby-&gt;bxy&#39;, j)
print(j_select.shape)
</pre></div>
</div>
</div>
<p>It would be much more efficient to do the calculation without the extra dimension in the first place. The <code class="docutils literal notranslate"><span class="pre">GradientTape.batch_jacobian</span></code> method does exactly that.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>jb = tape.batch_jacobian(y, x)
jb.shape
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>error = tf.reduce_max(abs(jb - j_sum))
assert error &lt; 1e-3
print(error.numpy())
</pre></div>
</div>
</div>
<p>Caution: <code class="docutils literal notranslate"><span class="pre">GradientTape.batch_jacobian</span></code> only verifies that the first dimension of the source and target match. It doesn’t check that the gradients are actually independent. It’s up to the user to ensure they only use <code class="docutils literal notranslate"><span class="pre">batch_jacobian</span></code> where it makes sense. For example adding a <code class="docutils literal notranslate"><span class="pre">layers.BatchNormalization</span></code> destroys the independence, since it normalizes across the <code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.random.normal([7, 5])

layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)
bn = tf.keras.layers.BatchNormalization()
layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)

with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:
  tape.watch(x)
  y = layer1(x)
  y = bn(y, training=True)
  y = layer2(y)

j = tape.jacobian(y, x)
print(f&#39;j.shape: {j.shape}&#39;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plot_as_patches(j)

_ = plt.title(&#39;These slices are not diagonal&#39;)
_ = plt.xlabel(&quot;Don&#39;t use `batch_jacobian`&quot;)
</pre></div>
</div>
</div>
<p>In this case <code class="docutils literal notranslate"><span class="pre">batch_jacobian</span></code> still runs and returns <em>something</em> with the expected shape, but it’s contents have an unclear meaning.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>jb = tape.batch_jacobian(y, x)
print(f&#39;jb.shape: {jb.shape}&#39;)
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>