

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2018 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2018 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/tpu.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2018-The-TensorFlow-Authors.">
<h1>Copyright 2018 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2018-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Use-TPUs">
<h2>Use TPUs<a class="headerlink" href="#Use-TPUs" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>89b0b2336c5a4a9a938d19bb952af9c1|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>213bf9c52836458680b6e59a393479ea|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>4433266e81f9433aabae92e745153caa|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>7ff7ebe44d114dec8b7c80db37264fed|Download notebook</p>
</td></table><p>Experimental support for Cloud TPUs is currently available for Keras and Google Colab. Before you run this Colab notebooks, ensure that your hardware accelerator is a TPU by checking your notebook settings: Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; TPU.</p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow as tf

import os
import tensorflow_datasets as tfds
</pre></div>
</div>
</div>
</div>
<div class="section" id="TPU-Initialization">
<h3>TPU Initialization<a class="headerlink" href="#TPU-Initialization" title="Enlazar permanentemente con este título">¶</a></h3>
<p>TPUs are usually on Cloud TPU workers which are different from the local process running the user python program. Thus some initialization work needs to be done to connect to the remote cluster and initialize TPUs. Note that the <code class="docutils literal notranslate"><span class="pre">tpu</span></code> argument to <code class="docutils literal notranslate"><span class="pre">TPUClusterResolver</span></code> is a special address just for Colab. In the case that you are running on Google Compute Engine (GCE), you should instead pass in the name of your CloudTPU.</p>
<p>Note: The TPU initialization code has to be at the beginning of your program.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=&#39;&#39;)
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(&quot;All devices: &quot;, tf.config.list_logical_devices(&#39;TPU&#39;))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Manual-device-placement">
<h3>Manual device placement<a class="headerlink" href="#Manual-device-placement" title="Enlazar permanentemente con este título">¶</a></h3>
<p>After the TPU is initialized, you can use manual device placement to place the computation on a single TPU device.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
with tf.device(&#39;/TPU:0&#39;):
  c = tf.matmul(a, b)
print(&quot;c device: &quot;, c.device)
print(c)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Distribution-strategies">
<h3>Distribution strategies<a class="headerlink" href="#Distribution-strategies" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Most times users want to run the model on multiple TPUs in a data parallel way. A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs. Simply swap out the distribution strategy and the model will run on the given device. See the <a class="reference internal" href="distributed_training.html"><span class="doc">distribution strategy guide</span></a> for more information.</p>
<p>First, creates the <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code> object.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>strategy = tf.distribute.TPUStrategy(resolver)
</pre></div>
</div>
</div>
<p>To replicate a computation so it can run in all TPU cores, you can simply pass it to <code class="docutils literal notranslate"><span class="pre">strategy.run</span></code> API. Below is an example that all the cores will obtain the same inputs <code class="docutils literal notranslate"><span class="pre">(a,</span> <span class="pre">b)</span></code>, and do the matmul on each core independently. The outputs will be the values from all the replicas.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def matmul_fn(x, y):
  z = tf.matmul(x, y)
  return z

z = strategy.run(matmul_fn, args=(a, b))
print(z)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Classification-on-TPUs">
<h3>Classification on TPUs<a class="headerlink" href="#Classification-on-TPUs" title="Enlazar permanentemente con este título">¶</a></h3>
<p>As we have learned the basic concepts, it is time to look at a more concrete example. This guide demonstrates how to use the distribution strategy <code class="docutils literal notranslate"><span class="pre">tf.distribute.TPUStrategy</span></code> to drive a Cloud TPU and train a Keras model.</p>
<div class="section" id="Define-a-Keras-model">
<h4>Define a Keras model<a class="headerlink" href="#Define-a-Keras-model" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Below is the definition of MNIST model using Keras, unchanged from what you would use on CPU or GPU. Note that Keras model creation needs to be inside <code class="docutils literal notranslate"><span class="pre">strategy.scope</span></code>, so the variables can be created on each TPU device. Other parts of the code is not necessary to be inside the strategy scope.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def create_model():
  return tf.keras.Sequential(
      [tf.keras.layers.Conv2D(256, 3, activation=&#39;relu&#39;, input_shape=(28, 28, 1)),
       tf.keras.layers.Conv2D(256, 3, activation=&#39;relu&#39;),
       tf.keras.layers.Flatten(),
       tf.keras.layers.Dense(256, activation=&#39;relu&#39;),
       tf.keras.layers.Dense(128, activation=&#39;relu&#39;),
       tf.keras.layers.Dense(10)])
</pre></div>
</div>
</div>
</div>
<div class="section" id="Input-datasets">
<h4>Input datasets<a class="headerlink" href="#Input-datasets" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Efficient use of the <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> API is critical when using a Cloud TPU, as it is impossible to use the Cloud TPUs unless you can feed them data quickly enough. See <a class="reference internal" href="data_performance.html"><span class="doc">Input Pipeline Performance Guide</span></a> for details on dataset performance.</p>
<p>For all but the simplest experimentation (using <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.from_tensor_slices</span></code> or other in-graph data) you will need to store all data files read by the Dataset in Google Cloud Storage (GCS) buckets.</p>
<p>For most use-cases, it is recommended to convert your data into <code class="docutils literal notranslate"><span class="pre">TFRecord</span></code> format and use a <code class="docutils literal notranslate"><span class="pre">tf.data.TFRecordDataset</span></code> to read it. See <a class="reference external" href="../tutorials/load_data/tfrecord.ipynb">TFRecord and tf.Example tutorial</a> for details on how to do this. This, however, is not a hard requirement and you can use other dataset readers (<code class="docutils literal notranslate"><span class="pre">FixedLengthRecordDataset</span></code> or <code class="docutils literal notranslate"><span class="pre">TextLineDataset</span></code>) if you prefer.</p>
<p>Small datasets can be loaded entirely into memory using <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.cache</span></code>.</p>
<p>Regardless of the data format used, it is strongly recommended that you use large files, on the order of 100MB. This is especially important in this networked setting as the overhead of opening a file is significantly higher.</p>
<p>Here you should use the <code class="docutils literal notranslate"><span class="pre">tensorflow_datasets</span></code> module to get a copy of the MNIST training data. Note that <code class="docutils literal notranslate"><span class="pre">try_gcs</span></code> is specified to use a copy that is available in a public GCS bucket. If you don’t specify this, the TPU will not be able to access the data that is downloaded.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def get_dataset(batch_size, is_training=True):
  split = &#39;train&#39; if is_training else &#39;test&#39;
  dataset, info = tfds.load(name=&#39;mnist&#39;, split=split, with_info=True,
                            as_supervised=True, try_gcs=True)

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255.0

    return image, label

  dataset = dataset.map(scale)

  # Only shuffle and repeat the dataset in training. The advantage to have a
  # infinite dataset for training is to avoid the potential last partial batch
  # in each epoch, so users don&#39;t need to think about scaling the gradients
  # based on the actual batch size.
  if is_training:
    dataset = dataset.shuffle(10000)
    dataset = dataset.repeat()

  dataset = dataset.batch(batch_size)

  return dataset
</pre></div>
</div>
</div>
</div>
<div class="section" id="Train-a-model-using-Keras-high-level-APIs">
<h4>Train a model using Keras high level APIs<a class="headerlink" href="#Train-a-model-using-Keras-high-level-APIs" title="Enlazar permanentemente con este título">¶</a></h4>
<p>You can train a model simply with Keras fit/compile APIs. Nothing here is TPU specific, you would write the same code below if you had mutliple GPUs and where using a <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> rather than a <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>. To learn more, check out the <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/keras">Distributed training with Keras</a> tutorial.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>with strategy.scope():
  model = create_model()
  model.compile(optimizer=&#39;adam&#39;,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[&#39;sparse_categorical_accuracy&#39;])

batch_size = 200
steps_per_epoch = 60000 // batch_size
validation_steps = 10000 // batch_size

train_dataset = get_dataset(batch_size, is_training=True)
test_dataset = get_dataset(batch_size, is_training=False)

model.fit(train_dataset,
          epochs=5,
          steps_per_epoch=steps_per_epoch,
          validation_data=test_dataset,
          validation_steps=validation_steps)
</pre></div>
</div>
</div>
<p>To reduce python overhead, and maximize the performance of your TPU, try out the <strong>experimental</strong> <code class="docutils literal notranslate"><span class="pre">experimental_steps_per_execution</span></code> argument to <code class="docutils literal notranslate"><span class="pre">Model.compile</span></code>. Here it increases throughput by about 50%:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>with strategy.scope():
  model = create_model()
  model.compile(optimizer=&#39;adam&#39;,
                # Anything between 2 and `steps_per_epoch` could help here.
                experimental_steps_per_execution = 50,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[&#39;sparse_categorical_accuracy&#39;])

model.fit(train_dataset,
          epochs=5,
          steps_per_epoch=steps_per_epoch,
          validation_data=test_dataset,
          validation_steps=validation_steps)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Train-a-model-using-custom-training-loop.">
<h4>Train a model using custom training loop.<a class="headerlink" href="#Train-a-model-using-custom-training-loop." title="Enlazar permanentemente con este título">¶</a></h4>
<p>You can also create and train your models using <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> APIs directly. <code class="docutils literal notranslate"><span class="pre">strategy.experimental_distribute_datasets_from_function</span></code> API is used to distribute the dataset given a dataset function. Note that the batch size passed into the dataset will be per replica batch size instead of global batch size in this case. To learn more, check out the <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/custom_training">Custom training with tf.distribute.Strategy</a> tutorial.</p>
<p>First, create the model, datasets and tf.functions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Create the model, optimizer and metrics inside strategy scope, so that the
# variables can be mirrored on each device.
with strategy.scope():
  model = create_model()
  optimizer = tf.keras.optimizers.Adam()
  training_loss = tf.keras.metrics.Mean(&#39;training_loss&#39;, dtype=tf.float32)
  training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      &#39;training_accuracy&#39;, dtype=tf.float32)

# Calculate per replica batch size, and distribute the datasets on each TPU
# worker.
per_replica_batch_size = batch_size // strategy.num_replicas_in_sync

train_dataset = strategy.experimental_distribute_datasets_from_function(
    lambda _: get_dataset(per_replica_batch_size, is_training=True))

@tf.function
def train_step(iterator):
  &quot;&quot;&quot;The step function for one training step&quot;&quot;&quot;

  def step_fn(inputs):
    &quot;&quot;&quot;The computation to run on each TPU device.&quot;&quot;&quot;
    images, labels = inputs
    with tf.GradientTape() as tape:
      logits = model(images, training=True)
      loss = tf.keras.losses.sparse_categorical_crossentropy(
          labels, logits, from_logits=True)
      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
    training_loss.update_state(loss * strategy.num_replicas_in_sync)
    training_accuracy.update_state(labels, logits)

  strategy.run(step_fn, args=(next(iterator),))
</pre></div>
</div>
</div>
<p>Then run the training loop.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>steps_per_eval = 10000 // batch_size

train_iterator = iter(train_dataset)
for epoch in range(5):
  print(&#39;Epoch: {}/5&#39;.format(epoch))

  for step in range(steps_per_epoch):
    train_step(train_iterator)
  print(&#39;Current step: {}, training loss: {}, accuracy: {}%&#39;.format(
      optimizer.iterations.numpy(),
      round(float(training_loss.result()), 4),
      round(float(training_accuracy.result()) * 100, 2)))
  training_loss.reset_states()
  training_accuracy.reset_states()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Improving-performance-by-multiple-steps-within-tf.function">
<h4>Improving performance by multiple steps within <code class="docutils literal notranslate"><span class="pre">tf.function</span></code><a class="headerlink" href="#Improving-performance-by-multiple-steps-within-tf.function" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The performance can be improved by running multiple steps within a <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>. This is achieved by wrapping the <code class="docutils literal notranslate"><span class="pre">strategy.run</span></code> call with a <code class="docutils literal notranslate"><span class="pre">tf.range</span></code> inside <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>, AutoGraph will convert it to a <code class="docutils literal notranslate"><span class="pre">tf.while_loop</span></code> on the TPU worker.</p>
<p>Although with better performance, there are tradeoffs comparing with a single step inside <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>. Running multiple steps in a <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> is less flexible, you cannot run things eagerly or arbitrary python code within the steps.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def train_multiple_steps(iterator, steps):
  &quot;&quot;&quot;The step function for one training step&quot;&quot;&quot;

  def step_fn(inputs):
    &quot;&quot;&quot;The computation to run on each TPU device.&quot;&quot;&quot;
    images, labels = inputs
    with tf.GradientTape() as tape:
      logits = model(images, training=True)
      loss = tf.keras.losses.sparse_categorical_crossentropy(
          labels, logits, from_logits=True)
      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
    training_loss.update_state(loss * strategy.num_replicas_in_sync)
    training_accuracy.update_state(labels, logits)

  for _ in tf.range(steps):
    strategy.run(step_fn, args=(next(iterator),))

# Convert `steps_per_epoch` to `tf.Tensor` so the `tf.function` won&#39;t get
# retraced if the value changes.
train_multiple_steps(train_iterator, tf.convert_to_tensor(steps_per_epoch))

print(&#39;Current step: {}, training loss: {}, accuracy: {}%&#39;.format(
      optimizer.iterations.numpy(),
      round(float(training_loss.result()), 4),
      round(float(training_accuracy.result()) * 100, 2)))
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Next-steps">
<h3>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://cloud.google.com/tpu/docs/">Google Cloud TPU Documentation</a> - Set up and run a Google Cloud TPU.</p></li>
<li><p><a class="reference internal" href="distributed_training.html"><span class="doc">Distributed training with TensorFlow</span></a> - How to use distribution strategy and links to many example showing best practices.</p></li>
<li><p><a class="reference external" href="../tutorials/distribute/save_and_load.ipynb">Saving/Loading models with TensorFlow</a> - How to save and load models with distribution strategies.</p></li>
<li><p><a class="reference external" href="https://github.com/tensorflow/models/tree/master/official">TensorFlow Official Models</a> - Examples of state of the art TensorFlow 2.x models that are Cloud TPU compatible.</p></li>
<li><p><a class="reference external" href="https://cloud.google.com/tpu/docs/performance-guide">The Google Cloud TPU Performance Guide</a> - Enhance Cloud TPU performance further by adjusting Cloud TPU configuration parameters for your application.</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>