

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>&lt;no title&gt; &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-texto/index.html">Analítica de Texto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/data_performance_analysis.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p># Analyze <cite>tf.data</cite> performance with the TF Profiler</p>
<p>## Overview</p>
<p>This guide assumes familiarity with the TensorFlow
[Profiler](<a class="reference external" href="https://www.tensorflow.org/guide/profiler">https://www.tensorflow.org/guide/profiler</a>) and
[<cite>tf.data</cite>](<a class="reference external" href="https://www.tensorflow.org/guide/data">https://www.tensorflow.org/guide/data</a>). It aims to provide step by
step instructions with examples to help users diagnose and fix input pipeline
performance issues.</p>
<p>To begin, collect a profile of your TensorFlow job. Instructions on how to do so
are available for
[CPUs/GPUs](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#collect_performance_data">https://www.tensorflow.org/guide/profiler#collect_performance_data</a>)
and
[Cloud TPUs](<a class="reference external" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_profile">https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_profile</a>).</p>
<p>![TensorFlow Trace Viewer](images/data_performance_analysis/trace_viewer.png «The trace viewer page of the TensorFlow Profiler»)</p>
<p>The analysis workflow detailed below focuses on the trace viewer tool in the
Profiler. This tool displays a timeline that shows the duration of ops executed
by your TensorFlow program and allows you to identify which ops take the longest
to execute. For more information on the trace viewer, check out
[this section](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#trace_viewer">https://www.tensorflow.org/guide/profiler#trace_viewer</a>) of the TF
Profiler guide. In general, <cite>tf.data</cite> events will appear on the host CPU
timeline.</p>
<p>## Analysis Workflow</p>
<p>_Please follow the workflow below. If you have feedback to help us improve it,
please
[create a github issue](<a class="reference external" href="https://github.com/tensorflow/tensorflow/issues/new/choose">https://github.com/tensorflow/tensorflow/issues/new/choose</a>)
with the label “comp:data”._</p>
<p>### 1. Is your <cite>tf.data</cite> pipeline producing data fast enough?</p>
<p>Begin by ascertaining whether the input pipeline is the bottleneck for your
TensorFlow program.</p>
<p>To do so, look for <cite>IteratorGetNext::DoCompute</cite> ops in the trace viewer. In
general, you expect to see these at the start of a step. These slices represent
the time it takes for your input pipeline to yield a batch of elements when it
is requested. If you’re using keras or iterating over your dataset in a
<cite>tf.function</cite>, these should be found in <cite>tf_data_iterator_get_next</cite> threads.</p>
<p>Note that if you’re using a
[distribution strategy](<a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">https://www.tensorflow.org/guide/distributed_training</a>),
you may see <cite>IteratorGetNextAsOptional::DoCompute</cite> events instead of
<a href="#id1"><span class="problematic" id="id2">`</span></a>IteratorGetNext::DoCompute`(as of TF 2.3).</p>
<p>![image](images/data_performance_analysis/get_next_fast.png «If your IteratorGetNext::DoCompute calls return quickly, <cite>tf.data</cite> is not your bottleneck.»)</p>
<p><strong>If the calls return quickly (&lt;= 50 us),</strong> this means that your data is
available when it is requested. The input pipeline is not your bottleneck; see
the [Profiler guide](<a class="reference external" href="https://www.tensorflow.org/guide/profiler">https://www.tensorflow.org/guide/profiler</a>) for more generic
performance analysis tips.</p>
<p>![image](images/data_performance_analysis/get_next_slow.png «If your IteratorGetNext::DoCompute calls return slowly, <cite>tf.data</cite> is not producing data quickly enough.»)</p>
<p><strong>If the calls return slowly,</strong> <cite>tf.data</cite> is unable to keep up with the
consumer’s requests. Continue to the next section.</p>
<p>### 2. Are you prefetching data?</p>
<p>The best practice for input pipeline performance is to insert a
<cite>tf.data.Dataset.prefetch</cite> transformation at the end of your <cite>tf.data</cite> pipeline.
This transformation overlaps the input pipeline’s preprocessing computation with
the next step of model computation and is required for optimal input pipeline
performance when training your model. If you’re prefetching data, you should see
a <cite>Iterator::Prefetch</cite> slice on the same thread as the
<cite>IteratorGetNext::DoCompute</cite> op.</p>
<p>![image](images/data_performance_analysis/prefetch.png «If you’re prefetching data, you should see a <cite>Iterator::Prefetch</cite> slice in the same stack as the <cite>IteratorGetNext::DoCompute</cite> op.»)</p>
<p><strong>If you don’t have a `prefetch` at the end of your pipeline</strong>, you should add
one. For more information about <cite>tf.data</cite> performance recommendations, see the
[tf.data performance guide](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance#prefetching">https://www.tensorflow.org/guide/data_performance#prefetching</a>).</p>
<p><strong>If you’re already prefetching data</strong>, and the input pipeline is still your
bottleneck, continue to the next section to further analyze performance.</p>
<p>### 3. Are you reaching high CPU utilization?</p>
<p><cite>tf.data</cite> achieves high throughput by trying to make the best possible use of
available resources. In general, even when running your model on an accelerator
like a GPU or TPU, the <cite>tf.data</cite> pipelines are run on the CPU. You can check
your utilization with tools like [sar](<a class="reference external" href="https://linux.die.net/man/1/sar">https://linux.die.net/man/1/sar</a>) and
[htop](<a class="reference external" href="https://en.wikipedia.org/wiki/Htop">https://en.wikipedia.org/wiki/Htop</a>), or in the
[cloud monitoring console](<a class="reference external" href="https://cloud.google.com/monitoring/docs/monitoring_in_console">https://cloud.google.com/monitoring/docs/monitoring_in_console</a>) if you’re running on GCP.</p>
<p><strong>If your utilization is low,</strong> this suggests that your input pipeline may not
be taking full advantage of the host CPU. You should consult the
[tf.data performance guide](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance">https://www.tensorflow.org/guide/data_performance</a>)
for best practices. If you have applied the best practices and utilization and
throughput remain low, continue to [Bottleneck analysis](#4_bottleneck_analysis)
below.</p>
<p><strong>If your utilization is approaching the resource limit</strong>, in order to improve
performance further, you need to either improve the efficiency of your input
pipeline (for example, avoiding unnecessary computation) or offload computation.</p>
<p>You can improve the efficiency of your input pipeline by avoiding unnecessary
computation in <cite>tf.data</cite>. One way of doing this is inserting a
[<cite>tf.data.Dataset.cache</cite>](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance#caching">https://www.tensorflow.org/guide/data_performance#caching</a>)
transformation after computation-intensive work if your data fits into memory;
this reduces computation at the cost of increased memory usage. Additionally,
disabling intra-op parallelism in <cite>tf.data</cite> has the potential to increase
efficiency by &gt; 10%, and can be done by setting the following option on your
input pipeline:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">...</span>
<span class="pre">options</span> <span class="pre">=</span> <span class="pre">tf.data.Options()</span>
<span class="pre">options.experimental_threading.max_intra_op_parallelism</span> <span class="pre">=</span> <span class="pre">1</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.with_options(options)</span>
<span class="pre">`</span></code></p>
<p>### 4. Bottleneck Analysis</p>
<p>The following section walks through how to read <cite>tf.data</cite> events in the trace
viewer to understand where the bottleneck is and possible mitigation strategies.</p>
<p>#### Understanding <cite>tf.data</cite> events in the Profiler</p>
<p>Each <cite>tf.data</cite> event in the Profiler has the name <cite>Iterator::&lt;Dataset&gt;</cite>, where
<cite>&lt;Dataset&gt;</cite> is the name of the dataset source or transformation. Each event also
has the long name <cite>Iterator::&lt;Dataset_1&gt;::…::&lt;Dataset_n&gt;</cite>, which you can see
by clicking on the <cite>tf.data</cite> event. In the long name, <cite>&lt;Dataset_n&gt;</cite> matches
<cite>&lt;Dataset&gt;</cite> from the (short) name, and the other datasets in the long name
represent downstream transformations.</p>
<p>![image](images/data_performance_analysis/map_long_name.png «tf.data.Dataset.range(10).map(lambda x: x).repeat(2).batch(5)»)</p>
<p>For example, the above screenshot was generated from the following code:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">tf.data.Dataset.range(10)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.map(lambda</span> <span class="pre">x:</span> <span class="pre">x)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.repeat(2)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.batch(5)</span>
<span class="pre">`</span></code></p>
<p>Here, the <cite>Iterator::Map</cite> event has the long name
<cite>Iterator::BatchV2::FiniteRepeat::Map</cite>. Note that the datasets name may differ
slightly from the python API (for example, FiniteRepeat instead of Repeat), but
should be intuitive enough to parse.</p>
<p>##### Synchronous and asynchronous transformations</p>
<p>For synchronous <cite>tf.data</cite> transformations (such as <cite>Batch</cite> and <cite>Map</cite>), you will
see events from upstream transformations on the same thread. In the above
example, since all the transformations used are synchronous, all the events
appear on the same thread.</p>
<p>For asynchronous transformations (such as <cite>Prefetch</cite>, <cite>ParallelMap</cite>,
<cite>ParallelInterleave</cite> and <cite>MapAndBatch</cite>) events from upstream transformations
will be on a different thread. In such cases, the “long name” can help you
identify which transformation in a pipeline an event corresponds to.</p>
<p>![image](images/data_performance_analysis/async_long_name.png «tf.data.Dataset.range(10).map(lambda x: x).repeat(2).batch(5).prefetch(1)»)</p>
<p>For example, the above screenshot was generated from the following code:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">tf.data.Dataset.range(10)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.map(lambda</span> <span class="pre">x:</span> <span class="pre">x)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.repeat(2)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.batch(5)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.prefetch(1)</span>
<span class="pre">`</span></code></p>
<p>Here, the <cite>Iterator::Prefetch</cite> events are on the <cite>tf_data_iterator_get_next</cite>
threads. Since <cite>Prefetch</cite> is asynchronous, its input events (<cite>BatchV2</cite>) will be
on a different thread, and can be located by searching for the long name
<cite>Iterator::Prefetch::BatchV2</cite>. In this case, they are on the
<cite>tf_data_iterator_resource</cite> thread. From its long name, you can deduce that
<cite>BatchV2</cite> is upstream of <cite>Prefetch</cite>. Furthermore, the <cite>parent_id</cite> of the
<cite>BatchV2</cite> event will match the ID of the <cite>Prefetch</cite> event.</p>
<p>#### Identifying the bottleneck</p>
<p>In general, to identify the bottleneck in your input pipeline, walk the input
pipeline from the outermost transformation all the way to the source. Starting
from the final transformation in your pipeline, recurse into upstream
transformations until you find a slow transformation or reach a source dataset,
such as <cite>TFRecord</cite>. In the example above, you would start from <cite>Prefetch</cite>, then
walk upstream to <cite>BatchV2</cite>, <cite>FiniteRepeat</cite>, <cite>Map</cite>, and finally <cite>Range</cite>.</p>
<p>In general, a slow transformation corresponds to one whose events are long, but
whose input events are short. Some examples follow below.</p>
<p>Note that the final (outermost) transformation in most host input pipelines is
the <cite>Iterator::Model</cite> event. The Model transformation is introduced
automatically by the <cite>tf.data</cite> runtime and is used for instrumenting and
autotuning the input pipeline performance.</p>
<p>If your job is using a
[distribution strategy](<a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">https://www.tensorflow.org/guide/distributed_training</a>),
the trace viewer will contain additional events that correspond to the device
input pipeline. The outermost transformation of the device pipeline (nested
under <cite>IteratorGetNextOp::DoCompute</cite> or
<cite>IteratorGetNextAsOptionalOp::DoCompute</cite>) will be an <cite>Iterator::Prefetch</cite> event
with an upstream <cite>Iterator::Generator</cite> event. You can find the corresponding
host pipeline by searching for <cite>Iterator::Model</cite> events.</p>
<p>##### Example 1</p>
<p>![image](images/data_performance_analysis/example_1_cropped.png «Example 1»)</p>
<p>The above screenshot is generated from the following input pipeline:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">tf.data.TFRecordDataset(filename)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.map(parse_record)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.batch(32)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.repeat()</span>
<span class="pre">`</span></code></p>
<p>In the screenshot, observe that (1) <cite>Iterator::Map</cite> events are long, but (2) its
input events (<cite>Iterator::FlatMap</cite>) return quickly. This suggests that the
sequential Map transformation is the bottleneck.</p>
<p>Note that in the screenshot, the <cite>InstantiatedCapturedFunction::Run</cite> event
corresponds to the time it takes to execute the map function.</p>
<p>##### Example 2</p>
<p>![image](images/data_performance_analysis/example_2_cropped.png «Example 2»)</p>
<p>The above screenshot is generated from the following input pipeline:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">tf.data.TFRecordDataset(filename)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.map(parse_record,</span> <span class="pre">num_parallel_calls=2)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.batch(32)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.repeat()</span>
<span class="pre">`</span></code></p>
<p>This example is similar to the above, but uses ParallelMap instead of Map. We
notice here that (1) <cite>Iterator::ParallelMap</cite> events are long, but (2) its input
events <cite>Iterator::FlatMap</cite> (which are on a different thread, since ParallelMap
is asynchronous) are short. This suggests that the ParallelMap transformation is
the bottleneck.</p>
<p>#### Addressing the bottleneck</p>
<p>##### Source datasets</p>
<p>If you’ve identified a dataset source as the bottleneck, such as reading from
TFRecord files, you can improve performance by parallelizing data extraction. To
do so, ensure that your data is sharded across multiple files and use
<cite>tf.data.Dataset.interleave</cite> with the <cite>num_parallel_calls</cite> parameter set to
<cite>tf.data.AUTOTUNE</cite>. If determinism is not important to your
program, you can further improve performance by setting the
<cite>deterministic=False</cite> flag on <cite>tf.data.Dataset.interleave</cite> as of TF 2.2. For
example, if you’re reading from TFRecords, you can do the following:</p>
<p><a href="#id3"><span class="problematic" id="id4">``</span></a><a href="#id5"><span class="problematic" id="id6">`</span></a>python
dataset = tf.data.Dataset.from_tensor_slices(filenames)
dataset = dataset.interleave(tf.data.TFRecordDataset,</p>
<blockquote>
<div><p>num_parallel_calls=tf.data.AUTOTUNE,
deterministic=False)</p>
</div></blockquote>
<p><a href="#id7"><span class="problematic" id="id8">``</span></a><a href="#id9"><span class="problematic" id="id10">`</span></a></p>
<p>Note that sharded files should be reasonably large to amortize the overhead of
opening a file. For more details on parallel data extraction, see
[this section](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance#parallelizing_data_extraction">https://www.tensorflow.org/guide/data_performance#parallelizing_data_extraction</a>)
of the <cite>tf.data</cite> performance guide.</p>
<p>##### Transformation datasets</p>
<p>If you’ve identified an intermediate <cite>tf.data</cite> transformation as the bottleneck,
you can address it by parallelizing the transformation or
[caching the computation](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance#caching">https://www.tensorflow.org/guide/data_performance#caching</a>)
if your data fits into memory and it is appropriate. Some transformations such
as <cite>Map</cite> have parallel counterparts; the
[<cite>tf.data</cite> performance guide demonstrates](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation">https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation</a>)
how to parallelize these. Other transformations, such as <cite>Filter</cite>, <cite>Unbatch</cite>,
and <cite>Batch</cite> are inherently sequential; you can parallelize them by introducing
“outer parallelism”. For example, supposing your input pipeline initially looks
like the following, with <cite>Batch</cite> as the bottleneck:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">filenames</span> <span class="pre">=</span> <span class="pre">tf.data.Dataset.list_files(file_path,</span> <span class="pre">shuffle=is_training)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">filenames_to_dataset(filenames)</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">dataset.batch(batch_size)</span>
<span class="pre">`</span></code></p>
<p>You can introduce “outer parallelism” by running multiple copies of the input
pipeline over sharded inputs and combining the results:</p>
<p><a href="#id11"><span class="problematic" id="id12">``</span></a><a href="#id13"><span class="problematic" id="id14">`</span></a>python
filenames = tf.data.Dataset.list_files(file_path, shuffle=is_training)</p>
<dl class="simple">
<dt>def make_dataset(shard_index):</dt><dd><p>filenames = filenames.shard(NUM_SHARDS, shard_index)
dataset = filenames_to_dataset(filenames)
Return dataset.batch(batch_size)</p>
</dd>
</dl>
<p>indices = tf.data.Dataset.range(NUM_SHARDS)
dataset = indices.interleave(make_dataset,</p>
<blockquote>
<div><p>num_parallel_calls=tf.data.AUTOTUNE)</p>
</div></blockquote>
<p>dataset = dataset.prefetch(tf.data.AUTOTUNE)
<a href="#id15"><span class="problematic" id="id16">``</span></a><a href="#id17"><span class="problematic" id="id18">`</span></a></p>
<p>## Additional resources</p>
<ul class="simple">
<li><p>[tf.data performance guide](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance">https://www.tensorflow.org/guide/data_performance</a>)
on how to write performance <cite>tf.data</cite> input pipelines</p></li>
<li><p>[Inside TensorFlow video: <cite>tf.data</cite> best practices ](<a class="reference external" href="https://www.youtube.com/watch?v=ZnukSLKEw34">https://www.youtube.com/watch?v=ZnukSLKEw34</a>)</p></li>
<li><p>[Profiler guide](<a class="reference external" href="https://www.tensorflow.org/guide/profiler">https://www.tensorflow.org/guide/profiler</a>)</p></li>
<li><p>[Profiler tutorial with colab](<a class="reference external" href="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras</a>)</p></li>
</ul>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>