

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>&lt;no title&gt; &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/effective_tf2.md.txt" rel="nofollow"> Ver código fuente de la página</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p># Effective TensorFlow 2</p>
<p>There are multiple changes in TensorFlow 2.0 to make TensorFlow users more
productive. TensorFlow 2.0 removes
[redundant APIs](<a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md">https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md</a>),
makes APIs more consistent
([Unified RNNs](<a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20180920-unify-rnn-interface.md">https://github.com/tensorflow/community/blob/master/rfcs/20180920-unify-rnn-interface.md</a>),
[Unified Optimizers](<a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20181016-optimizer-unification.md">https://github.com/tensorflow/community/blob/master/rfcs/20181016-optimizer-unification.md</a>)),
and better integrates with the Python runtime with
[Eager execution](<a class="reference external" href="https://www.tensorflow.org/guide/eager">https://www.tensorflow.org/guide/eager</a>).</p>
<p>Many
[RFCs](<a class="reference external" href="https://github.com/tensorflow/community/pulls?utf8=%E2%9C%93&amp;q=is%3Apr">https://github.com/tensorflow/community/pulls?utf8=%E2%9C%93&amp;q=is%3Apr</a>)
have explained the changes that have gone into making TensorFlow 2.0. This
guide presents a vision for what development in TensorFlow 2.0 should look like.
It’s assumed you have some familiarity with TensorFlow 1.x.</p>
<p>## A brief summary of major changes</p>
<p>### API Cleanup</p>
<p>Many APIs are either
[gone or moved](<a class="reference external" href="https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md">https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md</a>)
in TF 2.0. Some of the major changes include removing <cite>tf.app</cite>, <cite>tf.flags</cite>, and
<cite>tf.logging</cite> in favor of the now open-source
[absl-py](<a class="reference external" href="https://github.com/abseil/abseil-py">https://github.com/abseil/abseil-py</a>), rehoming projects that lived in
<cite>tf.contrib</cite>, and cleaning up the main <cite>tf.*</cite> namespace by moving lesser used
functions into subpackages like <cite>tf.math</cite>. Some APIs have been replaced with
their 2.0 equivalents - <cite>tf.summary</cite>, <cite>tf.keras.metrics</cite>, and
<cite>tf.keras.optimizers</cite>. The easiest way to automatically apply these renames
is to use the [v2 upgrade script](upgrade.md).</p>
<p>### Eager execution</p>
<p>TensorFlow 1.X requires users to manually stitch together an
[abstract syntax tree](<a class="reference external" href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">https://en.wikipedia.org/wiki/Abstract_syntax_tree</a>) (the
graph) by making <cite>tf.*</cite> API calls. It then requires users to manually compile
the abstract syntax tree by passing a set of output tensors and input tensors to
a <cite>session.run()</cite> call. TensorFlow 2.0 executes eagerly (like Python normally
does) and in 2.0, graphs and sessions should feel like implementation details.</p>
<p>One notable byproduct of eager execution is that <cite>tf.control_dependencies()</cite> is
no longer required, as all lines of code execute in order (within a
<cite>tf.function</cite>, code with side effects execute in the order written).</p>
<p>### No more globals</p>
<p>TensorFlow 1.X relied heavily on implicitly global namespaces. When you called
<cite>tf.Variable()</cite>, it would be put into the default graph, and it would remain
there, even if you lost track of the Python variable pointing to it. You could
then recover that <cite>tf.Variable</cite>, but only if you knew the name that it had been
created with. This was difficult to do if you were not in control of the
variable’s creation. As a result, all sorts of mechanisms proliferated to
attempt to help users find their variables again, and for frameworks to find
user-created variables: Variable scopes, global collections, helper methods like
<cite>tf.get_global_step()</cite>, <cite>tf.global_variables_initializer()</cite>, optimizers
implicitly computing gradients over all trainable variables, and so on.
TensorFlow 2.0 eliminates all of these mechanisms
([Variables 2.0 RFC](<a class="reference external" href="https://github.com/tensorflow/community/pull/11">https://github.com/tensorflow/community/pull/11</a>)) in favor
of the default mechanism: Keep track of your variables! If you lose track of a
<cite>tf.Variable</cite>, it gets garbage collected.</p>
<p>The requirement to track variables creates some extra work for the user, but
with Keras objects (see below), the burden is minimized.</p>
<p>### Functions, not sessions</p>
<p>A <cite>session.run()</cite> call is almost like a function call: You specify the inputs
and the function to be called, and you get back a set of outputs. In TensorFlow
2.0, you can decorate a Python function using <cite>tf.function()</cite> to mark it for JIT
compilation so that TensorFlow runs it as a single graph
([Functions 2.0 RFC](<a class="reference external" href="https://github.com/tensorflow/community/pull/20">https://github.com/tensorflow/community/pull/20</a>)). This
mechanism allows TensorFlow 2.0 to gain all of the benefits of graph mode:</p>
<ul class="simple">
<li><p>Performance: The function can be optimized (node pruning, kernel fusion,
etc.)</p></li>
<li><p>Portability: The function can be exported/reimported
([SavedModel 2.0 RFC](<a class="reference external" href="https://github.com/tensorflow/community/pull/34">https://github.com/tensorflow/community/pull/34</a>)),
allowing users to reuse and share modular TensorFlow functions.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">#</span> <span class="pre">TensorFlow</span> <span class="pre">1.X</span>
<span class="pre">outputs</span> <span class="pre">=</span> <span class="pre">session.run(f(placeholder),</span> <span class="pre">feed_dict={placeholder:</span> <span class="pre">input})</span>
<span class="pre">#</span> <span class="pre">TensorFlow</span> <span class="pre">2.0</span>
<span class="pre">outputs</span> <span class="pre">=</span> <span class="pre">f(input)</span>
<span class="pre">`</span></code></p>
<p>With the power to freely intersperse Python and TensorFlow code, users can take advantage of Python’s expressiveness. But portable
TensorFlow executes in contexts without a Python interpreter, such as mobile, C++, and
JavaScript. To help users avoid having to rewrite their code when adding <cite>&#64;tf.function</cite>,
[AutoGraph](function.ipynb) converts a subset of
Python constructs into their TensorFlow equivalents:</p>
<ul class="simple">
<li><p><cite>for</cite>/<cite>while</cite> -&gt; <cite>tf.while_loop</cite> (<cite>break</cite> and <cite>continue</cite> are supported)</p></li>
<li><p><cite>if</cite> -&gt; <cite>tf.cond</cite></p></li>
<li><p><cite>for _ in dataset</cite> -&gt; <cite>dataset.reduce</cite></p></li>
</ul>
<p>AutoGraph supports arbitrary nestings of control flow, which makes it possible
to performantly and concisely implement many complex ML programs such as
sequence models, reinforcement learning, custom training loops, and more.</p>
<p>## Recommendations for idiomatic TensorFlow 2.0</p>
<p>### Refactor your code into smaller functions</p>
<p>A common usage pattern in TensorFlow 1.X was the «kitchen sink» strategy, where
the union of all possible computations was preemptively laid out, and then
selected tensors were evaluated via <cite>session.run()</cite>. In TensorFlow 2.0, users
should refactor their code into smaller functions that are called as needed. In
general, it’s not necessary to decorate each of these smaller functions with
<cite>tf.function</cite>; only use <cite>tf.function</cite> to decorate high-level computations - for
example, one step of training or the forward pass of your model.</p>
<p>### Use Keras layers and models to manage variables</p>
<p>Keras models and layers offer the convenient <cite>variables</cite> and
<cite>trainable_variables</cite> properties, which recursively gather up all dependent
variables. This makes it easy to manage variables locally to where they are
being used.</p>
<p>Contrast:</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
def dense(x, W, b):</p>
<blockquote>
<div><p>return tf.nn.sigmoid(tf.matmul(x, W) + b)</p>
</div></blockquote>
<p>&#64;tf.function
def multilayer_perceptron(x, w0, b0, w1, b1, w2, b2 …):</p>
<blockquote>
<div><p>x = dense(x, w0, b0)
x = dense(x, w1, b1)
x = dense(x, w2, b2)
…</p>
</div></blockquote>
<p># You still have to manage w_i and b_i, and their shapes are defined far away from the code.
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>with the Keras version:</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
# Each layer can be called, with a signature equivalent to linear(x)
layers = [tf.keras.layers.Dense(hidden_size, activation=tf.nn.sigmoid) for _ in range(n)]
perceptron = tf.keras.Sequential(layers)</p>
<p># layers[3].trainable_variables =&gt; returns [w3, b3]
# perceptron.trainable_variables =&gt; returns [w0, b0, …]
<a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<p>Keras layers/models inherit from <cite>tf.train.Checkpointable</cite> and are integrated
with <cite>&#64;tf.function</cite>, which makes it possible to directly checkpoint or export
SavedModels from Keras objects. You do not necessarily have to use Keras’s
<cite>.fit()</cite> API to take advantage of these integrations.</p>
<p>Here’s a transfer learning example that demonstrates how Keras makes it easy to
collect a subset of relevant variables. Let’s say you’re training a multi-headed
model with a shared trunk:</p>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>python
trunk = tf.keras.Sequential([…])
head1 = tf.keras.Sequential([…])
head2 = tf.keras.Sequential([…])</p>
<p>path1 = tf.keras.Sequential([trunk, head1])
path2 = tf.keras.Sequential([trunk, head2])</p>
<p># Train on primary dataset
for x, y in main_dataset:</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.GradientTape() as tape:</dt><dd><p># training=True is only needed if there are layers with different
# behavior during training versus inference (e.g. Dropout).
prediction = path1(x, training=True)
loss = loss_fn_head1(prediction, y)</p>
</dd>
</dl>
<p># Simultaneously optimize trunk and head1 weights.
gradients = tape.gradient(loss, path1.trainable_variables)
optimizer.apply_gradients(zip(gradients, path1.trainable_variables))</p>
</div></blockquote>
<p># Fine-tune second head, reusing the trunk
for x, y in small_dataset:</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.GradientTape() as tape:</dt><dd><p># training=True is only needed if there are layers with different
# behavior during training versus inference (e.g. Dropout).
prediction = path2(x, training=True)
loss = loss_fn_head2(prediction, y)</p>
</dd>
</dl>
<p># Only optimize head2 weights, not trunk weights
gradients = tape.gradient(loss, head2.trainable_variables)
optimizer.apply_gradients(zip(gradients, head2.trainable_variables))</p>
</div></blockquote>
<p># You can publish just the trunk computation for other people to reuse.
tf.saved_model.save(trunk, output_path)
<a href="#id21"><span class="problematic" id="id22">``</span></a><a href="#id23"><span class="problematic" id="id24">`</span></a></p>
<p>### Combine tf.data.Datasets and &#64;tf.function</p>
<p>When iterating over training data that fits in memory, feel free to use regular
Python iteration. Otherwise, <cite>tf.data.Dataset</cite> is the best way to stream
training data from disk. Datasets are
[iterables (not iterators)](<a class="reference external" href="https://docs.python.org/3/glossary.html#term-iterable">https://docs.python.org/3/glossary.html#term-iterable</a>),
and work just like other Python iterables in Eager mode. You can fully utilize
dataset async prefetching/streaming features by wrapping your code in
<cite>tf.function()</cite>, which replaces Python iteration with the equivalent graph
operations using AutoGraph.</p>
<p><a href="#id25"><span class="problematic" id="id26">``</span></a><a href="#id27"><span class="problematic" id="id28">`</span></a>python
&#64;tf.function
def train(model, dataset, optimizer):</p>
<blockquote>
<div><dl>
<dt>for x, y in dataset:</dt><dd><dl class="simple">
<dt>with tf.GradientTape() as tape:</dt><dd><p># training=True is only needed if there are layers with different
# behavior during training versus inference (e.g. Dropout).
prediction = model(x, training=True)
loss = loss_fn(prediction, y)</p>
</dd>
</dl>
<p>gradients = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a></p>
<p>If you use the Keras <cite>.fit()</cite> API, you won’t have to worry about dataset
iteration.</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">model.compile(optimizer=optimizer,</span> <span class="pre">loss=loss_fn)</span>
<span class="pre">model.fit(dataset)</span>
<span class="pre">`</span></code></p>
<p>### Take advantage of AutoGraph with Python control flow</p>
<p>AutoGraph provides a way to convert data-dependent control flow into graph-mode
equivalents like <cite>tf.cond</cite> and <cite>tf.while_loop</cite>.</p>
<p>One common place where data-dependent control flow appears is in sequence
models. <cite>tf.keras.layers.RNN</cite> wraps an RNN cell, allowing you to either
statically or dynamically unroll the recurrence. For demonstration’s sake, you
could reimplement dynamic unroll as follows:</p>
<p><a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>python
class DynamicRNN(tf.keras.Model):</p>
<blockquote>
<div><dl>
<dt>def __init__(self, rnn_cell):</dt><dd><p>super(DynamicRNN, self).__init__(self)
self.cell = rnn_cell</p>
</dd>
<dt>def call(self, input_data):</dt><dd><p># [batch, time, features] -&gt; [time, batch, features]
input_data = tf.transpose(input_data, [1, 0, 2])
outputs = tf.TensorArray(tf.float32, input_data.shape[0])
state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32)
for i in tf.range(input_data.shape[0]):</p>
<blockquote>
<div><p>output, state = self.cell(input_data[i], state)
outputs = outputs.write(i, output)</p>
</div></blockquote>
<p>return tf.transpose(outputs.stack(), [1, 0, 2]), state</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id37"><span class="problematic" id="id38">``</span></a><a href="#id39"><span class="problematic" id="id40">`</span></a></p>
<p>For a more detailed overview of AutoGraph’s features, see
[the guide](./function.ipynb).</p>
<p>### tf.metrics aggregates data and tf.summary logs them</p>
<p>To log summaries, use <cite>tf.summary.(scalar|histogram|…)</cite> and redirect it to a
writer using a context manager. (If you omit the context manager, nothing
happens.) Unlike TF 1.x, the summaries are emitted directly to the writer; there
is no separate «merge» op and no separate <cite>add_summary()</cite> call, which means that
the <cite>step</cite> value must be provided at the callsite.</p>
<p><a href="#id41"><span class="problematic" id="id42">``</span></a><a href="#id43"><span class="problematic" id="id44">`</span></a>python
summary_writer = tf.summary.create_file_writer(“/tmp/summaries”)
with summary_writer.as_default():</p>
<blockquote>
<div><p>tf.summary.scalar(“loss”, 0.1, step=42)</p>
</div></blockquote>
<p><a href="#id45"><span class="problematic" id="id46">``</span></a><a href="#id47"><span class="problematic" id="id48">`</span></a></p>
<p>To aggregate data before logging them as summaries, use <cite>tf.metrics</cite>. Metrics
are stateful: They accumulate values and return a cumulative result when you
call <cite>.result()</cite>. Clear accumulated values with <cite>.reset_states()</cite>.</p>
<p><a href="#id49"><span class="problematic" id="id50">``</span></a><a href="#id51"><span class="problematic" id="id52">`</span></a>python
def train(model, optimizer, dataset, log_freq=10):</p>
<blockquote>
<div><p>avg_loss = tf.keras.metrics.Mean(name=”loss”, dtype=tf.float32)
for images, labels in dataset:</p>
<blockquote>
<div><p>loss = train_step(model, optimizer, images, labels)
avg_loss.update_state(loss)
if tf.equal(optimizer.iterations % log_freq, 0):</p>
<blockquote>
<div><p>tf.summary.scalar(“loss”, avg_loss.result(), step=optimizer.iterations)
avg_loss.reset_states()</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<dl class="simple">
<dt>def test(model, test_x, test_y, step_num):</dt><dd><p># training=False is only needed if there are layers with different
# behavior during training versus inference (e.g. Dropout).
loss = loss_fn(model(test_x, training=False), test_y)
tf.summary.scalar(“loss”, loss, step=step_num)</p>
</dd>
</dl>
<p>train_summary_writer = tf.summary.create_file_writer(“/tmp/summaries/train”)
test_summary_writer = tf.summary.create_file_writer(“/tmp/summaries/test”)</p>
<dl class="simple">
<dt>with train_summary_writer.as_default():</dt><dd><p>train(model, optimizer, dataset)</p>
</dd>
<dt>with test_summary_writer.as_default():</dt><dd><p>test(model, test_x, test_y, optimizer.iterations)</p>
</dd>
</dl>
<p><a href="#id53"><span class="problematic" id="id54">``</span></a><a href="#id55"><span class="problematic" id="id56">`</span></a></p>
<p>Visualize the generated summaries by pointing TensorBoard at the summary log
directory:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">tensorboard</span> <span class="pre">--logdir</span> <span class="pre">/tmp/summaries</span>
<span class="pre">`</span></code></p>
<p>### Use tf.config.experimental_run_functions_eagerly() when debugging</p>
<p>In TensorFlow 2.0, Eager execution lets you run the code step-by-step to inspect
shapes, data types and values. Certain APIs, like <cite>tf.function</cite>, <cite>tf.keras</cite>,
etc. are designed to use Graph execution, for performance and portability.
When debugging, use <cite>tf.config.experimental_run_functions_eagerly(True)</cite> to
use Eager execution inside this code.</p>
<p>For example:</p>
<p><a href="#id57"><span class="problematic" id="id58">``</span></a><a href="#id59"><span class="problematic" id="id60">`</span></a>python
&#64;tf.function
def f(x):</p>
<blockquote>
<div><dl class="simple">
<dt>if x &gt; 0:</dt><dd><p>import pdb
pdb.set_trace()
x = x + 1</p>
</dd>
</dl>
<p>return x</p>
</div></blockquote>
<p>tf.config.experimental_run_functions_eagerly(True)
f(tf.constant(1))
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
&gt;&gt;&gt; f()
-&gt; x = x + 1
(Pdb) l</p>
<blockquote>
<div><blockquote>
<div><p>6     &#64;tf.function
7     def f(x):
8       if x &gt; 0:
9         import pdb</p>
</div></blockquote>
<p>10         pdb.set_trace()
11  -&gt;     x = x + 1
12       return x
13
14     tf.config.experimental_run_functions_eagerly(True)
15     f(tf.constant(1))</p>
</div></blockquote>
<p>[EOF]
<a href="#id61"><span class="problematic" id="id62">``</span></a><a href="#id63"><span class="problematic" id="id64">`</span></a></p>
<p>This also works inside Keras models and other APIs that support Eager execution:</p>
<p><a href="#id65"><span class="problematic" id="id66">``</span></a>`
class CustomModel(tf.keras.models.Model):</p>
<blockquote>
<div><p>&#64;tf.function
def call(self, input_data):</p>
<blockquote>
<div><dl class="simple">
<dt>if tf.reduce_mean(input_data) &gt; 0:</dt><dd><p>return input_data</p>
</dd>
<dt>else:</dt><dd><p>import pdb
pdb.set_trace()
return input_data // 2</p>
</dd>
</dl>
</div></blockquote>
</div></blockquote>
<p>tf.config.experimental_run_functions_eagerly(True)
model = CustomModel()
model(tf.constant([-2, -4]))
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">`</span></code>
&gt;&gt;&gt; call()
-&gt; return input_data // 2
(Pdb) l</p>
<blockquote>
<div><p>10         if tf.reduce_mean(input_data) &gt; 0:
11           return input_data
12         else:
13           import pdb
14           pdb.set_trace()
15  -&gt;       return input_data // 2
16
17
18     tf.config.experimental_run_functions_eagerly(True)
19     model = CustomModel()
20     model(tf.constant([-2, -4]))</p>
</div></blockquote>
<p><a href="#id67"><span class="problematic" id="id68">``</span></a><a href="#id69"><span class="problematic" id="id70">`</span></a></p>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Construido con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>