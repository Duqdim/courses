

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2020 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2020 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/intro_to_graphs.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2020-The-TensorFlow-Authors.">
<h1>Copyright 2020 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2020-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Introduction-to-graphs-and-tf.function">
<h2>Introduction to graphs and tf.function<a class="headerlink" href="#Introduction-to-graphs-and-tf.function" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>11a678c9ffa04cd083270d5e7c48d6fc|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>e5fcd22b8f6e4f858d93d354f5b26809|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>77b0b2a0aa3e40e881f92423bc9b48e1|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>5577be9f45d9426999858a5a4bd4a0d3|Download notebook</p>
</td></table><div class="section" id="Overview">
<h3>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h3>
<p>This guide goes beneath the surface of TensorFlow and Keras to see how TensorFlow works. If you instead want to immediately get started with Keras, please see <a class="reference external" href="keras/">our collection of Keras guides</a>.</p>
<p>In this guide you’ll see the core of how TensorFlow allows you to make simple changes to your code to get graphs, how graphs are stored and represented, and how you can use them to accelerate your models.</p>
<p>Note: For those of you who are only familiar with TensorFlow 1.x, this guide demonstrates a very different view of graphs.</p>
<p><strong>This is a big-picture overview that covers how ``tf.function`` allows you to switch from eager execution to graph execution.</strong> For a more complete specification of <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>, see <cite>the ``tf.function`</cite> guide &lt;function&gt;`__.</p>
<div class="section" id="What-are-graphs?">
<h4>What are graphs?<a class="headerlink" href="#What-are-graphs?" title="Enlazar permanentemente con este título">¶</a></h4>
<p>In the previous three guides, you have seen TensorFlow running <strong>eagerly</strong>. This means TensorFlow operations are executed by Python, operation by operation, and returning results back to Python.</p>
<p>While eager execution has several unique advantages, graph execution enables portability outside Python and tends to offer better performance. <strong>Graph execution</strong> means that tensor computations are executed as a <em>TensorFlow graph</em>, sometimes referred to as a <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> or simply a “graph.”</p>
<p><strong>Graphs are data structures that contain a set of ``tf.Operation`` objects, which represent units of computation; and ``tf.Tensor`` objects, which represent the units of data that flow between operations.</strong> They are defined in a <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> context. Since these graphs are data structures, they can be saved, run, and restored all without the original Python code.</p>
<p>This is what a TensorFlow graph representing a two-layer neural network looks like when visualized in TensorBoard.</p>
<p><img alt="A simple TensorFlow graph" src="../../../_images/two-layer-network.png" /></p>
</div>
<div class="section" id="The-benefits-of-graphs">
<h4>The benefits of graphs<a class="headerlink" href="#The-benefits-of-graphs" title="Enlazar permanentemente con este título">¶</a></h4>
<p>With a graph, you have a great deal of flexibility. You can use your TensorFlow graph in environments that don’t have a Python interpreter, like mobile applications, embedded devices, and backend servers. TensorFlow uses graphs as the format for <a class="reference external" href="saved_model">saved models</a> when it exports them from Python.</p>
<p>Graphs are also easily optimized, allowing the compiler to do transformations like:</p>
<ul class="simple">
<li><p>Statically infer the value of tensors by folding constant nodes in your computation <em>(“constant folding”)</em>.</p></li>
<li><p>Separate sub-parts of a computation that are independent and split them between threads or devices.</p></li>
<li><p>Simplify arithmetic operations by eliminating common subexpressions.</p></li>
</ul>
<p>There is an entire optimization system, <a class="reference internal" href="graph_optimization.html"><span class="doc">Grappler</span></a>, to perform this and other speedups.</p>
<p>In short, graphs are extremely useful and let your TensorFlow run <strong>fast</strong>, run <strong>in parallel</strong>, and run efficiently <strong>on multiple devices</strong>.</p>
<p>However, you still want to define our machine learning models (or other computations) in Python for convenience, and then automatically construct graphs when you need them.</p>
</div>
</div>
<div class="section" id="Taking-advantage-of-graphs">
<h3>Taking advantage of graphs<a class="headerlink" href="#Taking-advantage-of-graphs" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You create and run a graph in TensorFlow by using <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>, either as a direct call or as a decorator. <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> takes a regular function as input and returns a <code class="docutils literal notranslate"><span class="pre">Function</span></code>. <strong>A ``Function`` is a Python callable that builds TensorFlow graphs from the Python function. You use a ``Function`` in the same way as its Python equivalent.</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow as tf
import timeit
from datetime import datetime
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Define a Python function.
def a_regular_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# `a_function_that_uses_a_graph` is a TensorFlow `Function`.
a_function_that_uses_a_graph = tf.function(a_regular_function)

# Make some tensors.
x1 = tf.constant([[1.0, 2.0]])
y1 = tf.constant([[2.0], [3.0]])
b1 = tf.constant(4.0)

orig_value = a_regular_function(x1, y1, b1).numpy()
# Call a `Function` like a Python function.
tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()
assert(orig_value == tf_function_value)
</pre></div>
</div>
</div>
<p>On the outside, a <code class="docutils literal notranslate"><span class="pre">Function</span></code> looks like a regular function you write using TensorFlow operations. <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py">Underneath</a>, however, it is <em>very different</em>. A <code class="docutils literal notranslate"><span class="pre">Function</span></code> <strong>encapsulates</strong><cite>several ``tf.Graph`</cite>s behind one API &lt;#polymorphism_one_function_many_graphs&gt;`__<strong>.</strong> That is how <code class="docutils literal notranslate"><span class="pre">Function</span></code> is able to give you the <a class="reference external" href="#the_benefits_of_graphs">benefits of graph execution</a>, like speed and
deployability.</p>
<p><code class="docutils literal notranslate"><span class="pre">tf.function</span></code> applies to a function <em>and all other functions it calls</em>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def inner_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# Use the decorator to make `outer_function` a `Function`.
@tf.function
def outer_function(x):
  y = tf.constant([[2.0], [3.0]])
  b = tf.constant(4.0)

  return inner_function(x, y, b)

# Note that the callable will create a graph that
# includes `inner_function` as well as `outer_function`.
outer_function(tf.constant([[1.0, 2.0]])).numpy()
</pre></div>
</div>
</div>
<p>If you have used TensorFlow 1.x, you will notice that at no time did you need to define a <code class="docutils literal notranslate"><span class="pre">Placeholder</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.Session</span></code>.</p>
<div class="section" id="Converting-Python-functions-to-graphs">
<h4>Converting Python functions to graphs<a class="headerlink" href="#Converting-Python-functions-to-graphs" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Any function you write with TensorFlow will contain a mixture of built-in TF operations and Python logic, such as <code class="docutils literal notranslate"><span class="pre">if-then</span></code> clauses, loops, <code class="docutils literal notranslate"><span class="pre">break</span></code>, <code class="docutils literal notranslate"><span class="pre">return</span></code>, <code class="docutils literal notranslate"><span class="pre">continue</span></code>, and more. While TensorFlow operations are easily captured by a <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code>, Python-specific logic needs to undergo an extra step in order to become part of the graph. <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> uses a library called AutoGraph (<code class="docutils literal notranslate"><span class="pre">tf.autograph</span></code>) to convert Python code into graph-generating code.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def simple_relu(x):
  if tf.greater(x, 0):
    return x
  else:
    return 0

# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.
tf_simple_relu = tf.function(simple_relu)

print(&quot;First branch, with graph:&quot;, tf_simple_relu(tf.constant(1)).numpy())
print(&quot;Second branch, with graph:&quot;, tf_simple_relu(tf.constant(-1)).numpy())
</pre></div>
</div>
</div>
<p>Though it is unlikely that you will need to view graphs directly, you can inspect the outputs to see the exact results. These are not easy to read, so no need to look too carefully!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># This is the graph-generating output of AutoGraph.
print(tf.autograph.to_code(simple_relu))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># This is the graph itself.
print(tf_simple_relu.get_concrete_function(tf.constant(1)).graph.as_graph_def())
</pre></div>
</div>
</div>
<p>Most of the time, <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> will work without special considerations. However, there are some caveats, and the <a class="reference internal" href="function.html"><span class="doc">tf.function guide</span></a> can help here, as well as the <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md">complete AutoGraph reference</a></p>
</div>
<div class="section" id="Polymorphism:-one-Function,-many-graphs">
<h4>Polymorphism: one <code class="docutils literal notranslate"><span class="pre">Function</span></code>, many graphs<a class="headerlink" href="#Polymorphism:-one-Function,-many-graphs" title="Enlazar permanentemente con este título">¶</a></h4>
<p>A <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> is specialized to a specific type of inputs (for example, tensors with a specific <code class="docutils literal notranslate"><span class="pre">`dtype</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/dtypes/DType">https://www.tensorflow.org/api_docs/python/tf/dtypes/DType</a>&gt;`__ or objects with the same <code class="docutils literal notranslate"><span class="pre">`id()</span></code> &lt;<a class="reference external" href="https://docs.python.org/3/library/functions.html#id%5D">https://docs.python.org/3/library/functions.html#id%5D</a>&gt;`__).</p>
<p>Each time you invoke a <code class="docutils literal notranslate"><span class="pre">Function</span></code> with new <code class="docutils literal notranslate"><span class="pre">dtypes</span></code> and shapes in its arguments, <code class="docutils literal notranslate"><span class="pre">Function</span></code> creates a new <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> for the new arguments. The <code class="docutils literal notranslate"><span class="pre">dtypes</span></code> and shapes of a <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code>’s inputs are known as an <strong>input signature</strong> or just a <strong>signature</strong>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Function</span></code> stores the <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> corresponding to that signature in a <code class="docutils literal notranslate"><span class="pre">ConcreteFunction</span></code>. <strong>A ``ConcreteFunction`` is a wrapper around a ``tf.Graph``.</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def my_relu(x):
  return tf.maximum(0., x)

# `my_relu` creates new graphs as it sees more signatures.
print(my_relu(tf.constant(5.5)))
print(my_relu([1, -1]))
print(my_relu(tf.constant([3., -3.])))
</pre></div>
</div>
</div>
<p>If the <code class="docutils literal notranslate"><span class="pre">Function</span></code> has already been called with that signature, <code class="docutils literal notranslate"><span class="pre">Function</span></code> does not create a new <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># These two calls do *not* create new graphs.
print(my_relu(tf.constant(-2.5))) # Signature matches `tf.constant(5.5)`.
print(my_relu(tf.constant([-1., 1.]))) # Signature matches `tf.constant([3., -3.])`.
</pre></div>
</div>
</div>
<p>Because it’s backed by multiple graphs, a <code class="docutils literal notranslate"><span class="pre">Function</span></code> is <strong>polymorphic</strong>. That enables it to support more input types than a single <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> could represent, as well as to optimize each <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> for better performance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># There are three `ConcreteFunction`s (one for each graph) in `my_relu`.
# The `ConcreteFunction` also knows the return type and shape!
print(my_relu.pretty_printed_concrete_signatures())
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Using-tf.function">
<h3>Using <code class="docutils literal notranslate"><span class="pre">tf.function</span></code><a class="headerlink" href="#Using-tf.function" title="Enlazar permanentemente con este título">¶</a></h3>
<p>So far, you’ve seen how you can convert a Python function into a graph simply by using <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> as a decorator or wrapper. But in practice, getting <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> to work correctly can be tricky! In the following sections, you’ll learn how you can make your code work as expected with <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>.</p>
<div class="section" id="Graph-execution-vs. eager-execution">
<h4>Graph execution vs. eager execution<a class="headerlink" href="#Graph-execution-vs. eager-execution" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The code in a <code class="docutils literal notranslate"><span class="pre">Function</span></code> can be executed both eagerly and as a graph. By default, <code class="docutils literal notranslate"><span class="pre">Function</span></code> executes its code as a graph:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def get_MSE(y_true, y_pred):
  sq_diff = tf.pow(y_true - y_pred, 2)
  return tf.reduce_mean(sq_diff)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)
y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)
print(y_true)
print(y_pred)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>get_MSE(y_true, y_pred)
</pre></div>
</div>
</div>
<p>To verify that your <code class="docutils literal notranslate"><span class="pre">Function</span></code>’s graph is doing the same computation as its equivalent Python function, you can make it execute eagerly with <code class="docutils literal notranslate"><span class="pre">tf.config.run_functions_eagerly(True)</span></code>. This is a switch that <strong>turns off ``Function``’s ability to create and run graphs</strong>, instead executing the code normally.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tf.config.run_functions_eagerly(True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>get_MSE(y_true, y_pred)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Don&#39;t forget to set it back when you are done.
tf.config.run_functions_eagerly(False)
</pre></div>
</div>
</div>
<p>However, <code class="docutils literal notranslate"><span class="pre">Function</span></code> can behave differently under graph and eager execution. The Python <code class="docutils literal notranslate"><span class="pre">`print</span></code> &lt;<a class="reference external" href="https://docs.python.org/3/library/functions.html#print">https://docs.python.org/3/library/functions.html#print</a>&gt;`__ function is one example of how these two modes differ. Let’s see what happens when you insert a <code class="docutils literal notranslate"><span class="pre">print</span></code> statement to our function and call it repeatedly.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def get_MSE(y_true, y_pred):
  print(&quot;Calculating MSE!&quot;)
  sq_diff = tf.pow(y_true - y_pred, 2)
  return tf.reduce_mean(sq_diff)
</pre></div>
</div>
</div>
<p>Observe what is printed:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>error = get_MSE(y_true, y_pred)
error = get_MSE(y_true, y_pred)
error = get_MSE(y_true, y_pred)
</pre></div>
</div>
</div>
<p>Is the output surprising? <strong>``get_MSE`` only printed once even though it was called three times.</strong></p>
<p>To explain, the <code class="docutils literal notranslate"><span class="pre">print</span></code> statement is executed when <code class="docutils literal notranslate"><span class="pre">Function</span></code> runs the original code in order to create the graph in a process known as <a class="reference internal" href="function.html#Tracing"><span class="std std-ref">“tracing”</span></a>. <strong>Tracing captures the TensorFlow operations into a graph, and ``print`` is not captured in the graph.</strong> That graph is then executed for all three calls <strong>without ever running the Python code again</strong>.</p>
<p>As a sanity check, let’s turn off graph execution to compare:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Now, globally set everything to run eagerly to force eager execution.
tf.config.run_functions_eagerly(True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Observe what is printed below.
error = get_MSE(y_true, y_pred)
error = get_MSE(y_true, y_pred)
error = get_MSE(y_true, y_pred)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tf.config.run_functions_eagerly(False)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">print</span></code> is a <em>Python side effect</em>, and there are <a class="reference external" href="function#limitations">other differences</a> that you should be aware of when converting a function into a <code class="docutils literal notranslate"><span class="pre">Function</span></code>.</p>
<p>Note: If you would like to print values in both eager and graph execution, use <code class="docutils literal notranslate"><span class="pre">tf.print</span></code> instead.</p>
<p>###<code class="docutils literal notranslate"><span class="pre">tf.function</span></code> best practices</p>
<p>It may take some time to get used to the behavior of <code class="docutils literal notranslate"><span class="pre">Function</span></code>. To get started quickly, first-time users should play around with decorating toy functions with <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> to get experience with going from eager to graph execution.</p>
<p><em>Designing for ``tf.function``</em> may be your best bet for writing graph-compatible TensorFlow programs. Here are some tips: - Toggle between eager and graph execution early and often with <code class="docutils literal notranslate"><span class="pre">tf.config.run_functions_eagerly</span></code> to pinpoint if/ when the two modes diverge. - Create <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>s outside the Python function and modify them on the inside. The same goes for objects that use <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>, like <code class="docutils literal notranslate"><span class="pre">keras.layers</span></code>, <code class="docutils literal notranslate"><span class="pre">keras.Model</span></code>s and <code class="docutils literal notranslate"><span class="pre">tf.optimizers</span></code>. - Avoid writing functions
that <a class="reference external" href="function#depending_on_python_global_and_free_variables">depend on outer Python variables</a>, excluding <code class="docutils literal notranslate"><span class="pre">tf.Variables</span></code> and Keras objects. - Prefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but <a class="reference external" href="function#depending_on_python_objects">be careful</a>! - Include as much computation as possible under a <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> to maximize the performance gain. For example, decorate a whole training step or the entire training loop.</p>
</div>
</div>
<div class="section" id="Seeing-the-speed-up">
<h3>Seeing the speed-up<a class="headerlink" href="#Seeing-the-speed-up" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.function</span></code> usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph. You can measure the difference in performance like so:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)

def power(x, y):
  result = tf.eye(10, dtype=tf.dtypes.int32)
  for _ in range(y):
    result = tf.matmul(x, result)
  return result
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;Eager execution:&quot;, timeit.timeit(lambda: power(x, 100), number=1000))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>power_as_graph = tf.function(power)
print(&quot;Graph execution:&quot;, timeit.timeit(lambda: power_as_graph(x, 100), number=1000))
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.function</span></code> is commonly used to speed up training loops, as you can see <a class="reference external" href="keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction">here</a> with Keras.</p>
<p>Note: You can also try <code class="docutils literal notranslate"><span class="pre">`tf.function(jit_compile=True)</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/xla#explicit_compilation_with_tffunctionjit_compiletrue">https://www.tensorflow.org/xla#explicit_compilation_with_tffunctionjit_compiletrue</a>&gt;`__ for a more significant performance boost, especially if your code is heavy on TF control flow and uses many small tensors.</p>
<div class="section" id="Performance-and-trade-offs">
<h4>Performance and trade-offs<a class="headerlink" href="#Performance-and-trade-offs" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Graphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph. <strong>This investment is usually quickly paid back with with the performance boost of subsequent executions, but it’s important to be aware that the first few steps of any large model training can be slower due to tracing.</strong></p>
<p>No matter how large your model, you want to avoid tracing frequently. The <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> guide discusses <a class="reference external" href="function#controlling_retracing">how to set input specifications and use tensor arguments</a> to avoid retracing. If you find you are getting unusually poor performance, it’s a good idea to check if you are retracing accidentally.</p>
</div>
</div>
<div class="section" id="When-is-a-Function-tracing?">
<h3>When is a <code class="docutils literal notranslate"><span class="pre">Function</span></code> tracing?<a class="headerlink" href="#When-is-a-Function-tracing?" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To figure out when your <code class="docutils literal notranslate"><span class="pre">Function</span></code> is tracing, add a <code class="docutils literal notranslate"><span class="pre">print</span></code> statement to its code. As a rule of thumb, <code class="docutils literal notranslate"><span class="pre">Function</span></code> will execute the <code class="docutils literal notranslate"><span class="pre">print</span></code> statement every time it traces.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def a_function_with_python_side_effect(x):
  print(&quot;Tracing!&quot;) # An eager-only side effect.
  return x * x + tf.constant(2)

# This is traced the first time.
print(a_function_with_python_side_effect(tf.constant(2)))
# The second time through, you won&#39;t see the side effect.
print(a_function_with_python_side_effect(tf.constant(3)))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># This retraces each time the Python argument changes,
# as a Python argument could be an epoch count or other
# hyperparameter.
print(a_function_with_python_side_effect(2))
print(a_function_with_python_side_effect(3))
</pre></div>
</div>
</div>
<p>Here, you see extra tracing because new Python arguments always trigger the creation of a new graph.</p>
</div>
<div class="section" id="Next-steps">
<h3>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You can read a more in-depth discussion at both the <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> API reference page and at the <a class="reference external" href="function">guide</a>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>