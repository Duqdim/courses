

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2020 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2020 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/autodiff.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2020-The-TensorFlow-Authors.">
<h1>Copyright 2020 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2020-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Introduction-to-gradients-and-automatic-differentiation">
<h2>Introduction to gradients and automatic differentiation<a class="headerlink" href="#Introduction-to-gradients-and-automatic-differentiation" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>1d3cf00b39274e659f411004c4b0de38|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>afc4f834f4e44b879e5c98915a056689|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>c57eeb3e191647c2a3e366d6a3a32806|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>872a1b1b0058464da9aa887c0984fb51|Download notebook</p>
</td></table><div class="section" id="Automatic-Differentiation-and-Gradients">
<h3>Automatic Differentiation and Gradients<a class="headerlink" href="#Automatic-Differentiation-and-Gradients" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation</a> is useful for implementing machine learning algorithms such as <a class="reference external" href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> for training neural networks.</p>
<p>In this guide, you will explore ways to compute gradients with TensorFlow, especially in <a class="reference internal" href="eager.html"><span class="doc">eager execution</span></a>.</p>
</div>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
</pre></div>
</div>
</div>
</div>
<div class="section" id="Computing-gradients">
<h3>Computing gradients<a class="headerlink" href="#Computing-gradients" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the <em>forward</em> pass. Then, during the <em>backward pass</em>, TensorFlow traverses this list of operations in reverse order to compute gradients.</p>
</div>
<div class="section" id="Gradient-tapes">
<h3>Gradient tapes<a class="headerlink" href="#Gradient-tapes" title="Enlazar permanentemente con este título">¶</a></h3>
<p>TensorFlow provides the <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>s. TensorFlow “records” relevant operations executed inside the context of a <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> onto a “tape”. TensorFlow then uses that tape to compute the gradients of a “recorded” computation using <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">reverse mode differentiation</a>.</p>
<p>Here is a simple example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(3.0)

with tf.GradientTape() as tape:
  y = x**2
</pre></div>
</div>
</div>
<p>Once you’ve recorded some operations, use <code class="docutils literal notranslate"><span class="pre">GradientTape.gradient(target,</span> <span class="pre">sources)</span></code> to calculate the gradient of some target (often a loss) relative to some source (often the model’s variables):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># dy = 2x * dx
dy_dx = tape.gradient(y, x)
dy_dx.numpy()
</pre></div>
</div>
</div>
<p>The above example uses scalars, but <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> works as easily on any tensor:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>w = tf.Variable(tf.random.normal((3, 2)), name=&#39;w&#39;)
b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=&#39;b&#39;)
x = [[1., 2., 3.]]

with tf.GradientTape(persistent=True) as tape:
  y = x @ w + b
  loss = tf.reduce_mean(y**2)
</pre></div>
</div>
</div>
<p>To get the gradient of <code class="docutils literal notranslate"><span class="pre">loss</span></code> with respect to both variables, you can pass both as sources to the <code class="docutils literal notranslate"><span class="pre">gradient</span></code> method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see <code class="docutils literal notranslate"><span class="pre">tf.nest</span></code>).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[dl_dw, dl_db] = tape.gradient(loss, [w, b])
</pre></div>
</div>
</div>
<p>The gradient with respect to each source has the shape of the source:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(w.shape)
print(dl_dw.shape)
</pre></div>
</div>
</div>
<p>Here is the gradient calculation again, this time passing a dictionary of variables:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>my_vars = {
    &#39;w&#39;: w,
    &#39;b&#39;: b
}

grad = tape.gradient(loss, my_vars)
grad[&#39;b&#39;]
</pre></div>
</div>
</div>
</div>
<div class="section" id="Gradients-with-respect-to-a-model">
<h3>Gradients with respect to a model<a class="headerlink" href="#Gradients-with-respect-to-a-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>It’s common to collect <code class="docutils literal notranslate"><span class="pre">tf.Variables</span></code> into a <code class="docutils literal notranslate"><span class="pre">tf.Module</span></code> or one of its subclasses (<code class="docutils literal notranslate"><span class="pre">layers.Layer</span></code>, <code class="docutils literal notranslate"><span class="pre">keras.Model</span></code>) for <a class="reference internal" href="checkpoint.html"><span class="doc">checkpointing</span></a> and <a class="reference internal" href="saved_model.html"><span class="doc">exporting</span></a>.</p>
<p>In most cases, you will want to calculate gradients with respect to a model’s trainable variables. Since all subclasses of <code class="docutils literal notranslate"><span class="pre">tf.Module</span></code> aggregate their variables in the <code class="docutils literal notranslate"><span class="pre">Module.trainable_variables</span></code> property, you can calculate these gradients in a few lines of code:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>layer = tf.keras.layers.Dense(2, activation=&#39;relu&#39;)
x = tf.constant([[1., 2., 3.]])

with tf.GradientTape() as tape:
  # Forward pass
  y = layer(x)
  loss = tf.reduce_mean(y**2)

# Calculate gradients with respect to every trainable variable
grad = tape.gradient(loss, layer.trainable_variables)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for var, g in zip(layer.trainable_variables, grad):
  print(f&#39;{var.name}, shape: {g.shape}&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Controlling-what-the-tape-watches">
<h3>Controlling what the tape watches<a class="headerlink" href="#Controlling-what-the-tape-watches" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The default behavior is to record all operations after accessing a trainable <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>. The reasons for this are:</p>
<ul class="simple">
<li><p>The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.</p></li>
<li><p>The tape holds references to intermediate outputs, so you don’t want to record unnecessary operations.</p></li>
<li><p>The most common use case involves calculating the gradient of a loss with respect to all a model’s trainable variables.</p></li>
</ul>
<p>For example, the following fails to calculate a gradient because the <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> is not “watched” by default, and the <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> is not trainable:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># A trainable variable
x0 = tf.Variable(3.0, name=&#39;x0&#39;)
# Not trainable
x1 = tf.Variable(3.0, name=&#39;x1&#39;, trainable=False)
# Not a Variable: A variable + tensor returns a tensor.
x2 = tf.Variable(2.0, name=&#39;x2&#39;) + 1.0
# Not a variable
x3 = tf.constant(3.0, name=&#39;x3&#39;)

with tf.GradientTape() as tape:
  y = (x0**2) + (x1**2) + (x2**2)

grad = tape.gradient(y, [x0, x1, x2, x3])

for g in grad:
  print(g)
</pre></div>
</div>
</div>
<p>You can list the variables being watched by the tape using the <code class="docutils literal notranslate"><span class="pre">GradientTape.watched_variables</span></code> method:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[var.name for var in tape.watched_variables()]
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> provides hooks that give the user control over what is or is not watched.</p>
<p>To record gradients with respect to a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>, you need to call <code class="docutils literal notranslate"><span class="pre">GradientTape.watch(x)</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant(3.0)
with tf.GradientTape() as tape:
  tape.watch(x)
  y = x**2

# dy = 2x * dx
dy_dx = tape.gradient(y, x)
print(dy_dx.numpy())
</pre></div>
</div>
</div>
<p>Conversely, to disable the default behavior of watching all <code class="docutils literal notranslate"><span class="pre">tf.Variables</span></code>, set <code class="docutils literal notranslate"><span class="pre">watch_accessed_variables=False</span></code> when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x0 = tf.Variable(0.0)
x1 = tf.Variable(10.0)

with tf.GradientTape(watch_accessed_variables=False) as tape:
  tape.watch(x1)
  y0 = tf.math.sin(x0)
  y1 = tf.nn.softplus(x1)
  y = y0 + y1
  ys = tf.reduce_sum(y)
</pre></div>
</div>
</div>
<p>Since <code class="docutils literal notranslate"><span class="pre">GradientTape.watch</span></code> was not called on <code class="docutils literal notranslate"><span class="pre">x0</span></code>, no gradient is computed with respect to it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)
grad = tape.gradient(ys, {&#39;x0&#39;: x0, &#39;x1&#39;: x1})

print(&#39;dy/dx0:&#39;, grad[&#39;x0&#39;])
print(&#39;dy/dx1:&#39;, grad[&#39;x1&#39;].numpy())
</pre></div>
</div>
</div>
</div>
<div class="section" id="Intermediate-results">
<h3>Intermediate results<a class="headerlink" href="#Intermediate-results" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You can also request gradients of the output with respect to intermediate values computed inside the <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> context.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant(3.0)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = x * x
  z = y * y

# Use the tape to compute the gradient of z with respect to the
# intermediate value y.
# dz_dx = 2 * y, where y = x ** 2
print(tape.gradient(z, y).numpy())
</pre></div>
</div>
</div>
<p>By default, the resources held by a <code class="docutils literal notranslate"><span class="pre">GradientTape</span></code> are released as soon as the <code class="docutils literal notranslate"><span class="pre">GradientTape.gradient</span></code> method is called. To compute multiple gradients over the same computation, create a gradient tape with <code class="docutils literal notranslate"><span class="pre">persistent=True</span></code>. This allows multiple calls to the <code class="docutils literal notranslate"><span class="pre">gradient</span></code> method as resources are released when the tape object is garbage collected. For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant([1, 3.0])
with tf.GradientTape(persistent=True) as tape:
  tape.watch(x)
  y = x * x
  z = y * y

print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)
print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>del tape   # Drop the reference to the tape
</pre></div>
</div>
</div>
</div>
<div class="section" id="Notes-on-performance">
<h3>Notes on performance<a class="headerlink" href="#Notes-on-performance" title="Enlazar permanentemente con este título">¶</a></h3>
<ul>
<li><p>There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.</p></li>
<li><p>Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.</p>
<p>For efficiency, some ops (like <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>) don’t need to keep their intermediate results and they are pruned during the forward pass. However, if you use <code class="docutils literal notranslate"><span class="pre">persistent=True</span></code> on your tape, <em>nothing is discarded</em> and your peak memory usage will be higher.</p>
</li>
</ul>
</div>
<div class="section" id="Gradients-of-non-scalar-targets">
<h3>Gradients of non-scalar targets<a class="headerlink" href="#Gradients-of-non-scalar-targets" title="Enlazar permanentemente con este título">¶</a></h3>
<p>A gradient is fundamentally an operation on a scalar.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.0)
with tf.GradientTape(persistent=True) as tape:
  y0 = x**2
  y1 = 1 / x

print(tape.gradient(y0, x).numpy())
print(tape.gradient(y1, x).numpy())
</pre></div>
</div>
</div>
<p>Thus, if you ask for the gradient of multiple targets, the result for each source is:</p>
<ul class="simple">
<li><p>The gradient of the sum of the targets, or equivalently</p></li>
<li><p>The sum of the gradients of each target.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.0)
with tf.GradientTape() as tape:
  y0 = x**2
  y1 = 1 / x

print(tape.gradient({&#39;y0&#39;: y0, &#39;y1&#39;: y1}, x).numpy())
</pre></div>
</div>
</div>
<p>Similarly, if the target(s) are not scalar the gradient of the sum is calculated:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.)

with tf.GradientTape() as tape:
  y = x * [3., 4.]

print(tape.gradient(y, x).numpy())
</pre></div>
</div>
</div>
<p>This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.</p>
<p>If you need a separate gradient for each item, refer to <a class="reference internal" href="advanced_autodiff.html#Jacobians"><span class="std std-ref">Jacobians</span></a>.</p>
<p>In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.linspace(-10.0, 10.0, 200+1)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = tf.nn.sigmoid(x)

dy_dx = tape.gradient(y, x)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.plot(x, y, label=&#39;y&#39;)
plt.plot(x, dy_dx, label=&#39;dy/dx&#39;)
plt.legend()
_ = plt.xlabel(&#39;x&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Control-flow">
<h3>Control flow<a class="headerlink" href="#Control-flow" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, <code class="docutils literal notranslate"><span class="pre">if</span></code> and <code class="docutils literal notranslate"><span class="pre">while</span></code> statements).</p>
<p>Here a different variable is used on each branch of an <code class="docutils literal notranslate"><span class="pre">if</span></code>. The gradient only connects to the variable that was used:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant(1.0)

v0 = tf.Variable(2.0)
v1 = tf.Variable(2.0)

with tf.GradientTape(persistent=True) as tape:
  tape.watch(x)
  if x &gt; 0.0:
    result = v0
  else:
    result = v1**2

dv0, dv1 = tape.gradient(result, [v0, v1])

print(dv0)
print(dv1)
</pre></div>
</div>
</div>
<p>Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.</p>
<p>Depending on the value of <code class="docutils literal notranslate"><span class="pre">x</span></code> in the above example, the tape either records <code class="docutils literal notranslate"><span class="pre">result</span> <span class="pre">=</span> <span class="pre">v0</span></code> or <code class="docutils literal notranslate"><span class="pre">result</span> <span class="pre">=</span> <span class="pre">v1**2</span></code>. The gradient with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code> is always <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>dx = tape.gradient(result, x)

print(dx)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Getting-a-gradient-of-None">
<h3>Getting a gradient of <code class="docutils literal notranslate"><span class="pre">None</span></code><a class="headerlink" href="#Getting-a-gradient-of-None" title="Enlazar permanentemente con este título">¶</a></h3>
<p>When a target is not connected to a source you will get a gradient of <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.)
y = tf.Variable(3.)

with tf.GradientTape() as tape:
  z = y * y
print(tape.gradient(z, x))
</pre></div>
</div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">z</span></code> is obviously not connected to <code class="docutils literal notranslate"><span class="pre">x</span></code>, but there are several less-obvious ways that a gradient can be disconnected.</p>
<div class="section" id="1.-Replaced-a-variable-with-a-tensor">
<h4>1. Replaced a variable with a tensor<a class="headerlink" href="#1.-Replaced-a-variable-with-a-tensor" title="Enlazar permanentemente con este título">¶</a></h4>
<p>In the section on <a class="reference external" href="#watches">“controlling what the tape watches”</a> you saw that the tape will automatically watch a <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> but not a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>.</p>
<p>One common error is to inadvertently replace a <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> with a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>, instead of using <code class="docutils literal notranslate"><span class="pre">Variable.assign</span></code> to update the <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>. Here is an example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable(2.0)

for epoch in range(2):
  with tf.GradientTape() as tape:
    y = x+1

  print(type(x).__name__, &quot;:&quot;, tape.gradient(y, x))
  x = x + 1   # This should be `x.assign_add(1)`
</pre></div>
</div>
</div>
</div>
<div class="section" id="2.-Did-calculations-outside-of-TensorFlow">
<h4>2. Did calculations outside of TensorFlow<a class="headerlink" href="#2.-Did-calculations-outside-of-TensorFlow" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The tape can’t record the gradient path if the calculation exits TensorFlow. For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable([[1.0, 2.0],
                 [3.0, 4.0]], dtype=tf.float32)

with tf.GradientTape() as tape:
  x2 = x**2

  # This step is calculated with NumPy
  y = np.mean(x2, axis=0)

  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor
  # using `tf.convert_to_tensor`.
  y = tf.reduce_mean(y, axis=0)

print(tape.gradient(y, x))
</pre></div>
</div>
</div>
</div>
<div class="section" id="3.-Took-gradients-through-an-integer-or-string">
<h4>3. Took gradients through an integer or string<a class="headerlink" href="#3.-Took-gradients-through-an-integer-or-string" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.</p>
<p>Nobody expects strings to be differentiable, but it’s easy to accidentally create an <code class="docutils literal notranslate"><span class="pre">int</span></code> constant or variable if you don’t specify the <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant(10)

with tf.GradientTape() as g:
  g.watch(x)
  y = x * x

print(g.gradient(y, x))
</pre></div>
</div>
</div>
<p>TensorFlow doesn’t automatically cast between types, so, in practice, you’ll often get a type error instead of a missing gradient.</p>
</div>
<div class="section" id="4.-Took-gradients-through-a-stateful-object">
<h4>4. Took gradients through a stateful object<a class="headerlink" href="#4.-Took-gradients-through-a-stateful-object" title="Enlazar permanentemente con este título">¶</a></h4>
<p>State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> is immutable. You can’t change a tensor once it’s created. It has a <em>value</em>, but no <em>state</em>. All the operations discussed so far are also stateless: the output of a <code class="docutils literal notranslate"><span class="pre">tf.matmul</span></code> only depends on its inputs.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> has internal state—its value. When you use the variable, the state is read. It’s normal to calculate a gradient with respect to a variable, but the variable’s state blocks gradient calculations from going farther back. For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x0 = tf.Variable(3.0)
x1 = tf.Variable(0.0)

with tf.GradientTape() as tape:
  # Update x1 = x1 + x0.
  x1.assign_add(x0)
  # The tape starts recording from x1.
  y = x1**2   # y = (x1 + x0)**2

# This doesn&#39;t work.
print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)
</pre></div>
</div>
</div>
<p>Similarly, <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> iterators and <code class="docutils literal notranslate"><span class="pre">tf.queue</span></code>s are stateful, and will stop all gradients on tensors that pass through them.</p>
</div>
</div>
<div class="section" id="No-gradient-registered">
<h3>No gradient registered<a class="headerlink" href="#No-gradient-registered" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Some <code class="docutils literal notranslate"><span class="pre">tf.Operation</span></code>s are <strong>registered as being non-differentiable</strong> and will return <code class="docutils literal notranslate"><span class="pre">None</span></code>. Others have <strong>no gradient registered</strong>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.raw_ops</span></code> page shows which low-level ops have gradients registered.</p>
<p>If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning <code class="docutils literal notranslate"><span class="pre">None</span></code>. This way you know something has gone wrong.</p>
<p>For example, the <code class="docutils literal notranslate"><span class="pre">tf.image.adjust_contrast</span></code> function wraps <code class="docutils literal notranslate"><span class="pre">raw_ops.AdjustContrastv2</span></code>, which could have a gradient but the gradient is not implemented:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>image = tf.Variable([[[0.5, 0.0, 0.0]]])
delta = tf.Variable(0.1)

with tf.GradientTape() as tape:
  new_image = tf.image.adjust_contrast(image, delta)

try:
  print(tape.gradient(new_image, [image, delta]))
  assert False   # This should not happen.
except LookupError as e:
  print(f&#39;{type(e).__name__}: {e}&#39;)

</pre></div>
</div>
</div>
<p>If you need to differentiate through this op, you’ll either need to implement the gradient and register it (using <code class="docutils literal notranslate"><span class="pre">tf.RegisterGradient</span></code>) or re-implement the function using other ops.</p>
</div>
<div class="section" id="Zeros-instead-of-None">
<h3>Zeros instead of None<a class="headerlink" href="#Zeros-instead-of-None" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In some cases it would be convenient to get 0 instead of <code class="docutils literal notranslate"><span class="pre">None</span></code> for unconnected gradients. You can decide what to return when you have unconnected gradients using the <code class="docutils literal notranslate"><span class="pre">unconnected_gradients</span></code> argument:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.Variable([2., 2.])
y = tf.Variable(3.)

with tf.GradientTape() as tape:
  z = y**2
print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>