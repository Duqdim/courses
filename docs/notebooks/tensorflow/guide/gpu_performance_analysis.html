

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>&lt;no title&gt; &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/gpu_performance_analysis.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p># Optimize TensorFlow GPU Performance with the TensorFlow Profiler</p>
<p>## Overview</p>
<p>This guide is for TensorFlow users utilizing GPUs to improve model performance.
Using the TensorFlow Profiler as the main tool to gain insight into performance,
this guide will help you debug when one or more of your GPUs are underutilized.
A quickstart guide to the TensorFlow Profiler can be found in the
[TensorFlow Profiler](<a class="reference external" href="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras</a>)
tutorial, and additional ways to obtain a profile are documented in the
[Optimize TensorFlow performance using the Profiler](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#collect_performance_data">https://www.tensorflow.org/guide/profiler#collect_performance_data</a>)
guide.</p>
<p>Keep in mind that offloading computations to GPU might not always be beneficial,
particularly for small models. There are overheads due to data transfer between
host (CPU) and device (GPU), as well as overheads due to the latency involved
when the host launches GPU kernels. Good performance is achieved when the host
successfully keeps the GPU occupied by offloading enough work.</p>
<p>### Performance Optimization Workflow</p>
<p>This guide outlines how to debug performance issues starting with a single GPU,
then moving to a single host with multiple GPUs. It is recommended to debug
performance issues in this order. For example, if you are using a TensorFlow
distribution strategy to train a model on a single host with multiple GPUs and
notice suboptimal GPU utilization, you should first optimize and debug
performance for 1 GPU, before debugging the multi-GPU system. The recommended
order is as follows:</p>
<ol class="arabic simple">
<li><p>Optimize and debug performance on 1 GPU
1.  Check if input pipeline is a bottleneck
2.  Debug performance of 1 GPU
3.  Enable fp16 and optionally enable XLA</p></li>
<li><p>Optimize and debug performance on multi-GPU single host</p></li>
</ol>
<p>As a baseline for getting performant code on GPU, this guide assumes you are
already using <cite>tf.function</cite>. The Keras compile/fit API will utilize
<cite>tf.function</cite> automatically under the hood. When writing a custom training loop,
refer to [this guide](<a class="reference external" href="https://www.tensorflow.org/guide/function">https://www.tensorflow.org/guide/function</a>) on how to
enable <cite>tf.function</cite>.</p>
<p>The next sections discuss suggested approaches for each of the scenarios above
to help identify and fix performance bottlenecks.</p>
<p>## Optimize Performance on 1 GPU</p>
<p>In an ideal case, your program should have high GPU utilization, minimal CPU
(host) to GPU (device) communication, and no overhead from the input pipeline.
The first step to analyzing performance is to get a profile for a model running
with one GPU.</p>
<p>The [Overview Page](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#overview_page">https://www.tensorflow.org/guide/profiler#overview_page</a>) of
the TensorFlow Profiler provides an idea of how far away your program is from
the ideal scenario.</p>
<p>![TensorFlow Profiler Overview Page](images/gpu_perf_analysis/overview_page.png «The overview page of the TensorFlow Profiler»)</p>
<p>The key numbers to look for on the overview page are:</p>
<ol class="arabic simple">
<li><p>How much of the step time is from actual device execution</p></li>
<li><p>The percentage of ops placed on device vs host</p></li>
<li><p>How many kernels use fp16</p></li>
</ol>
<p>Achieving optimal performance means maximizing these numbers in all three cases.
To get an in-depth understanding of your program, you will need to be familiar
with the TensorFlow Profiler
[trace viewer](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#trace_viewer">https://www.tensorflow.org/guide/profiler#trace_viewer</a>). The
sections below show some common trace viewer patterns that you should look for
when diagnosing performance bottlenecks.</p>
<p>Below is an image of a model trace view running on 1 GPU. From the _Tensorflow
Name <a href="#id1"><span class="problematic" id="id2">Scope_</span></a> and _Tensorflow <a href="#id3"><span class="problematic" id="id4">Ops_</span></a> sections, you can identify different parts of
the model, like the forward pass, the loss function, backward pass/gradient
calculation, and the optimizer weight update. You can also see the operations
running on the GPU next to each _Stream_, which refer to CUDA streams. Each
stream is used for specific tasks. In this trace, _Stream#118_ is used to launch
compute kernels and device to device copies. _Stream#119_ is used for host to
device copy and <em>Stream#120</em> for device to host copy.</p>
<p>![image](images/gpu_perf_analysis/traceview_ideal.png «An example TensorFlow Profiler trace view»)</p>
<p>This trace shows common characteristics of a performant model. For example, the
GPU compute timeline (_Stream#118_) looks busy with very few gaps. There are
minimal copies from host to device (_Stream #119_) and from device to host
(_Stream #120_), as well as minimal gaps between steps. When you run the
TensorFlow Profiler for your program, you might not see these ideal
characteristics in your trace view. The rest of this guide covers common
scenarios and how to fix them.</p>
<p>### Debug Input Pipeline</p>
<p>The first step in GPU performance debugging is to determine if your program is
input-bound. The easiest way to figure this out is to use the TensorFlow
Profiler’s
[Input-Pipeline Analyzer](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#input_pipeline_analyzer">https://www.tensorflow.org/guide/profiler#input_pipeline_analyzer</a>),
which gives an overview of time spent in the input pipeline.</p>
<p>![image](images/gpu_perf_analysis/input_pipeline_analyzer.png «TensorFlow Profiler Input-Analyzer»)</p>
<p>The following are potential actions you can take if your input-pipeline
contributes significantly to step time:</p>
<ul class="simple">
<li><p>Refer to the <cite>tf.data</cite> specific
[guide](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance_analysis">https://www.tensorflow.org/guide/data_performance_analysis</a>) to learn
how to debug your input pipeline.</p></li>
<li><p>Another quick way to check if the input pipeline is the bottleneck is to use
randomly generated input data that does not need any pre-processing.
[Here is an example](<a class="reference external" href="https://github.com/tensorflow/models/blob/4a5770827edf1c3974274ba3e4169d0e5ba7478a/official/vision/image_classification/resnet/resnet_runnable.py#L50-L57">https://github.com/tensorflow/models/blob/4a5770827edf1c3974274ba3e4169d0e5ba7478a/official/vision/image_classification/resnet/resnet_runnable.py#L50-L57</a>)
of using this technique for a ResNet model. If the input pipeline is optimal
you should see similar performance with real data and with generated
random/synthetic data. The only overhead in the synthetic data case will be
due to input data copy which again can be prefetched and optimized.</p></li>
</ul>
<p>### Debug Performance of 1 GPU</p>
<p>There are several factors that can contribute to low GPU utilization. Below are
some scenarios commonly observed when looking at the trace viewer and potential
solutions.</p>
<p>#### Analyze Gaps Between Steps</p>
<p>A common observation when your program is not running optimally is gaps between
training steps. In the image below, there is a large gap between steps 8 and 9,
meaning that the GPU is idle during that time.</p>
<p>![image](images/gpu_perf_analysis/traceview_step_gaps.png «TensorFlow Profile trace view showing gaps between steps»)</p>
<p>If your trace viewer shows large gaps between steps, this could be an indication
that your program is input bound. In that case you should refer to the previous
section on debugging your input pipeline if you haven’t already done so.
However, even with an optimized input pipeline, you can still see gaps between
the end of one step and the start of another due to CPU thread contention.
<cite>tf.data</cite> makes use of background threads to parallelize pipeline processing.
These threads may interfere with GPU host-side activity that happens at the
beginning of each step, such as copying data or scheduling GPU operations.</p>
<p>If you see large gaps on the host side, which schedules these ops on the GPU,
you can set the environment variable <cite>TF_GPU_THREAD_MODE=gpu_private</cite>. This
ensures that GPU kernels are launched from their own dedicated threads, and
don’t get queued behind <cite>tf.data</cite> work.</p>
<p>Gaps between steps can also be caused by metric calculations, Keras callbacks,
or ops outside of <cite>tf.function</cite> that run on the host. These ops don’t have as
good performance as the ops inside a TensorFlow graph. Additionally, some of
these ops run on the CPU and copy tensors back and forth from the GPU.</p>
<p>If after optimizing your input pipeline you still notice gaps between steps in
the trace viewer, you should look at the model code between steps, and see if
disabling callbacks/metrics improves performance. Some details of these ops are
also on the trace viewer (both device and host side).The recommendation in this
scenario is to amortize the overhead of these ops by executing them after a
fixed number of steps instead of every step. When using the <cite>compile</cite> method in
the <cite>tf.keras</cite> API, setting the <cite>experimental_steps_per_execution</cite> flag does
this automatically. For custom training loops, use <cite>tf.while_loop</cite>.</p>
<p>#### Achieve Higher Device Utilization</p>
<p>##### Small GPU Kernels and Host Kernel Launch Delays</p>
<p>The host enqueues kernels to be run on the GPU, but there is a latency (around
20-40 μs) involved before kernels are actually executed on the GPU. In an ideal
case, the host enqueues enough kernels on the GPU such that the GPU spends most
of its time executing, rather than waiting on the host to enqueue more kernels.</p>
<p>The TensorFlow Profiler’s
[Overview Page](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#overview_page">https://www.tensorflow.org/guide/profiler#overview_page</a>) shows
how much time the GPU was idle due to waiting on the host to launch kernels. In
the image below, the GPU is idle for about 10% of the step time waiting on
kernels to be launched.</p>
<p>![image](images/gpu_perf_analysis/performance_summary.png «Summary of performance from TensorFlow Profile»)</p>
<p>The trace viewer for this same program shows small gaps between kernels where
the host is busy launching kernels on the GPU.</p>
<p>![image](images/gpu_perf_analysis/traceview_kernel_gaps.png «TensorFlow Profile trace view demonstrating gaps between kernels»)</p>
<p>By launching a lot of small ops on the GPU (like a scalar add for example), the
host might not keep up with the GPU. The
[Tensorflow Stats](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#tensorflow_stats">https://www.tensorflow.org/guide/profiler#tensorflow_stats</a>)
page for the same TensorFlow Profile shows 126,224 Mul operations taking 2.77
seconds. Thus, each kernel is about 21.9 μs, which is very small (around the
same time as launch latency) and can potentially result in host kernel launch
delays.</p>
<p>![image](images/gpu_perf_analysis/tensorflow_stats_page.png «TensorFlow Profile stats page»)</p>
<p>If your trace viewer shows many small gaps between ops on the GPU like the image
above, you can:</p>
<ul class="simple">
<li><p>Concatenate small tensors and use vectorized operations or use a larger
batch size to make each kernel launched do more work, which will keep the
GPU busy for longer.</p></li>
<li><p>Make sure you’re using <cite>tf.function</cite> to create TF graphs and not running
operations in a pure eager mode (Using <cite>tf.keras.Model.compile</cite>
automatically does this).</p></li>
<li><p>Fuse kernels using XLA. For more details, see the section below on how to
enable XLA to get higher performance. This is an experimental feature, but
leads to high device utilization.</p></li>
</ul>
<p>##### Tensorflow Op Placement</p>
<p>The TensorFlow Profiler
[Overview Page](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#overview_page">https://www.tensorflow.org/guide/profiler#overview_page</a>) tells
you the percentage of ops placed on the host vs device (you can also verify the
placement of specific ops by looking at the trace viewer). Like in the image
below, you want the percentage of ops on the host to be very small compared to
the device.</p>
<p>![image](images/gpu_perf_analysis/opp_placement.png «TF Op Placement»)</p>
<p>Ideally most of the compute intensive ops should be placed on the GPU. To find
out which devices the operations and tensors in your model are assigned to, set
<cite>tf.debugging.set_log_device_placement(True)</cite> as the first statement of your
program. Note that in some cases, even if you specify an op to be placed on a
particular device, its implementation might override this condition
(example:<cite>tf.unique</cite>). Even for single GPU training, specifying a distribution
strategy, such as <cite>tf.distribute.OneDeviceStrategy</cite>, can result in more
deterministic placement of ops on your device.</p>
<p>One reason for having the majority of ops placed on the GPU is to prevent
excessive memory copies between host and device (memory copies for model
input/output data between host and device are expected). An example of excessive
copying can be seen in the trace view below on GPU streams _#167_, _#168_, and
_#169_.</p>
<p>![image](images/gpu_perf_analysis/traceview_excessive_copy.png «TensorFlow Profile trace view demonstrating excessive H2D/D2H copies»)</p>
<p>These copies can sometimes hurt performance if they block GPU kernels from
executing. Memory copy operations in the trace viewer have more information
about the ops that are the source of these copied tensors, but it might not
always be easy to associate a memCopy with an op. In these cases, it is helpful
to look at the ops nearby to see if the memory copy happens at the same location
in every step.</p>
<p>#### More Efficient Kernels on GPUs</p>
<p>Once your program’s GPU utilization is acceptable, the next step is to look into
increasing the efficiency of the GPU kernels by utilizing Tensor Cores or fusing
ops.</p>
<p>##### Utilizing Tensor Cores</p>
<p>Modern GPUs have specialized tensor cores that can significantly improve the
performance of kernels that are eligible. The
[GPU kernel stats page](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#gpu_kernel_stats">https://www.tensorflow.org/guide/profiler#gpu_kernel_stats</a>)
indicates which GPU kernels are Tensor Core eligible, and which kernels are
using the Tensor Core. Enabling fp16 (see Enabling Mixed Precision section
below) is one way to make your program’s General Matrix Multiply (GEMM) kernels
(matmul ops) utilize the Tensor Core.</p>
<p>For other detailed recommendations on how to make kernels efficient for GPUs,
refer to the
[NVIDIA Deep Learning Performance](<a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/index.html#perf-guidelines">https://docs.nvidia.com/deeplearning/performance/index.html#perf-guidelines</a>)
guide, which covers a variety of techniques you can experiment with such as
using NCHW vs NHWC format to represent inputs, or making the input dimensions to
be a multiple of 8.</p>
<p>##### Fusing Ops</p>
<p>With the <cite>tf.xla.experimental_compile</cite> feature, TensorFlow can fuse smaller ops
to form bigger kernels leading to significant performance gains. More details
are discussed in the Enable XLA section below.</p>
<p>### Enable fp16 and XLA</p>
<p>After following the above steps, enabling mixed-precision and XLA are two
optional steps you can take to improve performance further. The suggested
approach is to enable them one by one and verify that the performance benefits
are as expected.</p>
<p>#### Enabling Mixed Precision</p>
<p>The TensorFlow
[Mixed Precision](<a class="reference external" href="https://www.tensorflow.org/guide/keras/mixed_precision">https://www.tensorflow.org/guide/keras/mixed_precision</a>) guide
shows how to enable fp16 precision on GPUs. When it comes to realizing the
performance benefits of fp16 there are a few pointers to keep in mind.</p>
<p>##### Using Optimal fp16 Kernels</p>
<p>With fp16 enabled, your program’s matrix multiplications (GEMM) kernels, should
use the corresponding fp16 version that utilizes the Tensor Cores. However, in
some cases, this does not happen and you do not see the expected speedup from
enabling fp16, as your program falls back to the inefficient implementation
instead.</p>
<p>![image](images/gpu_perf_analysis/gpu_kernels.png «TensorFlow Profile GPU Kernel Stats page»)</p>
<p>The [GPU kernel](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#gpu_kernel_stats">https://www.tensorflow.org/guide/profiler#gpu_kernel_stats</a>)
stats page shows which ops are Tensor Core eligible and which kernels are
actually using the efficient Tensor Core. The
[NVIDIA guide on deep learning performance](<a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#opt-tensor-cores">https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#opt-tensor-cores</a>)
contains additional suggestions on how to leverage Tensor Cores. Additionally,
the benefits of fp16 will also show in kernels that were previously memory
bound, as now the operation will take half the time.</p>
<p>##### Dynamic vs Static Loss Scaling</p>
<p>Loss scaling is necessary when using fp16 to prevent underflow due to low
precision. There are two types of loss scaling, dynamic and static, both of
which are explained in greater detail in the
[Mixed Precision](<a class="reference external" href="https://www.tensorflow.org/guide/keras/mixed_precision">https://www.tensorflow.org/guide/keras/mixed_precision</a>) guide.
When trying to optimize performance, it is important to remember that dynamic
loss scaling can introduce additional conditional ops that run on the host, and
lead to gaps that will be visible between steps in the trace viewer. On the
other hand, static loss scaling does not have such overheads and can be a better
option in terms of performance with the catch that you need to specify the
correct static-loss scale value.</p>
<p>#### Enabling XLA</p>
<p>As the final step to getting the best performance with a single GPU, you can
experiment with enabling XLA, which will fuse ops and lead to better device
utilization and a lower memory footprint. For details on how to enable XLA in
your program, refer to the
[XLA: Optimizing Compiler for Machine Learning](<a class="reference external" href="https://www.tensorflow.org/xla">https://www.tensorflow.org/xla</a>)
guide.</p>
<p>## Optimize Performance on Multi-GPU Single Host</p>
<p>The <cite>tf.distribute.MirroredStrategy</cite> API can be used to scale model training
from 1 GPU to multiple GPUs on a single host. To learn more about how to do
distributed training with Tensorflow, please refer to the
[Distributed training with Keras](<a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/keras">https://www.tensorflow.org/tutorials/distribute/keras</a>)
guide. Although the transition from one GPU to multiple GPUs should ideally be
scalable out of the box, you can sometimes encounter performance issues.</p>
<p>When going from training with a single GPU to multiple GPUs on the same host,
ideally you should see performance scaling with only the additional overhead of
gradient communication and increased host thread utilization. Because of this
overhead, you will not see an exact 2x speedup if you move from 1 to 2 GPUs for
example. The trace view below shows an example of the extra communication
overhead when training on multiple GPUs. There is some overhead to concatenate
the gradients, communicate them across replicas, and split them before doing the
weight update.</p>
<p>![image](images/gpu_perf_analysis/traceview_multi_gpu.png «TensorFlow Profile trace view for single host multi GPU scenario»)</p>
<p>The following checklist will help you to achieve better performance when
optimizing performance in the multi-GPU scenario:</p>
<ol class="arabic simple">
<li><p>Try to maximize the batch size, which will lead to higher device utilization
and amortize the costs of communication across multiple GPUs. Using the
[memory profiler](<a class="reference external" href="https://www.tensorflow.org/guide/profiler#memory_profile_summary">https://www.tensorflow.org/guide/profiler#memory_profile_summary</a>)
helps get a sense of how close your program is to peak memory utilization.
Note that while a higher batch size can affect convergence, this is usually
outweighed by the performance benefits.</p></li>
<li><p>When moving from a single GPU to multiple GPUs, the same host now has to
process much more input data. So after (1) it is recommended to re-check the
input pipeline performance and make sure it is not a bottleneck.</p></li>
<li><p>Check the GPU timeline in your program’s trace view to see if there are
uncecessary AllReduce calls, as this results in a synchronization across all
devices. In the trace view shown above, the AllReduce is done via the NCCL
kernel, and there is only one NCCL call on each GPU for the gradients on
each step.</p></li>
<li><p>Check for unnecessary D2H, H2D and D2D copy operations and see if they can
be minimized.</p></li>
<li><p>Check the step time to make sure each replica is doing the same work. It can
happen that one GPU (typically GPU0) is oversubscribed because the host
mistakenly ends up putting more work on it.</p></li>
<li><p>Lastly, check the training step across all GPUs in your trace view for any
ops that are executing sequntially. This usually happens when your program
includes control dependencies from one GPU to another. Debugging performance
in this situation has been solved on a case-by-case basis in the past. If
you observe this behavior in your program,
[file a Github issue](<a class="reference external" href="https://github.com/tensorflow/tensorflow/issues/new/choose">https://github.com/tensorflow/tensorflow/issues/new/choose</a>)
with images of your trace view.</p></li>
</ol>
<p>#### Optimize Gradient AllReduce</p>
<p>When training with a synchronous strategy, each device receives a portion of the
input data. After computing the forward and backwards passes through the model,
the gradients calculated on each device need to be aggregated and reduced. This
gradient AllReduce happens after the gradient calculation on each device, and
before the optimizer updates the model weights. Each GPU first concatenates the
gradients across the model layers, communicates them across GPUs using
<cite>tf.distribute.CrossDeviceOps</cite> (<cite>tf.distribute.NcclAllReduce</cite> is the default),
and then returns the gradients after reduction per layer. The optimizer will use
these reduced gradients to update the weights of your model. Ideally, this
process should happen at the same time on all GPUs to prevent any overheads. The
time to AllReduce should be approximately the same as:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">(number</span> <span class="pre">of</span> <span class="pre">parameters</span> <span class="pre">*</span> <span class="pre">4bytes)/</span> <span class="pre">(communication</span> <span class="pre">bandwidth)</span>
<span class="pre">`</span></code></p>
<p>This calculation is useful as a quick check to understand whether the
performance you see when running a distributed training job is as expected, or
if you need to do further performance debugging. You can get the number of
parameters in your model from <cite>tf.keras.Model.summary</cite>.</p>
<p>Note that each model parameter is 4 bytes since Tensorflow uses fp32 to
communicate gradients. Even when you have enabled fp16, NCCL AllReduce utilizes
fp32 parameters. In the future, Tensorflow will support AllReduce operations
using fp16, as well as pipelining the gradient AllReduce so it overlaps with the
gradient computation.</p>
<p>To see benefits of scaling, the step-time needs to be much higher compared to
these overheads. One way to achieve this is to use a higher-batch size since
batch size affects step time, but does not affect the communication overheads.</p>
<p>#### GPU Host Thread Contention</p>
<p>When running multiple GPUs, the CPU’s job is to keep all of the devices busy by
efficiently launching GPU kernels across the devices. However, when there are a
lot of independent operations that the CPU can schedule on one GPU, the CPU can
decide to use a lot of its host threads to keep one GPU busy, and then launch
kernels on another GPU in a non-deterministic order. This can cause a skew or
negative scaling, which can hurt performance.</p>
<p>The trace viewer below shows the overhead when the CPU staggers GPU kernel
launchs inefficiently, as GPU1 is idle and then starts running ops after GPU2
has started.</p>
<p>![image](images/gpu_perf_analysis/traceview_gpu_idle.png «TensorFlow Profile device trace view demonstrating inefficient kernel launch»)</p>
<p>The trace view for the host shows that the host is launching kernels on GPU2
before launching them on GPU1 (note that the below tf_Compute* ops are not
indicative of CPU threads).</p>
<p>![image](images/gpu_perf_analysis/traceview_host_contention.png «TensorFlow Profile host trace view demonstrating inefficient kernel launch»)</p>
<p>If you see this staggering of GPU kernels in your program’s trace view, the
recommended action is to:</p>
<ul class="simple">
<li><p>Set the TensorFlow environment variable <cite>TF_GPU_THREAD_MODE</cite> to
<cite>gpu_private</cite>. This environment variable will tell the host to keep threads
for a GPU private.</p></li>
<li><p>By default,`TF_GPU_THREAD_MODE=gpu_private` sets the number of threads to 2,
which is sufficient in most cases. However, that number can be changed by
setting the TensorFlow environment variable <cite>TF_GPU_THREAD_COUNT</cite> to the
desired number of threads.</p></li>
</ul>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>