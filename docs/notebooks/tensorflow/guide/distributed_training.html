

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2018 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2018 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/distributed_training.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2018-The-TensorFlow-Authors.">
<h1>Copyright 2018 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2018-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Distributed-training-with-TensorFlow">
<h2>Distributed training with TensorFlow<a class="headerlink" href="#Distributed-training-with-TensorFlow" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>86f572f6d81b4f12a46d89e0b5839e1f|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>a389d9f4a91e47d8bacf212b8263e259|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>d1115fe493964965a9820fa279b248df|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>8b79d654acf24e2abe1e8c6813660675|Download notebook</p>
</td></table><div class="section" id="Overview">
<h3>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.</p>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> has been designed with these key goals in mind:</p>
<ul class="simple">
<li><p>Easy to use and support multiple user segments, including researchers, ML engineers, etc.</p></li>
<li><p>Provide good performance out of the box.</p></li>
<li><p>Easy switching between strategies.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> can be used with a high-level API like <a class="reference external" href="https://www.tensorflow.org/guide/keras">Keras</a>, and can also be used to distribute custom training loops (and, in general, any computation using TensorFlow).</p>
<p>In TensorFlow 2.x, you can execute your programs eagerly, or in a graph using <code class="docutils literal notranslate"><span class="pre">`tf.function</span></code> &lt;function.ipynb&gt;`__. <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> intends to support both these modes of execution, but works best with <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>. Eager mode is only recommended for debugging purpose and not supported for <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>. Although training is the focus of this guide, this API can also be used for distributing evaluation and prediction on different platforms.</p>
<p>You can use <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with very few changes to your code, because we have changed the underlying components of TensorFlow to become strategy-aware. This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints.</p>
<p>In this guide, we explain various types of strategies and how you can use them in different situations. To learn how to debug performance issues, see the <a class="reference internal" href="gpu_performance_analysis.html"><span class="doc">Optimize TensorFlow GPU Performance</span></a> guide.</p>
<p>Note: For a deeper understanding of the concepts, please watch <a class="reference external" href="https://youtu.be/jKV53r9-H14">this deep-dive presentation</a>. This is especially recommended if you plan to write your own training loop.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Import TensorFlow
import tensorflow as tf
</pre></div>
</div>
</div>
</div>
<div class="section" id="Types-of-strategies">
<h3>Types of strategies<a class="headerlink" href="#Types-of-strategies" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> intends to cover a number of use cases along different axes. Some of these combinations are currently supported and others will be added in the future. Some of these axes are:</p>
<ul class="simple">
<li><p><em>Synchronous vs asynchronous training:</em> These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.</p></li>
<li><p><em>Hardware platform:</em> You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.</p></li>
</ul>
<p>In order to support these use cases, there are six strategies available. The next section explains which of these are supported in which scenarios in TF. Here is a quick overview:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 14%" />
<col style="width: 21%" />
<col style="width: 21%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Training API</p></th>
<th class="head"><p>MirroredStrategy</p></th>
<th class="head"><p>TPUStrategy</p></th>
<th class="head"><p>MultiWorkerMirroredStrategy</p></th>
<th class="head"><p>CentralStorageStrategy</p></th>
<th class="head"><p>ParameterServerStrategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Keras API</strong></p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
<td><p>Experimental support</p></td>
<td><p>Supported planned post 2.4</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Custom training loop</strong></p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
<td><p>Experimental support</p></td>
<td><p>Experimental support</p></td>
</tr>
<tr class="row-even"><td><p><strong>Estimator API</strong></p></td>
<td><p>Limited Support</p></td>
<td><p>Not supported</p></td>
<td><p>Limited Support</p></td>
<td><p>Limited Support</p></td>
<td><p>Limited Support</p></td>
</tr>
</tbody>
</table>
<p>Note: <a class="reference external" href="https://www.tensorflow.org/guide/versions#what_is_not_covered">Experimental support</a> means the APIs are not covered by any compatibilities guarantees.</p>
<p>Note: Estimator support is limited. Basic training and evaluation are experimental, and advanced features—such as scaffold—are not implemented. We recommend using Keras or custom training loops if a use case is not covered.</p>
<div class="section" id="MirroredStrategy">
<h4>MirroredStrategy<a class="headerlink" href="#MirroredStrategy" title="Enlazar permanentemente con este título">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Together, these variables form a single conceptual variable called <code class="docutils literal notranslate"><span class="pre">MirroredVariable</span></code>. These variables are kept in sync with each other by applying identical updates.</p>
<p>Efficient all-reduce algorithms are used to communicate the variable updates across the devices. All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device. It’s a fused algorithm that is very efficient and can reduce the overhead of synchronization significantly. There are many all-reduce algorithms and implementations available, depending on the type of communication available between devices. By default, it uses NVIDIA NCCL as the
all-reduce implementation. You can choose from a few other options, or write your own.</p>
<p>Here is the simplest way of creating <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mirrored_strategy = tf.distribute.MirroredStrategy()
</pre></div>
</div>
</div>
<p>This will create a <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> instance which will use all the GPUs that are visible to TensorFlow, and use NCCL as the cross device communication.</p>
<p>If you wish to use only some of the GPUs on your machine, you can do so like this:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mirrored_strategy = tf.distribute.MirroredStrategy(devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;])
</pre></div>
</div>
</div>
<p>If you wish to override the cross device communication, you can do so using the <code class="docutils literal notranslate"><span class="pre">cross_device_ops</span></code> argument by supplying an instance of <code class="docutils literal notranslate"><span class="pre">tf.distribute.CrossDeviceOps</span></code>. Currently, <code class="docutils literal notranslate"><span class="pre">tf.distribute.HierarchicalCopyAllReduce</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute.ReductionToOneDevice</span></code> are two options other than <code class="docutils literal notranslate"><span class="pre">tf.distribute.NcclAllReduce</span></code> which is the default.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mirrored_strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
</pre></div>
</div>
</div>
</div>
<div class="section" id="TPUStrategy">
<h4>TPUStrategy<a class="headerlink" href="#TPUStrategy" title="Enlazar permanentemente con este título">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.TPUStrategy</span></code> lets you run your TensorFlow training on Tensor Processing Units (TPUs). TPUs are Google’s specialized ASICs designed to dramatically accelerate machine learning workloads. They are available on Google Colab, the <a class="reference external" href="https://www.tensorflow.org/tfrc">TensorFlow Research Cloud</a> and <a class="reference external" href="https://cloud.google.com/tpu">Cloud TPU</a>.</p>
<p>In terms of distributed training architecture, <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code> is the same <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> - it implements synchronous distributed training. TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores, which are used in <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>.</p>
<p>Here is how you would instantiate <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>:</p>
<p>Note: To run this code in Colab, you should select TPU as the Colab runtime. See <a class="reference external" href="https://www.tensorflow.org/guide/tpu">TensorFlow TPU Guide</a>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu=tpu_address)
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
tpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">TPUClusterResolver</span></code> instance helps locate the TPUs. In Colab, you don’t need to specify any arguments to it.</p>
<p>If you want to use this for Cloud TPUs: - You must specify the name of your TPU resource in the <code class="docutils literal notranslate"><span class="pre">tpu</span></code> argument. - You must initialize the tpu system explicitly at the <em>start</em> of the program. This is required before TPUs can be used for computation. Initializing the tpu system also wipes out the TPU memory, so it’s important to complete this step first in order to avoid losing state.</p>
</div>
<div class="section" id="MultiWorkerMirroredStrategy">
<h4>MultiWorkerMirroredStrategy<a class="headerlink" href="#MultiWorkerMirroredStrategy" title="Enlazar permanentemente con este título">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code> is very similar to <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code>, it creates copies of all variables in the model on each device across all workers.</p>
<p>Here is the simplest way of creating <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>strategy = tf.distribute.MultiWorkerMirroredStrategy()
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> has two implementations for cross-device communications. <code class="docutils literal notranslate"><span class="pre">CommunicationImplementation.RING</span></code> is RPC-based and supports both CPU and GPU. <code class="docutils literal notranslate"><span class="pre">CommunicationImplementation.NCCL</span></code> uses <a class="reference external" href="https://developer.nvidia.com/nccl">Nvidia’s NCCL</a> and provides the state of art performance on GPU, but it doesn’t support CPU. <code class="docutils literal notranslate"><span class="pre">CollectiveCommunication.AUTO</span></code> defers the choice to Tensorflow. You can specify them in the following way:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>communication_options = tf.distribute.experimental.CommunicationOptions(
    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)
strategy = tf.distribute.MultiWorkerMirroredStrategy(
    communication_options=communication_options)
</pre></div>
</div>
</div>
<p>One of the key differences to get multi worker training going, as compared to multi-GPU training, is the multi-worker setup. The <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable is the standard way in TensorFlow to specify the cluster configuration to each worker that is part of the cluster. Learn more about <a class="reference external" href="#TF_CONFIG">setting up TF_CONFIG</a>.</p>
</div>
<div class="section" id="ParameterServerStrategy">
<h4>ParameterServerStrategy<a class="headerlink" href="#ParameterServerStrategy" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Parameter server training is a common data-parallel method to scale up model training on multiple machines. A parameter server training cluster consists of workers and parameter servers. Variables are created on parameter servers and they are read and updated by workers in each step. Please see the <a class="reference external" href="../tutorials/distribute/parameter_server_training.ipynb">parameter server training tutorial</a> for details.</p>
<p>TensorFlow 2 parameter server training uses a central-coordinator based architecture via the <code class="docutils literal notranslate"><span class="pre">tf.distribute.experimental.coordinator.ClusterCoordinator</span></code> class.</p>
<p>In this implementation the <code class="docutils literal notranslate"><span class="pre">worker</span></code> and <code class="docutils literal notranslate"><span class="pre">parameter</span> <span class="pre">server</span></code> tasks run <code class="docutils literal notranslate"><span class="pre">tf.distribute.Server</span></code>s that listen for tasks from the coordinator. The coordinator creates resources, dispatches training tasks, writes checkpoints, and deals with task failures.</p>
<p>In the programming running on the coordinator, you will use a <code class="docutils literal notranslate"><span class="pre">ParameterServerStrategy</span></code> object to define a training step and use a <code class="docutils literal notranslate"><span class="pre">ClusterCoordinator</span></code> to dispatch training steps to remote workers. Here is the simplest way to create them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">ParameterServerStrategy</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">TFConfigClusterResolver</span><span class="p">(),</span>
    <span class="n">variable_partitioner</span><span class="o">=</span><span class="n">variable_partitioner</span><span class="p">)</span>
<span class="n">coordinator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">coordinator</span><span class="o">.</span><span class="n">ClusterCoordinator</span><span class="p">(</span>
    <span class="n">strategy</span><span class="p">)</span>
</pre></div>
</div>
<p>Note you will need to configure TF_CONFIG environment variable if you use <code class="docutils literal notranslate"><span class="pre">TFConfigClusterResolver</span></code>. It is similar to <a class="reference external" href="#TF_CONFIG">TF_CONFIG</a> in <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> but has additional caveats.</p>
<p>In TF 1, <code class="docutils literal notranslate"><span class="pre">ParameterServerStrategy</span></code> is available only with estimator via <code class="docutils literal notranslate"><span class="pre">tf.compat.v1.distribute.experimental.ParameterServerStrategy</span></code> symbol.</p>
<p>Note: This strategy is <code class="docutils literal notranslate"><span class="pre">`experimental</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/guide/versions#what_is_not_covered">https://www.tensorflow.org/guide/versions#what_is_not_covered</a>&gt;`__ as it is currently under active development.</p>
</div>
<div class="section" id="CentralStorageStrategy">
<h4>CentralStorageStrategy<a class="headerlink" href="#CentralStorageStrategy" title="Enlazar permanentemente con este título">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.experimental.CentralStorageStrategy</span></code> does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.</p>
<p>Create an instance of <code class="docutils literal notranslate"><span class="pre">CentralStorageStrategy</span></code> by:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()
</pre></div>
</div>
</div>
<p>This will create a <code class="docutils literal notranslate"><span class="pre">CentralStorageStrategy</span></code> instance which will use all visible GPUs and CPU. Update to variables on replicas will be aggregated before being applied to variables.</p>
<p>Note: This strategy is <code class="docutils literal notranslate"><span class="pre">`experimental</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/guide/versions#what_is_not_covered">https://www.tensorflow.org/guide/versions#what_is_not_covered</a>&gt;`__ as it is currently a work in progress.</p>
</div>
<div class="section" id="Other-strategies">
<h4>Other strategies<a class="headerlink" href="#Other-strategies" title="Enlazar permanentemente con este título">¶</a></h4>
<p>In addition to the above strategies, there are two other strategies which might be useful for prototyping and debugging when using <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> APIs.</p>
<div class="section" id="Default-Strategy">
<h5>Default Strategy<a class="headerlink" href="#Default-Strategy" title="Enlazar permanentemente con este título">¶</a></h5>
<p>Default strategy is a distribution strategy which is present when no explicit distribution strategy is in scope. It implements the <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> interface but is a pass-through and provides no actual distribution. For instance, <code class="docutils literal notranslate"><span class="pre">strategy.run(fn)</span></code> will simply call <code class="docutils literal notranslate"><span class="pre">fn</span></code>. Code written using this strategy should behave exactly as code written without any strategy. You can think of it as a “no-op” strategy.</p>
<p>Default strategy is a singleton - and one cannot create more instances of it. It can be obtained using <code class="docutils literal notranslate"><span class="pre">tf.distribute.get_strategy()</span></code> outside any explicit strategy’s scope (the same API that can be used to get the current strategy inside an explicit strategy’s scope).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>default_strategy = tf.distribute.get_strategy()
</pre></div>
</div>
</div>
<p>This strategy serves two main purposes:</p>
<ul class="simple">
<li><p>It allows writing distribution aware library code unconditionally. For example, in <code class="docutils literal notranslate"><span class="pre">tf.optimizer</span></code>s can use <code class="docutils literal notranslate"><span class="pre">tf.distribute.get_strategy()</span></code> and use that strategy for reducing gradients - it will always return a strategy object on which we can call the reduce API.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># In optimizer or other library code
# Get currently active strategy
strategy = tf.distribute.get_strategy()
strategy.reduce(&quot;SUM&quot;, 1., axis=None)  # reduce some values
</pre></div>
</div>
</div>
<ul class="simple">
<li><p>Similar to library code, it can be used to write end users’ programs to work with and without distribution strategy, without requiring conditional logic. A sample code snippet illustrating this:</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>if tf.config.list_physical_devices(&#39;GPU&#39;):
  strategy = tf.distribute.MirroredStrategy()
else:  # use default strategy
  strategy = tf.distribute.get_strategy()

with strategy.scope():
  # do something interesting
  print(tf.Variable(1.))
</pre></div>
</div>
</div>
</div>
<div class="section" id="OneDeviceStrategy">
<h5>OneDeviceStrategy<a class="headerlink" href="#OneDeviceStrategy" title="Enlazar permanentemente con este título">¶</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.OneDeviceStrategy</span></code> is a strategy to place all variables and computation on a single specified device.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>strategy = tf.distribute.OneDeviceStrategy(device=&quot;/gpu:0&quot;)
</pre></div>
</div>
<p>This strategy is distinct from the default strategy in a number of ways. In default strategy, the variable placement logic remains unchanged when compared to running TensorFlow without any distribution strategy. But when using <code class="docutils literal notranslate"><span class="pre">OneDeviceStrategy</span></code>, all variables created in its scope are explicitly placed on the specified device. Moreover, any functions called via <code class="docutils literal notranslate"><span class="pre">OneDeviceStrategy.run</span></code> will also be placed on the specified device.</p>
<p>Input distributed through this strategy will be prefetched to the specified device. In default strategy, there is no input distribution.</p>
<p>Similar to the default strategy, this strategy could also be used to test your code before switching to other strategies which actually distribute to multiple devices/machines. This will exercise the distribution strategy machinery somewhat more than default strategy, but not to the full extent as using <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> or <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code> etc. If you want code that behaves as if no strategy, then use default strategy.</p>
<p>So far you’ve seen the different strategies available and how you can instantiate them. The next few sections show the different ways in which you can use them to distribute your training.</p>
</div>
</div>
</div>
<div class="section" id="Using-tf.distribute.Strategy-with-tf.keras.Model.fit">
<h3>Using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit</span></code><a class="headerlink" href="#Using-tf.distribute.Strategy-with-tf.keras.Model.fit" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is integrated into <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> which is TensorFlow’s implementation of the <a class="reference external" href="https://keras.io">Keras API specification</a>. <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> is a high-level API to build and train models. By integrating into <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> backend, we’ve made it seamless for you to distribute your training written in the Keras training framework using <code class="docutils literal notranslate"><span class="pre">model.fit</span></code>.</p>
<p>Here’s what you need to change in your code:</p>
<ol class="arabic simple">
<li><p>Create an instance of the appropriate <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>.</p></li>
<li><p>Move the creation of Keras model, optimizer and metrics inside <code class="docutils literal notranslate"><span class="pre">strategy.scope</span></code>.</p></li>
</ol>
<p>We support all types of Keras models - sequential, functional and subclassed.</p>
<p>Here is a snippet of code to do this for a very simple Keras model with one dense layer:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mirrored_strategy = tf.distribute.MirroredStrategy()

with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])

model.compile(loss=&#39;mse&#39;, optimizer=&#39;sgd&#39;)
</pre></div>
</div>
</div>
<p>This example usees <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> so you can run this on a machine with multiple GPUs. <code class="docutils literal notranslate"><span class="pre">strategy.scope()</span></code> indicates to Keras which strategy to use to distribute the training. Creating models/optimizers/metrics inside this scope allows us to create distributed variables instead of regular variables. Once this is set up, you can fit your model like you would normally. <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> takes care of replicating the model’s training on the available GPUs, aggregating gradients, and more.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)
model.fit(dataset, epochs=2)
model.evaluate(dataset)
</pre></div>
</div>
</div>
<p>Here a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> provides the training and eval input. You can also use numpy arrays:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import numpy as np
inputs, targets = np.ones((100, 1)), np.ones((100, 1))
model.fit(inputs, targets, epochs=2, batch_size=10)
</pre></div>
</div>
</div>
<p>In both cases (dataset or numpy), each batch of the given input is divided equally among the multiple replicas. For instance, if using <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> with 2 GPUs, each batch of size 10 will get divided among the 2 GPUs, with each receiving 5 input examples in each step. Each epoch will then train faster as you add more GPUs. Typically, you would want to increase your batch size as you add more accelerators so as to make effective use of the extra computing power. You will also need to
re-tune your learning rate, depending on the model. You can use <code class="docutils literal notranslate"><span class="pre">strategy.num_replicas_in_sync</span></code> to get the number of replicas.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Compute global batch size using number of replicas.
BATCH_SIZE_PER_REPLICA = 5
global_batch_size = (BATCH_SIZE_PER_REPLICA *
                     mirrored_strategy.num_replicas_in_sync)
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100)
dataset = dataset.batch(global_batch_size)

LEARNING_RATES_BY_BATCH_SIZE = {5: 0.1, 10: 0.15}
learning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]
</pre></div>
</div>
</div>
<div class="section" id="What’s-supported-now?">
<h4>What’s supported now?<a class="headerlink" href="#What’s-supported-now?" title="Enlazar permanentemente con este título">¶</a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 15%" />
<col style="width: 22%" />
<col style="width: 22%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Training API</p></th>
<th class="head"><p>MirroredStrategy</p></th>
<th class="head"><p>TPUStrategy</p></th>
<th class="head"><p>MultiWorkerMirroredStrategy</p></th>
<th class="head"><p>ParameterServerStrategy</p></th>
<th class="head"><p>CentralStorageStrategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Keras APIs</p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
<td><p>Experimental support</p></td>
<td><p>Experimental support</p></td>
<td><p>Experimental support</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Examples-and-Tutorials">
<h4>Examples and Tutorials<a class="headerlink" href="#Examples-and-Tutorials" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Here is a list of tutorials and examples that illustrate the above integration end to end with Keras:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/keras">Tutorial</a> to train MNIST with <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras">Tutorial</a> to train MNIST using <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/guide/tpu#train_a_model_using_keras_high_level_apis">Guide</a> on training MNIST using <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/parameter_server_training">Tutorial</a> for parameter server training in TensorFlow 2 with <code class="docutils literal notranslate"><span class="pre">ParameterServerStrategy</span></code>.</p></li>
<li><p>TensorFlow Model Garden <a class="reference external" href="https://github.com/tensorflow/models/tree/master/official">repository</a> containing collections of state-of-the-art models implemented using various strategies.</p></li>
</ol>
</div>
</div>
<div class="section" id="Using-tf.distribute.Strategy-with-custom-training-loops">
<h3>Using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with custom training loops<a class="headerlink" href="#Using-tf.distribute.Strategy-with-custom-training-loops" title="Enlazar permanentemente con este título">¶</a></h3>
<p>As you’ve seen, using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with Keras <code class="docutils literal notranslate"><span class="pre">model.fit</span></code> requires changing only a couple lines of your code. With a little more effort, you can also use <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with custom training loops.</p>
<p>If you need more flexibility and control over your training loops than is possible with Estimator or Keras, you can write custom training loops. For instance, when using a GAN, you may want to take a different number of generator or discriminator steps each round. Similarly, the high level frameworks are not very suitable for Reinforcement Learning training.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> classes provide a core set of methods through to support custom training loops. Using these may require minor restructuring of the code initially, but once that is done, you should be able to switch between GPUs, TPUs, and multiple machines simply by changing the strategy instance.</p>
<p>Here we will show a brief snippet illustrating this use case for a simple training example using the same Keras model as before.</p>
<p>First, create the model and optimizer inside the strategy’s scope. This ensures that any variables created with the model and optimizer are mirrored variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  optimizer = tf.keras.optimizers.SGD()
</pre></div>
</div>
</div>
<p>Next, create the input dataset and call <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code> to distribute the dataset based on the strategy.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(
    global_batch_size)
dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)
</pre></div>
</div>
</div>
<p>Then, define one step of the training. Use <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> to compute gradients and optimizer to apply those gradients to update our model’s variables. To distribute this training step, put it in a function <code class="docutils literal notranslate"><span class="pre">train_step</span></code> and pass it to <code class="docutils literal notranslate"><span class="pre">tf.distrbute.Strategy.run</span></code> along with the dataset inputs you got from the <code class="docutils literal notranslate"><span class="pre">dist_dataset</span></code> created before:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>loss_object = tf.keras.losses.BinaryCrossentropy(
  from_logits=True,
  reduction=tf.keras.losses.Reduction.NONE)

def compute_loss(labels, predictions):
  per_example_loss = loss_object(labels, predictions)
  return tf.nn.compute_average_loss(per_example_loss, global_batch_size=global_batch_size)

def train_step(inputs):
  features, labels = inputs

  with tf.GradientTape() as tape:
    predictions = model(features, training=True)
    loss = compute_loss(labels, predictions)

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return loss

@tf.function
def distributed_train_step(dist_inputs):
  per_replica_losses = mirrored_strategy.run(train_step, args=(dist_inputs,))
  return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)
</pre></div>
</div>
</div>
<p>A few other things to note in the code above:</p>
<ol class="arabic simple">
<li><p>It used <code class="docutils literal notranslate"><span class="pre">tf.nn.compute_average_loss</span></code> to compute the loss. <code class="docutils literal notranslate"><span class="pre">tf.nn.compute_average_loss</span></code> sums the per example loss and divide the sum by the global_batch_size. This is important because later after the gradients are calculated on each replica, they are aggregated across the replicas by <strong>summing</strong> them.</p></li>
<li><p>It used the <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.reduce</span></code> API to aggregate the results returned by <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.run</span></code>. <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.run</span></code> returns results from each local replica in the strategy, and there are multiple ways to consume this result. You can <code class="docutils literal notranslate"><span class="pre">reduce</span></code> them to get an aggregated value. You can also do <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_local_results</span></code> to get the list of values contained in the result, one per local replica.</p></li>
<li><p>When <code class="docutils literal notranslate"><span class="pre">apply_gradients</span></code> is called within a distribution strategy scope, its behavior is modified. Specifically, before applying gradients on each parallel instance during synchronous training, it performs a sum-over-all-replicas of the gradients.</p></li>
</ol>
<p>Finally, once you have defined the training step, we can iterate over <code class="docutils literal notranslate"><span class="pre">dist_dataset</span></code> and run the training in a loop:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for dist_inputs in dist_dataset:
  print(distributed_train_step(dist_inputs))
</pre></div>
</div>
</div>
<p>In the example above, you iterated over the <code class="docutils literal notranslate"><span class="pre">dist_dataset</span></code> to provide input to your training. We also provide the <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.make_experimental_numpy_dataset</span></code> to support numpy inputs. You can use this API to create a dataset before calling <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code>.</p>
<p>Another way of iterating over your data is to explicitly use iterators. You may want to do this when you want to run for a given number of steps as opposed to iterating over the entire dataset. The above iteration would now be modified to first create an iterator and then explicitly call <code class="docutils literal notranslate"><span class="pre">next</span></code> on it to get the input data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>iterator = iter(dist_dataset)
for _ in range(10):
  print(distributed_train_step(next(iterator)))
</pre></div>
</div>
</div>
<p>This covers the simplest case of using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> API to distribute custom training loops.</p>
<div class="section" id="id9">
<h4>What’s supported now?<a class="headerlink" href="#id9" title="Enlazar permanentemente con este título">¶</a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 17%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 21%" />
<col style="width: 17%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Training API</p></th>
<th class="head"><p>MirroredStrategy</p></th>
<th class="head"><p>TPUStrategy</p></th>
<th class="head"><p>MultiWorkerMirroredStrategy</p></th>
<th class="head"><p>ParameterServerStrategy</p></th>
<th class="head"><p>CentralStorageStrategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Custom Training Loop</p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
<td><p>Experimental support</p></td>
<td><p>Experimental support</p></td>
<td><p>Experimental support</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id10">
<h4>Examples and Tutorials<a class="headerlink" href="#id10" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Here are some examples for using distribution strategy with custom training loops:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/custom_training">Tutorial</a> to train MNIST using <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/guide/tpu#train_a_model_using_custom_training_loop">Guide</a> on training MNIST using <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>.</p></li>
<li><p>TensorFlow Model Garden <a class="reference external" href="https://github.com/tensorflow/models/tree/master/official">repository</a> containing collections of state-of-the-art models implemented using various strategies.</p></li>
</ol>
</div>
</div>
<div class="section" id="Other-topics">
<h3>Other topics<a class="headerlink" href="#Other-topics" title="Enlazar permanentemente con este título">¶</a></h3>
<p>This section covers some topics that are relevant to multiple use cases.</p>
<blockquote>
<div><p>### Setting up TF_CONFIG environment variable</p>
</div></blockquote>
<p>For multi-worker training, as mentioned before, you need to set <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable for each binary running in your cluster. The <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable is a JSON string which specifies what tasks constitute a cluster, their addresses and each task’s role in the cluster. The <a class="reference external" href="https://github.com/tensorflow/ecosystem">tensorflow/ecosystem</a> repo provides a Kubernetes template in which sets <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> for your training tasks.</p>
<p>There are two components of TF_CONFIG: cluster and task. cluster provides information about the training cluster, which is a dict consisting of different types of jobs such as worker. In multi-worker training, there is usually one worker that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular worker does. Such worker is referred to as the ‘chief’ worker, and it is customary that the worker with index 0 is appointed
as the chief worker (in fact this is how tf.distribute.Strategy is implemented). task on the other hand provides information of the current task. The first component cluster is the same for all workers, and the second component task is different on each worker and specifies the type and index of that worker.</p>
<p>One example of <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>os.environ[&quot;TF_CONFIG&quot;] = json.dumps({
    &quot;cluster&quot;: {
        &quot;worker&quot;: [&quot;host1:port&quot;, &quot;host2:port&quot;, &quot;host3:port&quot;],
        &quot;ps&quot;: [&quot;host4:port&quot;, &quot;host5:port&quot;]
    },
   &quot;task&quot;: {&quot;type&quot;: &quot;worker&quot;, &quot;index&quot;: 1}
})
</pre></div>
</div>
<p>This <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> specifies that there are three workers and two ps tasks in the cluster along with their hosts and ports. The “task” part specifies that the role of the current task in the cluster, worker 1 (the second worker). Valid roles in a cluster is “chief”, “worker”, “ps” and “evaluator”. There should be no “ps” job except when using <code class="docutils literal notranslate"><span class="pre">tf.distribute.experimental.ParameterServerStrategy</span></code>.</p>
</div>
<div class="section" id="What’s-next?">
<h3>What’s next?<a class="headerlink" href="#What’s-next?" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is actively under development. Try it out and provide and your feedback using <a class="reference external" href="https://github.com/tensorflow/tensorflow/issues/new">GitHub issues</a>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>