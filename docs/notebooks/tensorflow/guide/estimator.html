

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2019 The TensorFlow Authors. &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2019 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/estimator.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2019-The-TensorFlow-Authors.">
<h1>Copyright 2019 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2019-The-TensorFlow-Authors." title="Enlazar permanentemente con este título">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Estimators">
<h2>Estimators<a class="headerlink" href="#Estimators" title="Enlazar permanentemente con este título">¶</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>c7cc6b756a6e44db9770637c0ce22380|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>a04232c9d578473795d8f4cedc0f604c|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>c2729c2cdba04ea4bf54d9d745636389|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>341210c0ac7247cba7e33616ea2b341a|Download notebook</p>
</td></table><p>This document introduces <code class="docutils literal notranslate"><span class="pre">tf.estimator</span></code>—a high-level TensorFlow API. Estimators encapsulate the following actions:</p>
<ul class="simple">
<li><p>Training</p></li>
<li><p>Evaluation</p></li>
<li><p>Prediction</p></li>
<li><p>Export for serving</p></li>
</ul>
<p>TensorFlow implements several pre-made Estimators. Custom estimators are still suported, but mainly as a backwards compatibility measure. <strong>Custom estimators should not be used for new code</strong>. All Estimators—pre-made or custom ones—are classes based on the <code class="docutils literal notranslate"><span class="pre">tf.estimator.Estimator</span></code> class.</p>
<p>For a quick example, try <a class="reference external" href="../tutorials/estimator/linear.ipynb">Estimator tutorials</a>. For an overview of the API design, check the <a class="reference external" href="https://arxiv.org/abs/1708.02637">white paper</a>.</p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!pip install -U tensorflow_datasets
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tempfile
import os

import tensorflow as tf
import tensorflow_datasets as tfds
</pre></div>
</div>
</div>
</div>
<div class="section" id="Advantages">
<h3>Advantages<a class="headerlink" href="#Advantages" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Similar to a <code class="docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>, an <code class="docutils literal notranslate"><span class="pre">estimator</span></code> is a model-level abstraction. The <code class="docutils literal notranslate"><span class="pre">tf.estimator</span></code> provides some capabilities currently still under development for <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code>. These are:</p>
<ul class="simple">
<li><p>Parameter server based training</p></li>
<li><p>Full <a class="reference external" href="http://tensorflow.org/tfx">TFX</a> integration</p></li>
</ul>
</div>
<div class="section" id="Estimators-Capabilities">
<h3>Estimators Capabilities<a class="headerlink" href="#Estimators-Capabilities" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Estimators provide the following benefits:</p>
<ul class="simple">
<li><p>You can run Estimator-based models on a local host or on a distributed multi-server environment without changing your model. Furthermore, you can run Estimator-based models on CPUs, GPUs, or TPUs without recoding your model.</p></li>
<li><p>Estimators provide a safe distributed training loop that controls how and when to:</p>
<ul>
<li><p>Load data</p></li>
<li><p>Handle exceptions</p></li>
<li><p>Create checkpoint files and recover from failures</p></li>
<li><p>Save summaries for TensorBoard</p></li>
</ul>
</li>
</ul>
<p>When writing an application with Estimators, you must separate the data input pipeline from the model. This separation simplifies experiments with different datasets.</p>
</div>
<div class="section" id="Using-pre-made-Estimators">
<h3>Using pre-made Estimators<a class="headerlink" href="#Using-pre-made-Estimators" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Pre-made Estimators enable you to work at a much higher conceptual level than the base TensorFlow APIs. You no longer have to worry about creating the computational graph or sessions since Estimators handle all the “plumbing” for you. Furthermore, pre-made Estimators let you experiment with different model architectures by making only minimal code changes. <code class="docutils literal notranslate"><span class="pre">tf.estimator.DNNClassifier</span></code>, for example, is a pre-made Estimator class that trains classification models based on dense, feed-forward
neural networks.</p>
<p>A TensorFlow program relying on a pre-made Estimator typically consists of the following four steps:</p>
<div class="section" id="1.-Write-an-input-functions">
<h4>1. Write an input functions<a class="headerlink" href="#1.-Write-an-input-functions" title="Enlazar permanentemente con este título">¶</a></h4>
<p>For example, you might create one function to import the training set and another function to import the test set. Estimators expect their inputs to be formatted as a pair of objects:</p>
<ul class="simple">
<li><p>A dictionary in which the keys are feature names and the values are Tensors (or SparseTensors) containing the corresponding feature data</p></li>
<li><p>A Tensor containing one or more labels</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> should return a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> that yields pairs in that format.</p>
<p>For example, the following code builds a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> from the Titanic dataset’s <code class="docutils literal notranslate"><span class="pre">train.csv</span></code> file:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def train_input_fn():
  titanic_file = tf.keras.utils.get_file(&quot;train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)
  titanic = tf.data.experimental.make_csv_dataset(
      titanic_file, batch_size=32,
      label_name=&quot;survived&quot;)
  titanic_batches = (
      titanic.cache().repeat().shuffle(500)
      .prefetch(tf.data.AUTOTUNE))
  return titanic_batches
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> is executed in a <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> and can also directly return a <code class="docutils literal notranslate"><span class="pre">(features_dics,</span> <span class="pre">labels)</span></code> pair containing graph tensors, but this is error prone outside of simple cases like returning constants.</p>
</div>
<div class="section" id="2.-Define-the-feature-columns.">
<h4>2. Define the feature columns.<a class="headerlink" href="#2.-Define-the-feature-columns." title="Enlazar permanentemente con este título">¶</a></h4>
<p>Each <code class="docutils literal notranslate"><span class="pre">tf.feature_column</span></code> identifies a feature name, its type, and any input pre-processing.</p>
<p>For example, the following snippet creates three feature columns.</p>
<ul class="simple">
<li><p>The first uses the <code class="docutils literal notranslate"><span class="pre">age</span></code> feature directly as a floating-point input.</p></li>
<li><p>The second uses the <code class="docutils literal notranslate"><span class="pre">class</span></code> feature as a categorical input.</p></li>
<li><p>The third uses the <code class="docutils literal notranslate"><span class="pre">embark_town</span></code> as a categorical input, but uses the <code class="docutils literal notranslate"><span class="pre">hashing</span> <span class="pre">trick</span></code> to avoid the need to enumerate the options, and to set the number of options.</p></li>
</ul>
<p>For further information, check the <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/feature_columns">feature columns tutorial</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>age = tf.feature_column.numeric_column(&#39;age&#39;)
cls = tf.feature_column.categorical_column_with_vocabulary_list(&#39;class&#39;, [&#39;First&#39;, &#39;Second&#39;, &#39;Third&#39;])
embark = tf.feature_column.categorical_column_with_hash_bucket(&#39;embark_town&#39;, 32)
</pre></div>
</div>
</div>
</div>
<div class="section" id="3.-Instantiate-the-relevant-pre-made-Estimator.">
<h4>3. Instantiate the relevant pre-made Estimator.<a class="headerlink" href="#3.-Instantiate-the-relevant-pre-made-Estimator." title="Enlazar permanentemente con este título">¶</a></h4>
<p>For example, here’s a sample instantiation of a pre-made Estimator named <code class="docutils literal notranslate"><span class="pre">LinearClassifier</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model_dir = tempfile.mkdtemp()
model = tf.estimator.LinearClassifier(
    model_dir=model_dir,
    feature_columns=[embark, cls, age],
    n_classes=2
)
</pre></div>
</div>
</div>
<p>For more information, you can go the <a class="reference external" href="https://www.tensorflow.org/tutorials/estimator/linear">linear classifier tutorial</a>.</p>
</div>
<div class="section" id="4.-Call-a-training,-evaluation,-or-inference-method.">
<h4>4. Call a training, evaluation, or inference method.<a class="headerlink" href="#4.-Call-a-training,-evaluation,-or-inference-method." title="Enlazar permanentemente con este título">¶</a></h4>
<p>All Estimators provide <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">evaluate</span></code>, and <code class="docutils literal notranslate"><span class="pre">predict</span></code> methods.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model = model.train(input_fn=train_input_fn, steps=100)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>result = model.evaluate(train_input_fn, steps=10)

for key, value in result.items():
  print(key, &quot;:&quot;, value)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for pred in model.predict(train_input_fn):
  for key, value in pred.items():
    print(key, &quot;:&quot;, value)
  break
</pre></div>
</div>
</div>
</div>
<div class="section" id="Benefits-of-pre-made-Estimators">
<h4>Benefits of pre-made Estimators<a class="headerlink" href="#Benefits-of-pre-made-Estimators" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Pre-made Estimators encode best practices, providing the following benefits:</p>
<ul class="simple">
<li><p>Best practices for determining where different parts of the computational graph should run, implementing strategies on a single machine or on a cluster.</p></li>
<li><p>Best practices for event (summary) writing and universally useful summaries.</p></li>
</ul>
<p>If you don’t use pre-made Estimators, you must implement the preceding features yourself.</p>
</div>
</div>
<div class="section" id="Custom-Estimators">
<h3>Custom Estimators<a class="headerlink" href="#Custom-Estimators" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The heart of every Estimator—whether pre-made or custom—is its <em>model function</em>, <code class="docutils literal notranslate"><span class="pre">model_fn</span></code>, which is a method that builds graphs for training, evaluation, and prediction. When you are using a pre-made Estimator, someone else has already implemented the model function. When relying on a custom Estimator, you must write the model function yourself.</p>
<blockquote>
<div><p>Note: A custom <code class="docutils literal notranslate"><span class="pre">model_fn</span></code> will still run in 1.x-style graph mode. This means there is no eager execution and no automatic control dependencies. You should plan to migrate away from <code class="docutils literal notranslate"><span class="pre">tf.estimator</span></code> with custom <code class="docutils literal notranslate"><span class="pre">model_fn</span></code>. The alternative APIs are <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code>. If you still need an <code class="docutils literal notranslate"><span class="pre">Estimator</span></code> for some part of your training you can use the <code class="docutils literal notranslate"><span class="pre">tf.keras.estimator.model_to_estimator</span></code> converter to create an <code class="docutils literal notranslate"><span class="pre">Estimator</span></code> from a <code class="docutils literal notranslate"><span class="pre">keras.Model</span></code>.</p>
</div></blockquote>
</div>
<div class="section" id="Create-an-Estimator-from-a-Keras-model">
<h3>Create an Estimator from a Keras model<a class="headerlink" href="#Create-an-Estimator-from-a-Keras-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You can convert existing Keras models to Estimators with <code class="docutils literal notranslate"><span class="pre">tf.keras.estimator.model_to_estimator</span></code>. This is helpful if you want to modernize your model code, but your training pipeline still requires Estimators.</p>
<p>Instantiate a Keras MobileNet V2 model and compile the model with the optimizer, loss, and metrics to train with:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>keras_mobilenet_v2 = tf.keras.applications.MobileNetV2(
    input_shape=(160, 160, 3), include_top=False)
keras_mobilenet_v2.trainable = False

estimator_model = tf.keras.Sequential([
    keras_mobilenet_v2,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(1)
])

# Compile the model
estimator_model.compile(
    optimizer=&#39;adam&#39;,
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[&#39;accuracy&#39;])
</pre></div>
</div>
</div>
<p>Create an <code class="docutils literal notranslate"><span class="pre">Estimator</span></code> from the compiled Keras model. The initial model state of the Keras model is preserved in the created <code class="docutils literal notranslate"><span class="pre">Estimator</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>est_mobilenet_v2 = tf.keras.estimator.model_to_estimator(keras_model=estimator_model)
</pre></div>
</div>
</div>
<p>Treat the derived <code class="docutils literal notranslate"><span class="pre">Estimator</span></code> as you would with any other <code class="docutils literal notranslate"><span class="pre">Estimator</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>IMG_SIZE = 160  # All images will be resized to 160x160

def preprocess(image, label):
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def train_input_fn(batch_size):
  data = tfds.load(&#39;cats_vs_dogs&#39;, as_supervised=True)
  train_data = data[&#39;train&#39;]
  train_data = train_data.map(preprocess).shuffle(500).batch(batch_size)
  return train_data
</pre></div>
</div>
</div>
<p>To train, call Estimator’s train function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>est_mobilenet_v2.train(input_fn=lambda: train_input_fn(32), steps=50)
</pre></div>
</div>
</div>
<p>Similarly, to evaluate, call the Estimator’s evaluate function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>est_mobilenet_v2.evaluate(input_fn=lambda: train_input_fn(32), steps=10)
</pre></div>
</div>
</div>
<p>For more details, please refer to the documentation for <code class="docutils literal notranslate"><span class="pre">tf.keras.estimator.model_to_estimator</span></code>.</p>
</div>
<div class="section" id="Saving-object-based-checkpoints-with-Estimator">
<h3>Saving object-based checkpoints with Estimator<a class="headerlink" href="#Saving-object-based-checkpoints-with-Estimator" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Estimators by default save checkpoints with variable names rather than the object graph described in the <a class="reference internal" href="checkpoint.html"><span class="doc">Checkpoint guide</span></a>. <code class="docutils literal notranslate"><span class="pre">tf.train.Checkpoint</span></code> will read name-based checkpoints, but variable names may change when moving parts of a model outside of the Estimator’s <code class="docutils literal notranslate"><span class="pre">model_fn</span></code>. For forwards compatibility saving object-based checkpoints makes it easier to train a model inside an Estimator and then use it outside of one.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow.compat.v1 as tf_compat
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def toy_dataset():
  inputs = tf.range(10.)[:, None]
  labels = inputs * 5. + tf.range(5.)[None, :]
  return tf.data.Dataset.from_tensor_slices(
    dict(x=inputs, y=labels)).repeat().batch(2)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class Net(tf.keras.Model):
  &quot;&quot;&quot;A simple linear model.&quot;&quot;&quot;

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  def call(self, x):
    return self.l1(x)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def model_fn(features, labels, mode):
  net = Net()
  opt = tf.keras.optimizers.Adam(0.1)
  ckpt = tf.train.Checkpoint(step=tf_compat.train.get_global_step(),
                             optimizer=opt, net=net)
  with tf.GradientTape() as tape:
    output = net(features[&#39;x&#39;])
    loss = tf.reduce_mean(tf.abs(output - features[&#39;y&#39;]))
  variables = net.trainable_variables
  gradients = tape.gradient(loss, variables)
  return tf.estimator.EstimatorSpec(
    mode,
    loss=loss,
    train_op=tf.group(opt.apply_gradients(zip(gradients, variables)),
                      ckpt.step.assign_add(1)),
    # Tell the Estimator to save &quot;ckpt&quot; in an object-based format.
    scaffold=tf_compat.train.Scaffold(saver=ckpt))

tf.keras.backend.clear_session()
est = tf.estimator.Estimator(model_fn, &#39;./tf_estimator_example/&#39;)
est.train(toy_dataset, steps=10)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.train.Checkpoint</span></code> can then load the Estimator’s checkpoints from its <code class="docutils literal notranslate"><span class="pre">model_dir</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>opt = tf.keras.optimizers.Adam(0.1)
net = Net()
ckpt = tf.train.Checkpoint(
  step=tf.Variable(1, dtype=tf.int64), optimizer=opt, net=net)
ckpt.restore(tf.train.latest_checkpoint(&#39;./tf_estimator_example/&#39;))
ckpt.step.numpy()  # From est.train(..., steps=10)
</pre></div>
</div>
</div>
</div>
<div class="section" id="SavedModels-from-Estimators">
<h3>SavedModels from Estimators<a class="headerlink" href="#SavedModels-from-Estimators" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Estimators export SavedModels through <code class="docutils literal notranslate"><span class="pre">tf.Estimator.export_saved_model</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>input_column = tf.feature_column.numeric_column(&quot;x&quot;)

estimator = tf.estimator.LinearClassifier(feature_columns=[input_column])

def input_fn():
  return tf.data.Dataset.from_tensor_slices(
    ({&quot;x&quot;: [1., 2., 3., 4.]}, [1, 1, 0, 0])).repeat(200).shuffle(64).batch(16)
estimator.train(input_fn)
</pre></div>
</div>
</div>
<p>To save an <code class="docutils literal notranslate"><span class="pre">Estimator</span></code> you need to create a <code class="docutils literal notranslate"><span class="pre">serving_input_receiver</span></code>. This function builds a part of a <code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> that parses the raw data received by the SavedModel.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.estimator.export</span></code> module contains functions to help build these <code class="docutils literal notranslate"><span class="pre">receivers</span></code>.</p>
<p>The following code builds a receiver, based on the <code class="docutils literal notranslate"><span class="pre">feature_columns</span></code>, that accepts serialized <code class="docutils literal notranslate"><span class="pre">tf.Example</span></code> protocol buffers, which are often used with <a class="reference external" href="https://tensorflow.org/serving">tf-serving</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tmpdir = tempfile.mkdtemp()

serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
  tf.feature_column.make_parse_example_spec([input_column]))

estimator_base_path = os.path.join(tmpdir, &#39;from_estimator&#39;)
estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)
</pre></div>
</div>
</div>
<p>You can also load and run that model, from python:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>imported = tf.saved_model.load(estimator_path)

def predict(x):
  example = tf.train.Example()
  example.features.feature[&quot;x&quot;].float_list.value.extend([x])
  return imported.signatures[&quot;predict&quot;](
    examples=tf.constant([example.SerializeToString()]))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(predict(1.5))
print(predict(3.5))
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.estimator.export.build_raw_serving_input_receiver_fn</span></code> allows you to create input functions which take raw tensors rather than <code class="docutils literal notranslate"><span class="pre">tf.train.Example</span></code>s.</p>
</div>
<div class="section" id="Using-tf.distribute.Strategy-with-Estimator-(Limited-support)">
<h3>Using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with Estimator (Limited support)<a class="headerlink" href="#Using-tf.distribute.Strategy-with-Estimator-(Limited-support)" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.estimator</span></code> is a distributed training TensorFlow API that originally supported the async parameter server approach. <code class="docutils literal notranslate"><span class="pre">tf.estimator</span></code> now supports <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>. If you’re using <code class="docutils literal notranslate"><span class="pre">tf.estimator</span></code>, you can change to distributed training with very few changes to your code. With this, Estimator users can now do synchronous distributed training on multiple GPUs and multiple workers, as well as use TPUs. This support in Estimator is, however, limited. Check out the <a class="reference external" href="#estimator_support">What’s supported
now</a> section below for more details.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> with Estimator is slightly different than in the Keras case. Instead of using <code class="docutils literal notranslate"><span class="pre">strategy.scope</span></code>, now you pass the strategy object into the <code class="docutils literal notranslate"><span class="pre">RunConfig</span></code> for the Estimator.</p>
<p>You can refer to the <a class="reference internal" href="distributed_training.html"><span class="doc">distributed training guide</span></a> for more information.</p>
<p>Here is a snippet of code that shows this with a premade Estimator <code class="docutils literal notranslate"><span class="pre">LinearRegressor</span></code> and <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mirrored_strategy = tf.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(
    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)
regressor = tf.estimator.LinearRegressor(
    feature_columns=[tf.feature_column.numeric_column(&#39;feats&#39;)],
    optimizer=&#39;SGD&#39;,
    config=config)
</pre></div>
</div>
</div>
<p>Here, you use a premade Estimator, but the same code works with a custom Estimator as well. <code class="docutils literal notranslate"><span class="pre">train_distribute</span></code> determines how training will be distributed, and <code class="docutils literal notranslate"><span class="pre">eval_distribute</span></code> determines how evaluation will be distributed. This is another difference from Keras where you use the same strategy for both training and eval.</p>
<p>Now you can train and evaluate this Estimator with an input function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def input_fn():
  dataset = tf.data.Dataset.from_tensors(({&quot;feats&quot;:[1.]}, [1.]))
  return dataset.repeat(1000).batch(10)
regressor.train(input_fn=input_fn, steps=10)
regressor.evaluate(input_fn=input_fn, steps=10)
</pre></div>
</div>
</div>
<p>Another difference to highlight here between Estimator and Keras is the input handling. In Keras, each batch of the dataset is split automatically across the multiple replicas. In Estimator, however, you do not perform automatic batch splitting, nor automatically shard the data across different workers. You have full control over how you want your data to be distributed across workers and devices, and you must provide an <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> to specify how to distribute your data.</p>
<p>Your <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> is called once per worker, thus giving one dataset per worker. Then one batch from that dataset is fed to one replica on that worker, thereby consuming N batches for N replicas on 1 worker. In other words, the dataset returned by the <code class="docutils literal notranslate"><span class="pre">input_fn</span></code> should provide batches of size <code class="docutils literal notranslate"><span class="pre">PER_REPLICA_BATCH_SIZE</span></code>. And the global batch size for a step can be obtained as <code class="docutils literal notranslate"><span class="pre">PER_REPLICA_BATCH_SIZE</span> <span class="pre">*</span> <span class="pre">strategy.num_replicas_in_sync</span></code>.</p>
<p>When performing multi-worker training, you should either split your data across the workers, or shuffle with a random seed on each. You can check an example of how to do this in the <a class="reference external" href="../tutorials/distribute/multi_worker_with_estimator.ipynb">Multi-worker training with Estimator</a> tutorial.</p>
<p>And similarly, you can use multi worker and parameter server strategies as well. The code remains the same, but you need to use <code class="docutils literal notranslate"><span class="pre">tf.estimator.train_and_evaluate</span></code>, and set <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variables for each binary running in your cluster.</p>
<div class="section" id="What’s-supported-now?">
<h4>What’s supported now?<a class="headerlink" href="#What’s-supported-now?" title="Enlazar permanentemente con este título">¶</a></h4>
<p>There is limited support for training with Estimator using all strategies except <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>. Basic training and evaluation should work, but a number of advanced features such as <code class="docutils literal notranslate"><span class="pre">v1.train.Scaffold</span></code> do not. There may also be a number of bugs in this integration and there are no plans to actively improve this support (the focus is on Keras and custom training loop support). If at all possible, you should prefer to use <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> with those APIs instead.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 12%" />
<col style="width: 15%" />
<col style="width: 11%" />
<col style="width: 23%" />
<col style="width: 19%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Training API</p></th>
<th class="head"><p>MirroredStrategy</p></th>
<th class="head"><p>TPUStrategy</p></th>
<th class="head"><p>MultiWorkerMirroredStrategy</p></th>
<th class="head"><p>CentralStorageStrategy</p></th>
<th class="head"><p>ParameterServerStrategy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Estimator API</p></td>
<td><p>Limited support</p></td>
<td><p>Not supported</p></td>
<td><p>Limited support</p></td>
<td><p>Limited support</p></td>
<td><p>Limited support</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Examples-and-tutorials">
<h4>Examples and tutorials<a class="headerlink" href="#Examples-and-tutorials" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Here are some end-to-end examples that show how to use various strategies with Estimator:</p>
<ol class="arabic simple">
<li><p>The <a class="reference external" href="../tutorials/distribute/multi_worker_with_estimator.ipynb">Multi-worker Training with Estimator tutorial</a> shows how you can train with multiple workers using <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> on the MNIST dataset.</p></li>
<li><p>An end-to-end example of <a class="reference external" href="https://github.com/tensorflow/ecosystem/tree/master/distribution_strategy">running multi-worker training with distribution strategies</a> in <code class="docutils literal notranslate"><span class="pre">tensorflow/ecosystem</span></code> using Kubernetes templates. It starts with a Keras model and converts it to an Estimator using the <code class="docutils literal notranslate"><span class="pre">tf.keras.estimator.model_to_estimator</span></code> API.</p></li>
<li><p>The official <a class="reference external" href="https://github.com/tensorflow/models/blob/master/official/vision/image_classification/resnet_imagenet_main.py">ResNet50</a> model, which can be trained using either <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> or <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>.</p></li>
</ol>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>