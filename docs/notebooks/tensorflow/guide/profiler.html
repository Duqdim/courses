

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>&lt;no title&gt; &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/profiler.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p># Optimize TensorFlow performance using the Profiler</p>
<p>[TOC]</p>
<p>Use the tools available with the Profiler to track the performance of your
TensorFlow models. See how your model performs on the host (CPU), the device
(GPU), or on a combination of both the host and device(s).</p>
<p>Profiling helps you understand the hardware resource consumption (time and
memory) of the various TensorFlow operations (ops) in your model and resolve
performance bottlenecks and ultimately, make the model execute faster.</p>
<p>This guide will walk you through how to install the Profiler, the various tools
available, the different modes of how the Profiler collects performance data,
and some recommended best practices to optimize model performance.</p>
<p>If you want to profile your model performance on Cloud TPUs, refer to the
[Cloud TPU guide](<a class="reference external" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_profile">https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_profile</a>).</p>
<p>## Install the Profiler and GPU prerequisites</p>
<p>Install the Profiler by downloading and running the
[<cite>install_and_run.py</cite>](<a class="reference external" href="https://raw.githubusercontent.com/tensorflow/profiler/master/install_and_run.py">https://raw.githubusercontent.com/tensorflow/profiler/master/install_and_run.py</a>)
script from the [GitHub repository](<a class="reference external" href="https://github.com/tensorflow/profiler">https://github.com/tensorflow/profiler</a>).</p>
<p>To profile on the GPU, you must:</p>
<ol class="arabic">
<li><p>Meet the NVIDIA® GPU drivers and CUDA® Toolkit requirements listed on
[TensorFlow GPU support software requirements](<a class="reference external" href="https://www.tensorflow.org/install/gpu#linux_setup">https://www.tensorflow.org/install/gpu#linux_setup</a>).</p></li>
<li><p>Ensure CUPTI exists on the path:</p>
<p><code class="docutils literal notranslate"><span class="pre">`shell</span>
<span class="pre">/sbin/ldconfig</span> <span class="pre">-N</span> <span class="pre">-v</span> <span class="pre">$(sed</span> <span class="pre">'s/:/</span> <span class="pre">/g'</span> <span class="pre">&lt;&lt;&lt;</span> <span class="pre">$LD_LIBRARY_PATH)</span> <span class="pre">|</span> <span class="pre">\</span>
<span class="pre">grep</span> <span class="pre">libcupti</span>
<span class="pre">`</span></code></p>
</li>
</ol>
<p>If you don’t have CUPTI on the path, prepend its installation directory to the
<cite>$LD_LIBRARY_PATH</cite> environment variable by running:</p>
<p><code class="docutils literal notranslate"><span class="pre">`shell</span>
<span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH</span>
<span class="pre">`</span></code></p>
<p>Run the <cite>ldconfig</cite> command above again to verify that the CUPTI library is
found.</p>
<p>### Resolve privilege issues</p>
<p>When you run profiling with CUDA® Toolkit in a Docker environment or on Linux,
you may encounter issues related to insufficient CUPTI privileges
(<cite>CUPTI_ERROR_INSUFFICIENT_PRIVILEGES</cite>). See the
[NVIDIA Developer Docs](<a class="reference external" href="https://developer.nvidia.com/nvidia-development-tools-solutions-ERR_NVGPUCTRPERM-permission-issue-performance-counters">https://developer.nvidia.com/nvidia-development-tools-solutions-ERR_NVGPUCTRPERM-permission-issue-performance-counters</a>){:.external}
to learn more about how you can resolve these issues on Linux.</p>
<p>To resolve CUPTI privilege issues in a Docker environment, run</p>
<p><code class="docutils literal notranslate"><span class="pre">`shell</span>
<span class="pre">docker</span> <span class="pre">run</span> <span class="pre">option</span> <span class="pre">'--privileged=true'</span>
<span class="pre">`</span></code></p>
<p>&lt;a name=»profiler_tools»&gt;&lt;/a&gt;</p>
<p>## Profiler tools</p>
<p>Access the Profiler from the <strong>Profile</strong> tab in TensorBoard which appears only
after you have captured some model data.</p>
<p>Note: The Profiler requires internet access to load the
[Google Chart libraries](<a class="reference external" href="https://developers.google.com/chart/interactive/docs/basic_load_libs#basic-library-loading">https://developers.google.com/chart/interactive/docs/basic_load_libs#basic-library-loading</a>).
Some charts and tables may be missing if you run TensorBoard entirely offline on
your local machine, behind a corporate firewall, or in a data center.</p>
<p>The Profiler has a selection of tools to help with performance analysis:</p>
<ul class="simple">
<li><p>Overview page</p></li>
<li><p>Input pipeline analyzer</p></li>
<li><p>TensorFlow stats</p></li>
<li><p>Trace viewer</p></li>
<li><p>GPU kernel stats</p></li>
<li><p>Memory profile tool</p></li>
<li><p>Pod viewer</p></li>
</ul>
<p>&lt;a name=»overview_page»&gt;&lt;/a&gt;</p>
<p>### Overview page</p>
<p>The overview page provides a top level view of how your model performed during a
profile run. The page shows you an aggregated overview page for your host and
all devices, and some recommendations to improve your model training
performance. You can also select individual hosts in the Host dropdown.</p>
<p>The overview page displays data as follows:</p>
<p>![image](./images/tf_profiler/overview_page.png)</p>
<ul>
<li><p><strong>Performance summary -</strong> Displays a high-level summary of your model
performance. The performance summary has two parts:</p>
<ol class="arabic simple">
<li><p>Step-time breakdown - Breaks down the average step time into multiple
categories of where time is spent:</p>
<ul class="simple">
<li><p>Compilation - Time spent compiling kernels</p></li>
<li><p>Input - Time spent reading input data</p></li>
<li><p>Output - Time spent reading output data</p></li>
<li><p>Kernel launch - Time spent by the host to launch kernels</p></li>
<li><p>Host compute time</p></li>
<li><p>Device-to-device communication time</p></li>
<li><p>On-device compute time</p></li>
<li><p>All others, including Python overhead</p></li>
</ul>
</li>
<li><p>Device compute precisions - Reports the percentage of device compute
time that uses 16 and 32-bit computations</p></li>
</ol>
</li>
<li><p><strong>Step-time graph -</strong> Displays a graph of device step time (in milliseconds)
over all the steps sampled. Each step is broken into the multiple categories
(with different colors) of where time is spent. The red area corresponds to
the portion of the step time the devices were sitting idle waiting for input
data from the host. The green area shows how much of time the device was
actually working</p></li>
<li><p><strong>Top 10 TensorFlow operations on device -</strong> Displays the on-device ops that
ran the longest.</p>
<p>Each row displays an op’s self time (as the percentage of time taken by all
ops), cumulative time, category, and name.</p>
</li>
<li><p><strong>Run environment -</strong> Displays a high-level summary of the model run
environment including:</p>
<ul class="simple">
<li><p>Number of hosts used</p></li>
<li><p>Device type (GPU/TPU)</p></li>
<li><p>Number of device cores</p></li>
</ul>
</li>
<li><p><strong>Recommendation for next steps -</strong> Reports when a model is input bound and
recommends tools you can use to locate and resolve model performance
bottlenecks</p></li>
</ul>
<p>&lt;a name=»input_pipeline_analyzer»&gt;&lt;/a&gt;</p>
<p>### Input pipeline analyzer</p>
<p>When a TensorFlow program reads data from a file it begins at the top of the
TensorFlow graph in a pipelined manner. The read process is divided into
multiple data processing stages connected in series, where the output of one
stage is the input to the next one. This system of reading data is called the
<em>input pipeline</em>.</p>
<p>A typical pipeline for reading records from files has the following stages:</p>
<p>1.  File reading
1.  File preprocessing (optional)
1.  File transfer from the host to the device</p>
<p>An inefficient input pipeline can severely slow down your application. An
application is considered <strong>input bound</strong> when it spends a significant portion
of time in input pipeline. Use the insights obtained from the input pipeline
analyzer to understand where the input pipeline is inefficient.</p>
<p>The input pipeline analyzer tells you immediately whether your program is input
bound and walks you through device- and host-side analysis to debug performance
bottlenecks at any stage in the input pipeline.</p>
<p>See the guidance on input pipeline performance for recommended best practices to
optimize your data input pipelines.</p>
<p>#### Input pipeline dashboard</p>
<p>To open the input pipeline analyzer, select <strong>Profile</strong>, then select
<strong>input_pipeline_analyzer</strong> from the <strong>Tools</strong> dropdown.</p>
<p>![image](./images/tf_profiler/input_pipeline_analyzer.png)</p>
<p>The dashboard contains three sections:</p>
<ol class="arabic simple">
<li><p><strong>Summary -</strong> Summarizes the overall input pipeline with information on
whether your application is input bound and, if so, by how much</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Device-side analysis -</strong> Displays detailed, device-side analysis results,
including the device step-time and the range of device time spent waiting
for input data across cores at each step</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Host-side analysis -</strong> Shows a detailed analysis on the host side,
including a breakdown of input processing time on the host</p></li>
</ol>
<p>#### Input pipeline summary</p>
<p>The Summary reports if your program is input bound by presenting the percentage
of device time spent on waiting for input from the host. If you are using a
standard input pipeline that has been instrumented, the tool reports where most
of the input processing time is spent.</p>
<p>#### Device-side analysis</p>
<p>The device-side analysis provides insights on time spent on the device versus on
the host and how much device time was spent waiting for input data from the
host.</p>
<ol class="arabic simple">
<li><p><strong>Step time plotted against step number -</strong> Displays a graph of device step
time (in milliseconds) over all the steps sampled. Each step is broken into
the multiple categories (with different colors) of where time is spent. The
red area corresponds to the portion of the step time the devices were
sitting idle waiting for input data from the host. The green area shows how
much of time the device was actually working</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Step time statistics -</strong> Reports the average, standard deviation, and
range ([minimum, maximum]) of the device step time</p></li>
</ol>
<p>#### Host-side analysis</p>
<p>The host-side analysis reports a breakdown of the input processing time (the
time spent on <cite>tf.data</cite> API ops) on the host into several categories:</p>
<ul class="simple">
<li><p><strong>Reading data from files on demand -</strong> Time spent on reading data from
files without caching, prefetching, and interleaving</p></li>
<li><p><strong>Reading data from files in advance -</strong> Time spent reading files, including
caching, prefetching, and interleaving</p></li>
<li><p><strong>Data preprocessing -</strong> Time spent on preprocessing ops, such as image
decompression</p></li>
<li><p><strong>Enqueuing data to be transferred to device -</strong> Time spent putting data
into an infeed queue before transferring the data to the device</p></li>
</ul>
<p>Expand the <strong>Input Op Statistics</strong> to see the statistics for individual input
ops and their categories broken down by execution time.</p>
<p>![image](./images/tf_profiler/input_op_stats.png)</p>
<p>A source data table appears with each entry containing the following
information:</p>
<p>1.  <strong>Input Op -</strong> Shows the TensorFlow op name of the input op
1.  <strong>Count -</strong> Shows the total number of instances of op execution during the</p>
<blockquote>
<div><p>profiling period</p>
</div></blockquote>
<ol class="arabic simple">
<li><p><strong>Total Time (in ms) -</strong> Shows the cumulative sum of time spent on each of
those instances</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Total Time % -</strong> Shows the total time spent on an op as a fraction of the
total time spent in input processing</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Total Self Time (in ms) -</strong> Shows the cumulative sum of the self time
spent on each of those instances. The self time here measures the time spent
inside the function body, excluding the time spent in the function it calls.</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Total Self Time %</strong>. Shows the total self time as a fraction of the total
time spent on input processing</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Category</strong>. Shows the processing category of the input op</p></li>
</ol>
<p>&lt;a name=»tf_stats»&gt;&lt;/a&gt;</p>
<p>### TensorFlow stats</p>
<p>The TensorFlow Stats tool displays the performance of every TensorFlow op (op)
that is executed on the host or device during a profiling session.</p>
<p>![image](./images/tf_profiler/tf_stats.png)</p>
<p>The tool displays performance information in two panes:</p>
<ul>
<li><p>The upper pane displays upto four pie charts:</p>
<p>1.  The distribution of self-execution time of each op on the host
1.  The distribution of self-execution time of each op type on the host
1.  The distribution of self-execution time of each op on the device
1.  The distribution of self-execution time of each op type on the device</p>
</li>
<li><p>The lower pane shows a table that reports data about TensorFlow ops with one
row for each op and one column for each type of data (sort columns by
clicking the heading of the column). Click the Export as CSV button on the
right side of the upper pane to export the data from this table as a CSV
file.</p>
<p>Note that:</p>
<ul class="simple">
<li><p>If any ops have child ops:</p>
<ul>
<li><p>The total «accumulated» time of an op includes the time spent inside
the child ops</p></li>
<li><p>The total «self» time of an op does not include the time spent
inside the child ops</p></li>
</ul>
</li>
<li><p>If an op executes on the host:</p>
<ul>
<li><p>The percentage of the total self-time on device incurred by the op
on will be 0</p></li>
<li><p>The cumulative percentage of the total self-time on device upto and
including this op will be 0</p></li>
</ul>
</li>
<li><p>If an op executes on the device:</p>
<ul>
<li><p>The percentage of the total self-time on host incurred by this op
will be 0</p></li>
<li><p>The cumulative percentage of the total self-time on host upto and
including this op will be 0</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>You can choose to include or exclude Idle time in the pie charts and table.</p>
<p>&lt;a name=»trace_viewer»&gt;&lt;/a&gt;</p>
<p>### Trace viewer</p>
<p>The trace viewer displays a timeline that shows:</p>
<ul class="simple">
<li><p>Durations for the ops that were executed by your TensorFlow model</p></li>
<li><p>Which part of the system (host or device) executed an op. Typically, the
host executes input operations, preprocesses training data and transfers it
to the device, while the device executes the actual model training</p></li>
</ul>
<p>The trace viewer allows you to identify performance problems in your model, then
take steps to resolve them. For example, at a high level, you can identify
whether input or model training is taking the majority of the time. Drilling
down, you can identify which ops take the longest to execute. Note that the
trace viewer is limited to 1 million events per device.</p>
<p>#### Trace viewer interface</p>
<p>When you open the trace viewer, it appears displaying your most recent run:</p>
<p>![image](./images/tf_profiler/trace_viewer.png)</p>
<p>This screen contains the following main elements:</p>
<ol class="arabic simple">
<li><p><strong>Timeline pane -</strong> Shows ops that the device and the host executed over
time</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Details pane -</strong> Shows additional information for ops selected in the
Timeline pane</p></li>
</ol>
<p>The Timeline pane contains the following elements:</p>
<p>1.  <strong>Top bar -</strong> Contains various auxiliary controls
1.  <strong>Time axis -</strong> Shows time relative to the beginning of the trace
1.  <strong>Section and track labels -</strong> Each section contains multiple tracks and has</p>
<blockquote>
<div><p>a triangle on the left that you can click to expand and collapse the
section. There is one section for every processing element in the system</p>
</div></blockquote>
<ol class="arabic simple">
<li><p><strong>Tool selector -</strong> Contains various tools for interacting with the trace
viewer such as Zoom, Pan, Select, and Timing. Use the Timing tool to mark a
time interval.</p></li>
</ol>
<ol class="arabic simple">
<li><p><strong>Events -</strong> These show the time during which an op was executed or the
duration of meta-events, such as training steps</p></li>
</ol>
<p>##### Sections and tracks</p>
<p>The trace viewer contains the following sections:</p>
<ul>
<li><p><strong>One section for each device node</strong>, labeled with the number of the device
chip and the device node within the chip (for example, <cite>/device:GPU:0 (pid
0)</cite>). Each device node section contains the following tracks:
-   <strong>Step -</strong> Shows the duration of the training steps that were running on</p>
<blockquote>
<div><p>the device</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>TensorFlow Ops -</strong>. Shows the ops executed on the device</p></li>
<li><p><strong>XLA Ops -</strong> Shows [XLA](<a class="reference external" href="https://www.tensorflow.org/xla/">https://www.tensorflow.org/xla/</a>) operations
(ops) that ran on the device if XLA is the compiler used (each
TensorFlow op is translated into one or several XLA ops. The XLA
compiler translates the XLA ops into code that runs on the device).</p></li>
</ul>
</li>
<li><p><strong>One section for threads running on the host machine’s CPU,</strong> labeled
<strong>«Host Threads»</strong>. The section contains one track for each CPU thread. Note
that you can ignore the information displayed alongside the section labels.</p></li>
</ul>
<p>##### Events</p>
<p>Events within the timeline are displayed in different colors; the colors
themselves have no specific meaning.</p>
<p>The trace viewer can also display traces of Python function calls in your
TensorFlow program. If you use the <cite>tf.profiler.experimental.start()</cite> API, you
can enable Python tracing by using the <cite>ProfilerOptions</cite> namedtuple when
starting profiling. Alternatively, if you use the sampling mode for profiling,
you can select the level of tracing by using the dropdown options in the
<strong>Capture Profile</strong> dialog.</p>
<p>![image](./images/tf_profiler/python_tracer.png)</p>
<p>&lt;a name=»gpu_kernel_stats»&gt;&lt;/a&gt;</p>
<p>### GPU kernel stats</p>
<p>This tool shows performance statistics and the originating op for every GPU
accelerated kernel.</p>
<p>![image](./images/tf_profiler/gpu_kernel_stats.png)</p>
<p>The tool displays information in two panes:</p>
<ul class="simple">
<li><p>The upper pane displays a pie chart which shows the CUDA kernels that have
the highest total time elapsed</p></li>
<li><p>The lower pane displays a table with the following data for each unique
kernel-op pair:</p>
<ul>
<li><p>A rank in descending order of total elapsed GPU duration grouped by
kernel-op pair</p></li>
<li><p>The name of the launched kernel</p></li>
<li><p>The number of GPU registers used by the kernel</p></li>
<li><p>The total size of shared (static + dynamic shared) memory used in bytes</p></li>
<li><p>The block dimension expressed as <cite>blockDim.x, blockDim.y, blockDim.z</cite></p></li>
<li><p>The grid dimensions expressed as <cite>gridDim.x, gridDim.y, gridDim.z</cite></p></li>
<li><p>Whether the op is eligible to use TensorCores</p></li>
<li><p>Whether the kernel contains TensorCore instructions</p></li>
<li><p>The name of the op that launched this kernel</p></li>
<li><p>The number of occurrences of this kernel-op pair</p></li>
<li><p>The total elapsed GPU time in microseconds</p></li>
<li><p>The average elapsed GPU time in microseconds</p></li>
<li><p>The minimum elapsed GPU time in microseconds</p></li>
<li><p>The maximum elapsed GPU time in microseconds</p></li>
</ul>
</li>
</ul>
<p>&lt;a name=»memory_profile_tool»&gt;&lt;/a&gt;</p>
<p>### Memory profile tool {: id = “memory_profile_tool”}</p>
<p>The Memory Profile tool monitors the memory usage of your device during the
profiling interval. You can use this tool to:</p>
<ul class="simple">
<li><p>Debug out of memory (OOM) issues by pinpointing peak memory usage and the
corresponding memory allocation to TensorFlow ops. You can also debug OOM
issues that may arise when you run
[multi-tenancy](<a class="reference external" href="https://arxiv.org/pdf/1901.06887.pdf">https://arxiv.org/pdf/1901.06887.pdf</a>) inference</p></li>
<li><p>Debug memory fragmentation issues</p></li>
</ul>
<p>The memory profile tool displays data in three sections:</p>
<p>1.  Memory Profile Summary
1.  Memory Timeline Graph
1.  Memory Breakdown Table</p>
<p>#### Memory profile summary</p>
<p>This section displays a high-level summary of the memory profile of your
TensorFlow program as shown below:</p>
<p>&lt;img src=»./images/tf_profiler/memory_profile_summary.png» width=»400», height=»450»&gt;</p>
<p>The memory profile summary has six fields:</p>
<ol class="arabic simple">
<li><p>Memory ID - Dropdown which lists all available device memory systems. Select
the memory system you want to view from the dropdown</p></li>
</ol>
<ol class="arabic simple">
<li><p>#Allocation - The number of memory allocations made during the profiling
interval</p></li>
</ol>
<p>1.  #Deallocation - The number of memory deallocations in the profiling interval
1.  Memory Capacity - The total capacity (in GiBs) of the memory system that you</p>
<blockquote>
<div><p>select</p>
</div></blockquote>
<ol class="arabic simple">
<li><p>Peak Heap Usage - The peak memory usage (in GiBs) since the model started
running</p></li>
</ol>
<ol class="arabic">
<li><p>Peak Memory Usage - The peak memory usage (in GiBs) in the profiling
interval. This field contains the following sub-fields:
1.  Timestamp - The timestamp of when the peak memory usage occurred on the</p>
<blockquote>
<div><p>Timeline Graph</p>
</div></blockquote>
<p>1.  Stack Reservation - Amount of memory reserved on the stack (in GiBs)
1.  Heap Allocation - Amount of memory allocated on the heap (in GiBs)
1.  Free Memory - Amount of free memory (in GiBs). The Memory Capacity is</p>
<blockquote>
<div><p>the sum total of the Stack Reservation, Heap Allocation, and Free Memory</p>
</div></blockquote>
<ol class="arabic simple">
<li><p>Fragmentation - The percentage of fragmentation (lower is better). It is
calculated as a percentage of (1 - Size of the largest chunk of free
memory / Total free memory)</p></li>
</ol>
</li>
</ol>
<p>#### Memory timeline graph</p>
<p>This section displays a plot of the memory usage (in GiBs) and the percentage of
fragmentation versus time (in ms).</p>
<p>![image](./images/tf_profiler/memory_timeline_graph.png)</p>
<p>The X-axis represents the timeline (in ms) of the profiling interval. The Y-axis
on the left represents the memory usage (in GiBs) and the Y-axis on the right
represents the percentage of fragmentation. At each point in time on the X-axis,
the total memory is broken down into three categories: stack (in red), heap (in
orange), and free (in green). Hover over a specific timestamp to view the
details about the memory allocation/deallocation events at that point like
below:</p>
<p>![image](./images/tf_profiler/memory_timeline_graph_popup.png)</p>
<p>The pop-up window displays the following information:</p>
<ul class="simple">
<li><p>timestamp(ms) - The location of the selected event on the timeline</p></li>
<li><p>event - The type of event (allocation or deallocation)</p></li>
<li><p>requested_size(GiBs) - The amount of memory requested. This will be a
negative number for deallocation events</p></li>
<li><p>allocation_size(GiBs) - The actual amount of memory allocated. This will be
a negative number for deallocation events</p></li>
<li><p>tf_op - The TensorFlow Op that requests the allocation/deallocation</p></li>
<li><p>step_id - The training step in which this event occurred</p></li>
<li><p>region_type - The data entity type that this allocated memory is for.
Possible values are <cite>temp</cite> for temporaries, <cite>output</cite> for activations and
gradients, and <cite>persist</cite>/<cite>dynamic</cite> for weights and constants</p></li>
<li><p>data_type - The tensor element type (e.g., uint8 for 8-bit unsigned integer)</p></li>
<li><p>tensor_shape - The shape of the tensor being allocated/deallocated</p></li>
<li><p>memory_in_use(GiBs) - The total memory that is in use at this point of time</p></li>
</ul>
<p>#### Memory breakdown table</p>
<p>This table shows the active memory allocations at the point of peak memory usage
in the profiling interval.</p>
<p>![image](./images/tf_profiler/memory_breakdown_table.png)</p>
<p>There is one row for each TensorFlow Op and each row has the following columns:</p>
<ul class="simple">
<li><p>Op Name - The name of the TensorFlow op</p></li>
<li><p>Allocation Size (GiBs) - The total amount of memory allocated to this op</p></li>
<li><p>Requested Size (GiBs) - The total amount of memory requested for this op</p></li>
<li><p>Occurrences - The number of allocations for this op</p></li>
<li><p>Region type - The data entity type that this allocated memory is for.
Possible values are <cite>temp</cite> for temporaries, <cite>output</cite> for activations and
gradients, and <cite>persist</cite>/<cite>dynamic</cite> for weights and constants</p></li>
<li><p>Data type - The tensor element type</p></li>
<li><p>Shape - The shape of the allocated tensors</p></li>
</ul>
<p>Note: You can sort any column in the table and also filter rows by op name.</p>
<p>&lt;a name=»pod_viewer»&gt;&lt;/a&gt;</p>
<p>### Pod viewer</p>
<p>The Pod Viewer tool shows the breakdown of a training step across all workers.</p>
<p>![image](./images/tf_profiler/pod_viewer.png)</p>
<ul class="simple">
<li><p>The upper pane has slider for selecting the step number.</p></li>
<li><p>The lower pane displays a stacked column chart. This is a high level view of
broken down step-time categories placed atop one another. Each stacked
column represents a unique worker.</p></li>
<li><p>When you hover over a stacked column, the card on the left-hand side shows
more details about the step breakdown.</p></li>
</ul>
<p>&lt;a name=»tf_data_bottleneck_analysis»&gt;&lt;/a&gt;</p>
<p>### tf.data bottleneck analysis</p>
<p>Warning: This tool is experimental. Please report
[here](<a class="reference external" href="https://github.com/tensorflow/profiler/issues">https://github.com/tensorflow/profiler/issues</a>) if the analysis result
seems off.</p>
<p>tf.data bottleneck analysis automatically detects bottlenecks in tf.data input
pipelines in your program and provides recommendations on how to fix them. It
works with any program using tf.data regardless of the platform (CPU/GPU/TPU) or
the framework (TensorFlow/JAX). Its analysis and recommendations are based on
this [guide](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance_analysis">https://www.tensorflow.org/guide/data_performance_analysis</a>).</p>
<p>It detects a bottleneck by following these steps:</p>
<p>1.  Find the most input bound host.
1.  Find the slowest execution of tf.data input pipeline.
1.  Reconstruct the input pipeline graph from the profiler trace.
1.  Find the critical path in the input pipeline graph.
1.  Identify the slowest transformation on the critical path as a bottleneck.</p>
<p>The UI is divided into three sections: Performance Analysis Summary, Summary of
All Input Pipelines and Input Pipeline Graph.</p>
<p>#### Performance analysis summary</p>
<p>![image](./images/tf_profiler/tf_data_summary.png)</p>
<p>This section provides the summary of the analysis. It tells whether a slow
tf.data input pipeline is detected in the profile. If so, it shows the most
input bound host and its slowest input pipeline with the max latency. And most
importantly, it tells which part of the input pipeline is the bottleneck and how
to fix it. The bottleneck information is provided with the iterator type and its
long name.</p>
<p>##### How to read tf.data iterator’s long name</p>
<p>A long name is formatted as <cite>Iterator::&lt;Dataset_1&gt;::…::&lt;Dataset_n&gt;</cite>. In the
long name, <cite>&lt;Dataset_n&gt;</cite> matches the iterator type and the other datasets in the
long name represent downstream transformations.</p>
<p>For example, consider the following input pipeline dataset:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">dataset</span> <span class="pre">=</span> <span class="pre">tf.data.Dataset.range(10).map(lambda</span> <span class="pre">x:</span> <span class="pre">x).repeat(2).batch(5)</span>
<span class="pre">`</span></code></p>
<p>The long names for the iterators from the above dataset will be:</p>
<p>Iterator Type | Long Name
:———— | :———————————-
Range         | Iterator::Batch::Repeat::Map::Range
Map           | Iterator::Batch::Repeat::Map
Repeat        | Iterator::Batch::Repeat
Batch         | Iterator::Batch</p>
<p>#### Summary of All Input Pipelines</p>
<p>![image](./images/tf_profiler/tf_data_all_hosts.png)</p>
<p>This section provides the summary of all input pipelines across all hosts.
Typically there is one input pipeline. When using the distribution strategy,
there are one host input pipeline running the program’s tf.data code and
multiple device input pipelines retrieving data from the host input pipeline and
transferring it to the devices.</p>
<p>For each input pipeline, it shows the statistics of its execution time. A call
is counted as slow if it takes longer than 50 μs.</p>
<p>#### Input Pipeline Graph</p>
<p>![image](./images/tf_profiler/tf_data_graph_selector.png)</p>
<p>This section shows the input pipeline graph with the execution time information.
You can use «Host» and «Input Pipeline» to choose which host and input pipeline
to see. Executions of the input pipeline are sorted by the execution time in
descending order which you can use «Rank» to choose.</p>
<p>![image](./images/tf_profiler/tf_data_graph.png)</p>
<p>The nodes on the critical path have bold outlines. The bottleneck node, which is
the node with the longest self time on the critical path, has a red outline. The
other non-critical nodes have gray dashed outlines.</p>
<p>In each node, «Start Time» indicates the start time of the execution. The same
node may be executed multiple times, for example, if there is Batch in the input
pipeline. If it is executed multiple times, it is the start time of the first
execution.</p>
<p>«Total Duration» is the wall time of the execution. If it is executed multiple
times, it is the sum of the wall times of all executions.</p>
<p>«Self Time» is «Total Time» without the overlapped time with its immediate child
nodes.</p>
<p>«# Calls» is the number of times the input pipeline is executed.</p>
<p>&lt;a name=»collect_performance_data»&gt;&lt;/a&gt;</p>
<p>## Collect performance data</p>
<p>The TensorFlow Profiler collects host activities and GPU traces of your
TensorFlow model. You can configure the Profiler to collect performance data
through either the programmatic mode or the sampling mode.</p>
<p>### Profiling APIs</p>
<p>You can use the following APIs to perform profiling.</p>
<ul>
<li><p>Programmatic mode using the TensorBoard Keras Callback
(<cite>tf.keras.callbacks.TensorBoard</cite>)</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
# Profile from batches 10 to 15
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,</p>
<blockquote>
<div><p>profile_batch=”10, 15”)</p>
</div></blockquote>
<p># Train the model and use the TensorBoard Keras callback to collect
# performance profiling data
model.fit(train_data,</p>
<blockquote>
<div><p>steps_per_epoch=20,
epochs=5,
callbacks=[tb_callback])</p>
</div></blockquote>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
</li>
<li><p>Programmatic mode using the <cite>tf.profiler</cite> Function API</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">tf.profiler.experimental.start('logdir')</span>
<span class="pre">#</span> <span class="pre">Train</span> <span class="pre">the</span> <span class="pre">model</span> <span class="pre">here</span>
<span class="pre">tf.profiler.experimental.stop()</span>
<span class="pre">`</span></code></p>
</li>
<li><p>Programmatic mode using the context manager</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
with tf.profiler.experimental.Profile(“logdir”):</p>
<blockquote>
<div><p># Train the model here
pass</p>
</div></blockquote>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
</li>
</ul>
<p>Note: Running the Profiler for too long can cause it to run out of memory. It is
recommended to profile no more than 10 steps at a time. Avoid profiling the
first few batches to avoid inaccuracies due to initialization overhead.</p>
<p>&lt;a name=»sampling_mode»&gt;&lt;/a&gt;</p>
<ul>
<li><p>Sampling mode - Perform on-demand profiling by using
<cite>tf.profiler.experimental.server.start()</cite> to start a gRPC server with your
TensorFlow model run. After starting the gRPC server and running your model,
you can capture a profile through the <strong>Capture Profile</strong> button in the
TensorBoard profile plugin. Use the script in the Install profiler section
above to launch a TensorBoard instance if it is not already running.</p>
<p>As an example,</p>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>python
# Start a profiler server before your model runs.
tf.profiler.experimental.server.start(6009)
# (Model code goes here).
#  Send a request to the profiler server to collect a trace of your model.
tf.profiler.experimental.client.trace(“grpc://localhost:6009”,</p>
<blockquote>
<div><p>“gs://your_tb_logdir”, 2000)</p>
</div></blockquote>
<p><a href="#id21"><span class="problematic" id="id22">``</span></a><a href="#id23"><span class="problematic" id="id24">`</span></a></p>
<p>An example for profiling multiple workers:</p>
<p><a href="#id25"><span class="problematic" id="id26">``</span></a><a href="#id27"><span class="problematic" id="id28">`</span></a>python
# E.g. your worker IP addresses are 10.0.0.2, 10.0.0.3, 10.0.0.4, and you
# would like to profile for a duration of 2 seconds.
tf.profiler.experimental.client.trace(</p>
<blockquote>
<div><p>“grpc://10.0.0.2:8466,grpc://10.0.0.3:8466,grpc://10.0.0.4:8466”,
“gs://your_tb_logdir”,
2000)</p>
</div></blockquote>
<p><a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a></p>
</li>
</ul>
<p>&lt;a name=»capture_dialog»&gt;&lt;/a&gt;</p>
<p>&lt;img src=»./images/tf_profiler/capture_profile.png» width=»400», height=»450»&gt;</p>
<p>Use the <strong>Capture Profile</strong> dialog to specify:</p>
<ul class="simple">
<li><p>A comma delimited list of profile service URLs or TPU name.</p></li>
<li><p>A profiling duration.</p></li>
<li><p>The level of device, host, and Python function call tracing.</p></li>
<li><p>How many times you want the Profiler to retry capturing profiles if
unsuccessful at first.</p></li>
</ul>
<p>### Profiling custom training loops</p>
<p>To profile custom training loops in your TensorFlow code, instrument the
training loop with the <cite>tf.profiler.experimental.Trace</cite> API to mark the step
boundaries for the Profiler. The <cite>name</cite> argument is used as a prefix for the
step names, the <cite>step_num</cite> keyword argument is appended in the step names, and
the <cite>_r</cite> keyword argument makes this trace event get processed as a step event
by the Profiler.</p>
<p>As an example,</p>
<p><a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>python
for step in range(NUM_STEPS):</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.profiler.experimental.Trace(“train”, step_num=step, _r=1):</dt><dd><p>train_data = next(dataset)
train_step(train_data)</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id37"><span class="problematic" id="id38">``</span></a><a href="#id39"><span class="problematic" id="id40">`</span></a></p>
<p>This will enable the Profiler’s step-based performance analysis and cause the
step events to show up in the trace viewer.</p>
<p>Ensure that you include the dataset iterator within the
<cite>tf.profiler.experimental.Trace</cite> context for accurate analysis of the input
pipeline.</p>
<p>The code snippet below is an anti-pattern:</p>
<p>Warning: This will result in inaccurate analysis of the input pipeline.</p>
<p><a href="#id41"><span class="problematic" id="id42">``</span></a><a href="#id43"><span class="problematic" id="id44">`</span></a>python
for step, train_data in enumerate(dataset):</p>
<blockquote>
<div><dl class="simple">
<dt>with tf.profiler.experimental.Trace(“train”, step_num=step, _r=1):</dt><dd><p>train_step(train_data)</p>
</dd>
</dl>
</div></blockquote>
<p><a href="#id45"><span class="problematic" id="id46">``</span></a><a href="#id47"><span class="problematic" id="id48">`</span></a></p>
<p>### Profiling use cases</p>
<p>The profiler covers a number of use cases along four different axes. Some of the
combinations are currently supported and others will be added in the future.
Some of the use cases are:</p>
<ul class="simple">
<li><p>Local vs. Remote profiling: These are two common ways of setting up your
profiling environment. In local profiling, the profiling API is called on
the same machine your model is executing, for example, a local workstation
with GPUs. In remote profiling, the profiling API is called on a different
machine from where your model is executing, for example, on a Cloud TPU.</p></li>
<li><p>Profiling multiple workers: You can profile multiple machines when using the
distributed training capabilities of TensorFlow.</p></li>
<li><p>Hardware platform: Profile CPUs, GPUs, and TPUs.</p></li>
</ul>
<p>The table below is a quick overview of which of the above use cases are
supported by the various profiling APIs in TensorFlow:</p>
<p>&lt;a name=»profiling_api_table»&gt;&lt;/a&gt;</p>
<div class="line-block">
<div class="line">Profiling API                | Local     | Remote    | Multiple  | Hardware  |</div>
</div>
<p>:                              :           :           : workers   : Platforms :
| :————————— | :——– | :——– | :——– | :——– |
| <strong>TensorBoard Keras          | Supported | Not       | Not       | CPU, GPU  |
: Callback</strong>                   :           : Supported : Supported :           :
| <strong>`tf.profiler.experimental` | Supported | Not       | Not       | CPU, GPU  |
: start/stop [API][API_0]</strong>    :           : Supported : Supported :           :
| <strong>`tf.profiler.experimental` | Supported | Supported | Supported | CPU, GPU, |
: client.trace [API][API_1]</strong>  :           :           :           : TPU       :
| <strong>Context manager API</strong>      | Supported | Not       | Not       | CPU, GPU  |
:                              :           : supported : Supported :           :</p>
<p>[API_0]: <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/profiler/experimental#functions_2">https://www.tensorflow.org/api_docs/python/tf/profiler/experimental#functions_2</a>
[API_1]: <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace">https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace</a></p>
<p>&lt;a name=»performance_best_practices»&gt;&lt;/a&gt;</p>
<p>## Best practices for optimal model performance</p>
<p>Use the following recommendations as applicable for your TensorFlow models to
achieve optimal performance.</p>
<p>In general, perform all transformations on the device and ensure that you use
the latest compatible version of libraries like cuDNN and Intel MKL for your
platform.</p>
<p>### Optimize the input data pipeline</p>
<p>An efficient data input pipeline can drastically improve the speed of your model
execution by reducing device idle time. Consider incorporating the following
best practices as detailed
[here](<a class="reference external" href="https://www.tensorflow.org/guide/data_performance">https://www.tensorflow.org/guide/data_performance</a>) to make your data
input pipeline more efficient:</p>
<ul class="simple">
<li><p>Prefetch data</p></li>
<li><p>Parallelize data extraction</p></li>
<li><p>Parallelize data transformation</p></li>
<li><p>Cache data in memory</p></li>
<li><p>Vectorize user-defined functions</p></li>
<li><p>Reduce memory usage when applying transformations</p></li>
</ul>
<p>Additionally, try running your model with synthetic data to check if the input
pipeline is a performance bottleneck.</p>
<p>### Improve device performance</p>
<ul class="simple">
<li><p>Increase training mini-batch size (number of training samples used per
device in one iteration of the training loop)</p></li>
<li><p>Use TF Stats to find out how efficiently on-device ops run</p></li>
<li><p>Use <cite>tf.function</cite> to perform computations and optionally, enable the
<cite>experimental_compile</cite> flag</p></li>
<li><p>Minimize host Python operations between steps and reduce callbacks.
Calculate metrics every few steps instead of at every step</p></li>
<li><p>Keep the device compute units busy</p></li>
<li><p>Send data to multiple devices in parallel</p></li>
<li><p>Optimize data layout to prefer channels first (e.g. NCHW over NHWC). Certain
GPUs like the NVIDIA® V100 perform better with a NHWC data layout.</p></li>
<li><p>Consider using 16-bit numerical representations such as <cite>fp16</cite>, the
half-precision floating point format specified by IEEE or the Brain
floating-point [bfloat16](<a class="reference external" href="https://cloud.google.com/tpu/docs/bfloat16">https://cloud.google.com/tpu/docs/bfloat16</a>) format</p></li>
<li><p>Consider using the
[Keras mixed precision API](<a class="reference external" href="https://www.tensorflow.org/guide/keras/mixed_precision">https://www.tensorflow.org/guide/keras/mixed_precision</a>)</p></li>
<li><p>When training on GPUs, make use of the TensorCore. GPU kernels use the
TensorCore when the precision is fp16 and input/output dimensions are
divisible by 8 or 16 (for int8)</p></li>
</ul>
<p>## Additional resources</p>
<ul class="simple">
<li><p>See the end-to-end
[TensorBoard profiler tutorial](<a class="reference external" href="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras</a>)
to implement the advice in this guide.</p></li>
<li><p>Watch the
[Performance profiling in TF 2](<a class="reference external" href="https://www.youtube.com/watch?v=pXHAQIhhMhI">https://www.youtube.com/watch?v=pXHAQIhhMhI</a>)
talk from the TensorFlow Dev Summit 2020.</p></li>
</ul>
<p>## Known limitations</p>
<p>### Profiling multiple GPUs on TensorFlow 2.2 and TensorFlow 2.3</p>
<p>TensorFlow 2.2 and 2.3 support multiple GPU profiling for single host systems
only; multiple GPU profiling for multi-host systems is not supported. To profile
multi-worker GPU configurations, each worker has to be profiled independently.
On TensorFlow 2.4, multiple workers can be profiled using the
[<cite>tf.profiler.experimental.trace</cite>](<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace">https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace</a>)
API.</p>
<p>CUDA® Toolkit 10.2 or later is required to profile multiple GPUs. As TensorFlow
2.2 and 2.3 support CUDA® Toolkit versions only up to 10.1 , create symbolic
links to <cite>libcudart.so.10.1</cite> and <cite>libcupti.so.10.1</cite>.</p>
<p><code class="docutils literal notranslate"><span class="pre">`shell</span>
<span class="pre">sudo</span> <span class="pre">ln</span> <span class="pre">-s</span> <span class="pre">/usr/local/cuda/lib64/libcudart.so.10.2</span> <span class="pre">/usr/local/cuda/lib64/libcudart.so.10.1</span>
<span class="pre">sudo</span> <span class="pre">ln</span> <span class="pre">-s</span> <span class="pre">/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.2</span> <span class="pre">/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.1</span>
<span class="pre">`</span></code></p>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>