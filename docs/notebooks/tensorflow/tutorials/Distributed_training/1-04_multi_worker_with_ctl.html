

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Custom training loop with Keras and MultiWorkerMirroredStrategy &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Custom training loop with Keras and MultiWorkerMirroredStrategy</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Distributed_training/1-04_multi_worker_with_ctl.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Custom-training-loop-with-Keras-and-MultiWorkerMirroredStrategy">
<h1>Custom training loop with Keras and MultiWorkerMirroredStrategy<a class="headerlink" href="#Custom-training-loop-with-Keras-and-MultiWorkerMirroredStrategy" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>f7a6943227da455a94427d8b02320b0c|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>c3302f0ca52a477ea53382492a0ecece|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>59b7d63572684219a9056d22cd88741e|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>c2c5d144b4364bd4b9ddda2a743628c3|Download notebook</p>
</td></table><div class="section" id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial demonstrates multi-worker training with custom training loop API, distributed via MultiWorkerMirroredStrategy, so a Keras model designed to run on <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/custom_training">single-worker</a> can seamlessly work on multiple workers with minimal code change.</p>
<p>We are using custom training loops to train our model because they give us flexibility and a greater control on training. Moreover, it is easier to debug the model and the training loop. More detailed information is available in <a class="reference external" href="https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch">Writing a training loop from scratch</a>.</p>
<p>If you are looking for how to use <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> with keras <code class="docutils literal notranslate"><span class="pre">model.fit</span></code>, refer to this <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras">tutorial</a> instead.</p>
<p><a class="reference internal" href="../../guide/distributed_training.html"><span class="doc">Distributed Training in TensorFlow</span></a> guide is available for an overview of the distribution strategies TensorFlow supports for those interested in a deeper understanding of <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> APIs.</p>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<p>First, some necessary imports.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
</pre></div>
</div>
</div>
<p>Before importing TensorFlow, make a few changes to the environment.</p>
<p>Disable all GPUs. This prevents errors caused by the workers all trying to use the same GPU. For a real application each worker would be on a different machine.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;-1&quot;</span>
</pre></div>
</div>
</div>
<p>Reset the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable, you’ll see more about this later.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Be sure that the current directory is on python’s path. This allows the notebook to import the files written by <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> later.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">if</span> <span class="s1">&#39;.&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
  <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now import TensorFlow.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
<div class="section" id="Dataset-and-model-definition">
<h3>Dataset and model definition<a class="headerlink" href="#Dataset-and-model-definition" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next create an <code class="docutils literal notranslate"><span class="pre">mnist.py</span></code> file with a simple model and dataset setup. This python file will be used by the worker-processes in this tutorial:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%writefile</span> mnist.py

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">mnist_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
  <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
  <span class="c1"># The `x` arrays are in uint8 and have values in the range [0, 255].</span>
  <span class="c1"># You need to convert them to float32 with values in the range [0, 1]</span>
  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span>
  <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
      <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">60000</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_dataset</span>

<span class="k">def</span> <span class="nf">dataset_fn</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">input_context</span><span class="p">):</span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_context</span><span class="o">.</span><span class="n">get_per_replica_batch_size</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">mnist_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">input_context</span><span class="o">.</span><span class="n">num_input_pipelines</span><span class="p">,</span>
                          <span class="n">input_context</span><span class="o">.</span><span class="n">input_pipeline_id</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">build_cnn_model</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
  <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Multi-worker-Configuration">
<h2>Multi-worker Configuration<a class="headerlink" href="#Multi-worker-Configuration" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Now let’s enter the world of multi-worker training. In TensorFlow, the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable is required for training on multiple machines, each of which possibly has a different role. <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> used below, is a JSON string used to specify the cluster configuration on each worker that is part of the cluster. This is the default method for specifying a cluster, using <code class="docutils literal notranslate"><span class="pre">cluster_resolver.TFConfigClusterResolver</span></code>, but there are other options available in the
<code class="docutils literal notranslate"><span class="pre">distribute.cluster_resolver</span></code> module.</p>
<div class="section" id="Describe-your-cluster">
<h3>Describe your cluster<a class="headerlink" href="#Describe-your-cluster" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Here is an example configuration:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;cluster&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;worker&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;localhost:12345&#39;</span><span class="p">,</span> <span class="s1">&#39;localhost:23456&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;task&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>Here is the same <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> serialized as a JSON string:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>There are two components of <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code>: <code class="docutils literal notranslate"><span class="pre">cluster</span></code> and <code class="docutils literal notranslate"><span class="pre">task</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cluster</span></code> is the same for all workers and provides information about the training cluster, which is a dict consisting of different types of jobs such as <code class="docutils literal notranslate"><span class="pre">worker</span></code>. In multi-worker training with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>, there is usually one <code class="docutils literal notranslate"><span class="pre">worker</span></code> that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular <code class="docutils literal notranslate"><span class="pre">worker</span></code> does. Such a worker is referred to as the <code class="docutils literal notranslate"><span class="pre">chief</span></code> worker, and it is customary that
the <code class="docutils literal notranslate"><span class="pre">worker</span></code> with <code class="docutils literal notranslate"><span class="pre">index</span></code> 0 is appointed as the chief <code class="docutils literal notranslate"><span class="pre">worker</span></code> (in fact this is how <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is implemented).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task</span></code> provides information of the current task and is different on each worker. It specifies the <code class="docutils literal notranslate"><span class="pre">type</span></code> and <code class="docutils literal notranslate"><span class="pre">index</span></code> of that worker.</p></li>
</ul>
<p>In this example, you set the task <code class="docutils literal notranslate"><span class="pre">type</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;worker&quot;</span></code> and the task <code class="docutils literal notranslate"><span class="pre">index</span></code> to <code class="docutils literal notranslate"><span class="pre">0</span></code>. This machine is the first worker and will be appointed as the chief worker and do more work than the others. Note that other machines will need to have the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable set as well, and it should have the same <code class="docutils literal notranslate"><span class="pre">cluster</span></code> dict, but different task <code class="docutils literal notranslate"><span class="pre">type</span></code> or task <code class="docutils literal notranslate"><span class="pre">index</span></code> depending on what the roles of those machines are.</p>
<p>For illustration purposes, this tutorial shows how one may set a <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> with 2 workers on <code class="docutils literal notranslate"><span class="pre">localhost</span></code>. In practice, users would create multiple workers on external IP addresses/ports, and set <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> on each worker appropriately.</p>
<p>In this example you will use 2 workers, the first worker’s <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is shown above. For the second worker you would set <code class="docutils literal notranslate"><span class="pre">tf_config['task']['index']=1</span></code></p>
<p>Above, <code class="docutils literal notranslate"><span class="pre">tf_config</span></code> is just a local variable in python. To actually use it to configure training, this dictionary needs to be serialized as JSON, and placed in the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable.</p>
</div>
<div class="section" id="Environment-variables-and-subprocesses-in-notebooks">
<h3>Environment variables and subprocesses in notebooks<a class="headerlink" href="#Environment-variables-and-subprocesses-in-notebooks" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Subprocesses inherit environment variables from their parent. So if you set an environment variable in this <code class="docutils literal notranslate"><span class="pre">jupyter</span> <span class="pre">notebook</span></code> process:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;GREETINGS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Hello TensorFlow!&#39;</span>
</pre></div>
</div>
</div>
<p>You can access the environment variable from a subprocesses:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
<span class="nb">echo</span> <span class="si">${</span><span class="nv">GREETINGS</span><span class="si">}</span>
</pre></div>
</div>
</div>
<p>In the next section, you’ll use this to pass the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> to the worker subprocesses. You would never really launch your jobs this way, but it’s sufficient for the purposes of this tutorial: To demonstrate a minimal multi-worker example.</p>
</div>
</div>
<div class="section" id="MultiWorkerMirroredStrategy">
<h2>MultiWorkerMirroredStrategy<a class="headerlink" href="#MultiWorkerMirroredStrategy" title="Enlazar permanentemente con este título">¶</a></h2>
<p>To train the model, use an instance of <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>, which creates copies of all variables in the model’s layers on each device across all workers. The <code class="docutils literal notranslate"><span class="pre">`tf.distribute.Strategy</span></code> guide &lt;../../guide/distributed_training.ipynb&gt;`__ has more details about this strategy.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Note: <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is parsed and TensorFlow’s GRPC servers are started at the time <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy()</span></code> is called, so the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable must be set before a <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> instance is created.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.scope</span></code> to specify that a strategy should be used when building your model. This puts you in the “<a class="reference external" href="https://www.tensorflow.org/guide/distributed_training?hl=en#mirroredstrategy">cross-replica context</a>” for this strategy, which means the strategy is put in control of things like variable placement.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">mnist</span>
<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="c1"># Model building needs to be within `strategy.scope()`.</span>
  <span class="n">multi_worker_model</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">build_cnn_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Auto-shard-your-data-across-workers">
<h2>Auto-shard your data across workers<a class="headerlink" href="#Auto-shard-your-data-across-workers" title="Enlazar permanentemente con este título">¶</a></h2>
<p>In multi-worker training, dataset sharding is not necessarily needed, however it gives you exactly once semantic which makes more training more reproducible, i.e. training on multiple workers should be the same as training on one worker. Note: performance can be affected in some cases.</p>
<p>See: <code class="docutils literal notranslate"><span class="pre">`distribute_datasets_from_function</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#distribute_datasets_from_function">https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#distribute_datasets_from_function</a>&gt;`__</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">per_worker_batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf_config</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">][</span><span class="s1">&#39;worker&#39;</span><span class="p">])</span>
<span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">per_worker_batch_size</span> <span class="o">*</span> <span class="n">num_workers</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="n">multi_worker_dataset</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">distribute_datasets_from_function</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">input_context</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">dataset_fn</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">input_context</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-Custom-Training-Loop-and-Train-the-model">
<h2>Define Custom Training Loop and Train the model<a class="headerlink" href="#Define-Custom-Training-Loop-and-Train-the-model" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Specify an optimizer</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="c1"># The creation of optimizer and train_accuracy will need to be in</span>
  <span class="c1"># `strategy.scope()` as well, since they create variables.</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
  <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
      <span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Define a training step with <code class="docutils literal notranslate"><span class="pre">tf.function</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Training step function.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">step_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Per-Replica step function.&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
      <span class="n">predictions</span> <span class="o">=</span> <span class="n">multi_worker_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">per_batch_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
          <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span>
          <span class="n">per_batch_loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="n">train_accuracy</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

  <span class="n">per_replica_losses</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">step_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">),))</span>
  <span class="k">return</span> <span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">per_replica_losses</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Checkpoint-saving-and-restoring">
<h3>Checkpoint saving and restoring<a class="headerlink" href="#Checkpoint-saving-and-restoring" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Checkpointing implementation in a Custom Training Loop requires the user to handle it instead of using a keras callback. It allows you to save model’s weights and restore them without having to save the whole model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">util</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">util</span><span class="o">.</span><span class="n">get_temp_dir</span><span class="p">(),</span> <span class="s1">&#39;ckpt&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">task_type</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;chief&#39;</span> <span class="ow">or</span> <span class="p">(</span><span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;worker&#39;</span> <span class="ow">and</span>
                                                       <span class="n">task_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_get_temp_dir</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">base_dirpath</span> <span class="o">=</span> <span class="s1">&#39;workertemp_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span>
  <span class="n">temp_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">base_dirpath</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">temp_dir</span>

<span class="k">def</span> <span class="nf">write_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">dirpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="n">base</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
    <span class="n">dirpath</span> <span class="o">=</span> <span class="n">_get_temp_dir</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">base</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Note: Checkpointing and Saving need to happen on each worker and they need to write to different paths as they would override each others. If you chose to only checkpoint/save on the chief, this can lead to deadlock and is not recommended.</p>
<p>Here, you’ll create one <code class="docutils literal notranslate"><span class="pre">tf.train.Checkpoint</span></code> that tracks the model, which is managed by a <code class="docutils literal notranslate"><span class="pre">tf.train.CheckpointManager</span></code> so that only the latest checkpoint is preserved.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">epoch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">step_in_epoch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;step_in_epoch&#39;</span><span class="p">)</span>
<span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">task_type</span><span class="p">,</span>
                      <span class="n">strategy</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">multi_worker_model</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">step_in_epoch</span><span class="o">=</span><span class="n">step_in_epoch</span><span class="p">)</span>

<span class="n">write_checkpoint_dir</span> <span class="o">=</span> <span class="n">write_filepath</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
<span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">write_checkpoint_dir</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, when you need to restore, you can find the latest checkpoint saved using the convenient <code class="docutils literal notranslate"><span class="pre">tf.train.latest_checkpoint</span></code> function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">latest_checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
<span class="k">if</span> <span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">checkpoint</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">latest_checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>After restoring the checkpoint, you can continue with training your custom training loop.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_steps_per_epoch</span> <span class="o">=</span> <span class="mi">70</span>

<span class="k">while</span> <span class="n">epoch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">num_epochs</span><span class="p">:</span>
  <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">multi_worker_dataset</span><span class="p">)</span>
  <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">while</span> <span class="n">step_in_epoch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">num_steps_per_epoch</span><span class="p">:</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
    <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">step_in_epoch</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_batches</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">%d</span><span class="s1">, accuracy: </span><span class="si">%f</span><span class="s1">, train_loss: </span><span class="si">%f</span><span class="s1">.&#39;</span>
                <span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">train_loss</span><span class="p">))</span>

  <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>

  <span class="c1"># Once the `CheckpointManager` is set up, you&#39;re now ready to save, and remove</span>
  <span class="c1"># the checkpoints non-chief workers saved.</span>
  <span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">write_checkpoint_dir</span><span class="p">)</span>

  <span class="n">epoch</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">step_in_epoch</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Full-code-setup-on-workers">
<h2>Full code setup on workers<a class="headerlink" href="#Full-code-setup-on-workers" title="Enlazar permanentemente con este título">¶</a></h2>
<p>To actually run with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> you’ll need to run worker processes and pass a <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> to them.</p>
<p>Like the <code class="docutils literal notranslate"><span class="pre">mnist.py</span></code> file written earlier, here is the <code class="docutils literal notranslate"><span class="pre">main.py</span></code> that contain the same code we walked through step by step previously in this colab, we’re just writing it to a file so each of the workers will run it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%writefile</span> main.py
<span class="c1">#@title File: `main.py`</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">mnist</span>
<span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">util</span>

<span class="n">per_worker_batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">tf_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">])</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf_config</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">][</span><span class="s1">&#39;worker&#39;</span><span class="p">])</span>
<span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">per_worker_batch_size</span> <span class="o">*</span> <span class="n">num_workers</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_steps_per_epoch</span><span class="o">=</span><span class="mi">70</span>

<span class="c1"># Checkpoint saving and restoring</span>
<span class="k">def</span> <span class="nf">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">task_type</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;chief&#39;</span> <span class="ow">or</span> <span class="p">(</span><span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;worker&#39;</span> <span class="ow">and</span>
                                                       <span class="n">task_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_get_temp_dir</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">base_dirpath</span> <span class="o">=</span> <span class="s1">&#39;workertemp_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span>
  <span class="n">temp_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">base_dirpath</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">temp_dir</span>

<span class="k">def</span> <span class="nf">write_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">dirpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="n">base</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
    <span class="n">dirpath</span> <span class="o">=</span> <span class="n">_get_temp_dir</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">base</span><span class="p">)</span>

<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">util</span><span class="o">.</span><span class="n">get_temp_dir</span><span class="p">(),</span> <span class="s1">&#39;ckpt&#39;</span><span class="p">)</span>

<span class="c1"># Define Strategy</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="c1"># Model building/compiling need to be within `strategy.scope()`.</span>
  <span class="n">multi_worker_model</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">build_cnn_model</span><span class="p">()</span>

  <span class="n">multi_worker_dataset</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">distribute_datasets_from_function</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">input_context</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">dataset_fn</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">,</span> <span class="n">input_context</span><span class="p">))</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
  <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
      <span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Training step function.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">step_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Per-Replica step function.&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
      <span class="n">predictions</span> <span class="o">=</span> <span class="n">multi_worker_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">per_batch_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
          <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span>
          <span class="n">per_batch_loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
    <span class="n">train_accuracy</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>

  <span class="n">per_replica_losses</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">step_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">),))</span>
  <span class="k">return</span> <span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">per_replica_losses</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">epoch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">step_in_epoch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;step_in_epoch&#39;</span><span class="p">)</span>

<span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">task_type</span><span class="p">,</span>
                      <span class="n">strategy</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">multi_worker_model</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">step_in_epoch</span><span class="o">=</span><span class="n">step_in_epoch</span><span class="p">)</span>

<span class="n">write_checkpoint_dir</span> <span class="o">=</span> <span class="n">write_filepath</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
<span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">write_checkpoint_dir</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Restoring the checkpoint</span>
<span class="n">latest_checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
<span class="k">if</span> <span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">checkpoint</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">latest_checkpoint</span><span class="p">)</span>

<span class="c1"># Resume our CTL training</span>
<span class="k">while</span> <span class="n">epoch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">num_epochs</span><span class="p">:</span>
  <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">multi_worker_dataset</span><span class="p">)</span>
  <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">while</span> <span class="n">step_in_epoch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">num_steps_per_epoch</span><span class="p">:</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
    <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">step_in_epoch</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">train_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_batches</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">%d</span><span class="s1">, accuracy: </span><span class="si">%f</span><span class="s1">, train_loss: </span><span class="si">%f</span><span class="s1">.&#39;</span>
                <span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">train_loss</span><span class="p">))</span>

  <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>

  <span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">write_checkpoint_dir</span><span class="p">)</span>

  <span class="n">epoch</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">step_in_epoch</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Train-and-Evaluate">
<h2>Train and Evaluate<a class="headerlink" href="#Train-and-Evaluate" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The current directory now contains both Python files:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
ls *.py
</pre></div>
</div>
</div>
<p>So json-serialize the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> and add it to the environment variables:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, you can launch a worker process that will run the <code class="docutils literal notranslate"><span class="pre">main.py</span></code> and use the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># first kill any previous runs</span>
<span class="o">%</span><span class="k">killbgscripts</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash --bg
python main.py <span class="p">&amp;</span>&gt; job_0.log
</pre></div>
</div>
</div>
<p>There are a few things to note about the above command:</p>
<ol class="arabic simple">
<li><p>It uses the <code class="docutils literal notranslate"><span class="pre">%%bash</span></code> which is a <a class="reference external" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">notebook “magic”</a> to run some bash commands.</p></li>
<li><p>It uses the <code class="docutils literal notranslate"><span class="pre">--bg</span></code> flag to run the <code class="docutils literal notranslate"><span class="pre">bash</span></code> process in the background, because this worker will not terminate. It waits for all the workers before it starts.</p></li>
</ol>
<p>The backgrounded worker process won’t print output to this notebook, so the <code class="docutils literal notranslate"><span class="pre">&amp;&gt;</span></code> redirects its output to a file, so you can see what happened.</p>
<p>So, wait a few seconds for the process to start up:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now look what’s been output to the worker’s logfile so far:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
cat job_0.log
</pre></div>
</div>
</div>
<p>The last line of the log file should say: <code class="docutils literal notranslate"><span class="pre">Started</span> <span class="pre">server</span> <span class="pre">with</span> <span class="pre">target:</span> <span class="pre">grpc://localhost:12345</span></code>. The first worker is now ready, and is waiting for all the other worker(s) to be ready to proceed.</p>
<p>So update the <code class="docutils literal notranslate"><span class="pre">tf_config</span></code> for the second worker’s process to pick up:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf_config</span><span class="p">[</span><span class="s1">&#39;task&#39;</span><span class="p">][</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now launch the second worker. This will start the training since all the workers are active (so there’s no need to background this process):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
python main.py &gt; /dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
</pre></div>
</div>
</div>
<p>Now if you recheck the logs written by the first worker you’ll see that it participated in training that model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
cat job_0.log
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Delete the `TF_CONFIG`, and kill any background tasks so they don&#39;t affect the next section.</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="o">%</span><span class="k">killbgscripts</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Multi-worker-training-in-depth">
<h2>Multi worker training in depth<a class="headerlink" href="#Multi-worker-training-in-depth" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial has demonstrated a <code class="docutils literal notranslate"><span class="pre">Custom</span> <span class="pre">Training</span> <span class="pre">Loop</span></code> workflow of the multi-worker setup. A detailed description of other topics is available in the <code class="docutils literal notranslate"><span class="pre">`model.fit's</span> <span class="pre">guide</span></code> &lt;<a class="reference external" href="https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb">https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb</a>&gt;`__ of the multi-worker setup and applicable to CTLs.</p>
</div>
<div class="section" id="See-also">
<h2>See also<a class="headerlink" href="#See-also" title="Enlazar permanentemente con este título">¶</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">Distributed Training in TensorFlow</a> guide provides an overview of the available distribution strategies.</p></li>
<li><p><a class="reference external" href="https://github.com/tensorflow/models/tree/master/official">Official models</a>, many of which can be configured to run multiple distribution strategies.</p></li>
<li><p>The <a class="reference internal" href="../../guide/function.html"><span class="doc">Performance section</span></a> in the guide provides information about other strategies and <a class="reference internal" href="../../guide/profiler.html"><span class="doc">tools</span></a> you can use to optimize the performance of your TensorFlow models.</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>