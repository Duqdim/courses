

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Multi-worker training with Keras &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Multi-worker training with Keras</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Distributed_training/1-03_multi_worker_with_keras.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Multi-worker-training-with-Keras">
<h1>Multi-worker training with Keras<a class="headerlink" href="#Multi-worker-training-with-Keras" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;a target=&quot;_blank&quot; href=&quot;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/images/tf_logo_32px.png&quot; /&gt;View on TensorFlow.org&lt;/a&gt;
</pre></div>
</div>
</td><td><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/images/colab_logo_32px.png&quot; /&gt;Run in Google Colab&lt;/a&gt;
</pre></div>
</div>
</td><td><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/images/GitHub-Mark-32px.png&quot; /&gt;View source on GitHub&lt;/a&gt;
</pre></div>
</div>
</td><td><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;a href=&quot;https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/distribute/multi_worker_with_keras.ipynb&quot;&gt;&lt;img src=&quot;https://www.tensorflow.org/images/download_logo_32px.png&quot; /&gt;Download notebook&lt;/a&gt;
</pre></div>
</div>
</td></table><div class="section" id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial demonstrates multi-worker distributed training with Keras model using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> API, specifically <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>. With the help of this strategy, a Keras model that was designed to run on single-worker can seamlessly work on multiple workers with minimal code change.</p>
<p><a class="reference internal" href="../../guide/distributed_training.html"><span class="doc">Distributed Training in TensorFlow</span></a> guide is available for an overview of the distribution strategies TensorFlow supports for those interested in a deeper understanding of <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> APIs.</p>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<p>First, some necessary imports.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
</pre></div>
</div>
</div>
<p>Before importing TensorFlow, make a few changes to the environment.</p>
<p>Disable all GPUs. This prevents errors caused by the workers all trying to use the same GPU. For a real application each worker would be on a different machine.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;-1&quot;</span>
</pre></div>
</div>
</div>
<p>Reset the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable, you’ll see more about this later.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Be sure that the current directory is on python’s path. This allows the notebook to import the files written by <code class="docutils literal notranslate"><span class="pre">%%writefile</span></code> later.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">if</span> <span class="s1">&#39;.&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
  <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now import TensorFlow.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
<div class="section" id="Dataset-and-model-definition">
<h3>Dataset and model definition<a class="headerlink" href="#Dataset-and-model-definition" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next create an <code class="docutils literal notranslate"><span class="pre">mnist.py</span></code> file with a simple model and dataset setup. This python file will be used by the worker-processes in this tutorial:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%writefile</span> mnist.py

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">mnist_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
  <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
  <span class="c1"># The `x` arrays are in uint8 and have values in the range [0, 255].</span>
  <span class="c1"># You need to convert them to float32 with values in the range [0, 1]</span>
  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span>
  <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
      <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">60000</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_dataset</span>

<span class="k">def</span> <span class="nf">build_and_compile_cnn_model</span><span class="p">():</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">target_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
  <span class="p">])</span>
  <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
      <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
<p>Try training the model for a small number of epochs and observe the results of a single worker to make sure everything works correctly. As training progresses, the loss should drop and the accuracy should increase.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">mnist</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">single_worker_dataset</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">mnist_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">single_worker_model</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">build_and_compile_cnn_model</span><span class="p">()</span>
<span class="n">single_worker_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">single_worker_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">70</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Multi-worker-Configuration">
<h2>Multi-worker Configuration<a class="headerlink" href="#Multi-worker-Configuration" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Now let’s enter the world of multi-worker training. In TensorFlow, the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable is required for training on multiple machines, each of which possibly has a different role. <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is a JSON string used to specify the cluster configuration on each worker that is part of the cluster.</p>
<p>Here is an example configuration:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;cluster&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;worker&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;localhost:12345&#39;</span><span class="p">,</span> <span class="s1">&#39;localhost:23456&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;task&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>Here is the same <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> serialized as a JSON string:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>There are two components of <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code>: <code class="docutils literal notranslate"><span class="pre">cluster</span></code> and <code class="docutils literal notranslate"><span class="pre">task</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cluster</span></code> is the same for all workers and provides information about the training cluster, which is a dict consisting of different types of jobs such as <code class="docutils literal notranslate"><span class="pre">worker</span></code>. In multi-worker training with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>, there is usually one <code class="docutils literal notranslate"><span class="pre">worker</span></code> that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular <code class="docutils literal notranslate"><span class="pre">worker</span></code> does. Such a worker is referred to as the <code class="docutils literal notranslate"><span class="pre">chief</span></code> worker, and it is customary that
the <code class="docutils literal notranslate"><span class="pre">worker</span></code> with <code class="docutils literal notranslate"><span class="pre">index</span></code> 0 is appointed as the chief <code class="docutils literal notranslate"><span class="pre">worker</span></code> (in fact this is how <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is implemented).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task</span></code> provides information of the current task and is different on each worker. It specifies the <code class="docutils literal notranslate"><span class="pre">type</span></code> and <code class="docutils literal notranslate"><span class="pre">index</span></code> of that worker.</p></li>
</ul>
<p>In this example, you set the task <code class="docutils literal notranslate"><span class="pre">type</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;worker&quot;</span></code> and the task <code class="docutils literal notranslate"><span class="pre">index</span></code> to <code class="docutils literal notranslate"><span class="pre">0</span></code>. This machine is the first worker and will be appointed as the chief worker and do more work than the others. Note that other machines will need to have the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable set as well, and it should have the same <code class="docutils literal notranslate"><span class="pre">cluster</span></code> dict, but different task <code class="docutils literal notranslate"><span class="pre">type</span></code> or task <code class="docutils literal notranslate"><span class="pre">index</span></code> depending on what the roles of those machines are.</p>
<p>For illustration purposes, this tutorial shows how one may set a <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> with 2 workers on <code class="docutils literal notranslate"><span class="pre">localhost</span></code>. In practice, users would create multiple workers on external IP addresses/ports, and set <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> on each worker appropriately.</p>
<p>In this example you will use 2 workers, the first worker’s <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is shown above. For the second worker you would set <code class="docutils literal notranslate"><span class="pre">tf_config['task']['index']=1</span></code></p>
<p>Above, <code class="docutils literal notranslate"><span class="pre">tf_config</span></code> is just a local variable in python. To actually use it to configure training, this dictionary needs to be serialized as JSON, and placed in the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable.</p>
<div class="section" id="Environment-variables-and-subprocesses-in-notebooks">
<h3>Environment variables and subprocesses in notebooks<a class="headerlink" href="#Environment-variables-and-subprocesses-in-notebooks" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Subprocesses inherit environment variables from their parent. So if you set an environment variable in this <code class="docutils literal notranslate"><span class="pre">jupyter</span> <span class="pre">notebook</span></code> process:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;GREETINGS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Hello TensorFlow!&#39;</span>
</pre></div>
</div>
</div>
<p>You can access the environment variable from a subprocesses:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
<span class="nb">echo</span> <span class="si">${</span><span class="nv">GREETINGS</span><span class="si">}</span>
</pre></div>
</div>
</div>
<p>In the next section, you’ll use this to pass the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> to the worker subprocesses. You would never really launch your jobs this way, but it’s sufficient for the purposes of this tutorial: To demonstrate a minimal multi-worker example.</p>
</div>
</div>
<div class="section" id="Choose-the-right-strategy">
<h2>Choose the right strategy<a class="headerlink" href="#Choose-the-right-strategy" title="Enlazar permanentemente con este título">¶</a></h2>
<p>In TensorFlow there are two main forms of distributed training:</p>
<ul class="simple">
<li><p>Synchronous training, where the steps of training are synced across the workers and replicas, and</p></li>
<li><p>Asynchronous training, where the training steps are not strictly synced.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>, which is the recommended strategy for synchronous multi-worker training, will be demonstrated in this guide. To train the model, use an instance of <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> creates copies of all variables in the model’s layers on each device across all workers. It uses <code class="docutils literal notranslate"><span class="pre">CollectiveOps</span></code>, a TensorFlow op for collective communication, to aggregate gradients and keep the variables in sync. The <code class="docutils literal notranslate"><span class="pre">`tf.distribute.Strategy</span></code> guide &lt;../../guide/distributed_training.ipynb&gt;`__ has more details about this strategy.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Note: <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is parsed and TensorFlow’s GRPC servers are started at the time <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy()</span></code> is called, so the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> environment variable must be set before a <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> instance is created. Since <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> is not set yet the above strategy is effectively single-worker training.</p>
<p><code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> provides multiple implementations via the <code class="docutils literal notranslate"><span class="pre">`CommunicationOptions</span></code> &lt;<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions">https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions</a>&gt;`__ parameter. <code class="docutils literal notranslate"><span class="pre">RING</span></code> implements ring-based collectives using gRPC as the cross-host communication layer. <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> uses <a class="reference external" href="https://developer.nvidia.com/nccl">Nvidia’s NCCL</a> to implement collectives. <code class="docutils literal notranslate"><span class="pre">AUTO</span></code> defers the choice to the runtime. The best choice of collective implementation depends upon the
number and kind of GPUs, and the network interconnect in the cluster.</p>
</div>
<div class="section" id="Train-the-model">
<h2>Train the model<a class="headerlink" href="#Train-the-model" title="Enlazar permanentemente con este título">¶</a></h2>
<p>With the integration of <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> API into <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code>, the only change you will make to distribute the training to multiple-workers is enclosing the model building and <code class="docutils literal notranslate"><span class="pre">model.compile()</span></code> call inside <code class="docutils literal notranslate"><span class="pre">strategy.scope()</span></code>. The distribution strategy’s scope dictates how and where the variables are created, and in the case of <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>, the variables created are <code class="docutils literal notranslate"><span class="pre">MirroredVariable</span></code>s, and they are replicated on each of the workers.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="c1"># Model building/compiling need to be within `strategy.scope()`.</span>
  <span class="n">multi_worker_model</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">build_and_compile_cnn_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Note: Currently there is a limitation in <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> where TensorFlow ops need to be created after the instance of strategy is created. If you see <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Collective</span> <span class="pre">ops</span> <span class="pre">must</span> <span class="pre">be</span> <span class="pre">configured</span> <span class="pre">at</span> <span class="pre">program</span> <span class="pre">startup</span></code>, try creating the instance of <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> at the beginning of the program and put the code that may create ops after the strategy is instantiated.</p>
<p>To actually run with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> you’ll need to run worker processes and pass a <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> to them.</p>
<p>Like the <code class="docutils literal notranslate"><span class="pre">mnist.py</span></code> file written earlier, here is the <code class="docutils literal notranslate"><span class="pre">main.py</span></code> that each of the workers will run:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%writefile</span> main.py

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">mnist</span>

<span class="n">per_worker_batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">tf_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">])</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf_config</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">][</span><span class="s1">&#39;worker&#39;</span><span class="p">])</span>

<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>

<span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">per_worker_batch_size</span> <span class="o">*</span> <span class="n">num_workers</span>
<span class="n">multi_worker_dataset</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">mnist_dataset</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="c1"># Model building/compiling need to be within `strategy.scope()`.</span>
  <span class="n">multi_worker_model</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">build_and_compile_cnn_model</span><span class="p">()</span>


<span class="n">multi_worker_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">multi_worker_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">70</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In the code snippet above note that the <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code>, which gets passed to <code class="docutils literal notranslate"><span class="pre">Dataset.batch</span></code>, is set to <code class="docutils literal notranslate"><span class="pre">per_worker_batch_size</span> <span class="pre">*</span> <span class="pre">num_workers</span></code>. This ensures that each worker processes batches of <code class="docutils literal notranslate"><span class="pre">per_worker_batch_size</span></code> examples regardless of the number of workers.</p>
<p>The current directory now contains both Python files:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
ls *.py
</pre></div>
</div>
</div>
<p>So json-serialize the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> and add it to the environment variables:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, you can launch a worker process that will run the <code class="docutils literal notranslate"><span class="pre">main.py</span></code> and use the <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># first kill any previous runs</span>
<span class="o">%</span><span class="k">killbgscripts</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash --bg
python main.py <span class="p">&amp;</span>&gt; job_0.log
</pre></div>
</div>
</div>
<p>There are a few things to note about the above command:</p>
<ol class="arabic simple">
<li><p>It uses the <code class="docutils literal notranslate"><span class="pre">%%bash</span></code> which is a <a class="reference external" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">notebook «magic»</a> to run some bash commands.</p></li>
<li><p>It uses the <code class="docutils literal notranslate"><span class="pre">--bg</span></code> flag to run the <code class="docutils literal notranslate"><span class="pre">bash</span></code> process in the background, because this worker will not terminate. It waits for all the workers before it starts.</p></li>
</ol>
<p>The backgrounded worker process won’t print output to this notebook, so the <code class="docutils literal notranslate"><span class="pre">&amp;&gt;</span></code> redirects its output to a file, so you can see what happened.</p>
<p>So, wait a few seconds for the process to start up:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now look what’s been output to the worker’s logfile so far:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
cat job_0.log
</pre></div>
</div>
</div>
<p>The last line of the log file should say: <code class="docutils literal notranslate"><span class="pre">Started</span> <span class="pre">server</span> <span class="pre">with</span> <span class="pre">target:</span> <span class="pre">grpc://localhost:12345</span></code>. The first worker is now ready, and is waiting for all the other worker(s) to be ready to proceed.</p>
<p>So update the <code class="docutils literal notranslate"><span class="pre">tf_config</span></code> for the second worker’s process to pick up:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf_config</span><span class="p">[</span><span class="s1">&#39;task&#39;</span><span class="p">][</span><span class="s1">&#39;index&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now launch the second worker. This will start the training since all the workers are active (so there’s no need to background this process):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
python main.py
</pre></div>
</div>
</div>
<p>Now if you recheck the logs written by the first worker you’ll see that it participated in training that model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
cat job_0.log
</pre></div>
</div>
</div>
<p>Unsurprisingly this ran <em>slower</em> than the the test run at the beginning of this tutorial. Running multiple workers on a single machine only adds overhead. The goal here was not to improve the training time, but only to give an example of multi-worker training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Delete the `TF_CONFIG`, and kill any background tasks so they don&#39;t affect the next section.</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;TF_CONFIG&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="o">%</span><span class="k">killbgscripts</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Multi-worker-training-in-depth">
<h2>Multi worker training in depth<a class="headerlink" href="#Multi-worker-training-in-depth" title="Enlazar permanentemente con este título">¶</a></h2>
<p>So far this tutorial has demonstrated a basic multi-worker setup. The rest of this document looks in detail other factors which may be useful or important for real use cases.</p>
<div class="section" id="Dataset-sharding">
<h3>Dataset sharding<a class="headerlink" href="#Dataset-sharding" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In multi-worker training, dataset sharding is needed to ensure convergence and performance.</p>
<p>The example in the previous section relies on the default autosharding provided by the <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> API. You can control the sharding by setting the <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.AutoShardPolicy</span></code> of the <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.DistributeOptions</span></code>. To learn more about auto-sharding see the <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/input#sharding">Distributed input guide</a>.</p>
<p>Here is a quick example of how to turn OFF the auto sharding, so each replica processes every example (not recommended):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
<span class="n">options</span><span class="o">.</span><span class="n">experimental_distribute</span><span class="o">.</span><span class="n">auto_shard_policy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AutoShardPolicy</span><span class="o">.</span><span class="n">OFF</span>

<span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">multi_worker_dataset</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">mnist_dataset</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">dataset_no_auto_shard</span> <span class="o">=</span> <span class="n">multi_worker_dataset</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Evaluation">
<h3>Evaluation<a class="headerlink" href="#Evaluation" title="Enlazar permanentemente con este título">¶</a></h3>
<p>If you pass <code class="docutils literal notranslate"><span class="pre">validation_data</span></code> into <code class="docutils literal notranslate"><span class="pre">model.fit</span></code>, it will alternate between training and evaluation for each epoch. The evaluation taking <code class="docutils literal notranslate"><span class="pre">validation_data</span></code> is distributed across the same set of workers and the evaluation results are aggregated and available for all workers. Similar to training, the validation dataset is automatically sharded at the file level. You need to set a global batch size in the validation dataset and set <code class="docutils literal notranslate"><span class="pre">validation_steps</span></code>. A repeated dataset is also recommended
for evaluation.</p>
<p>Alternatively, you can also create another task that periodically reads checkpoints and runs the evaluation. This is what Estimator does. But this is not a recommended way to perform evaluation and thus its details are omitted.</p>
</div>
<div class="section" id="Performance">
<h3>Performance<a class="headerlink" href="#Performance" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You now have a Keras model that is all set up to run in multiple workers with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>. You can try the following techniques to tweak performance of multi-worker training with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> provides multiple <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation">collective communication implementations</a>. <code class="docutils literal notranslate"><span class="pre">RING</span></code> implements ring-based collectives using gRPC as the cross-host communication layer. <code class="docutils literal notranslate"><span class="pre">NCCL</span></code> uses <a class="reference external" href="https://developer.nvidia.com/nccl">Nvidia’s NCCL</a> to implement collectives. <code class="docutils literal notranslate"><span class="pre">AUTO</span></code> defers the choice to the runtime. The best choice of collective implementation depends upon the number
and kind of GPUs, and the network interconnect in the cluster. To override the automatic choice, specify <code class="docutils literal notranslate"><span class="pre">communication_options</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>’s constructor, e.g. <code class="docutils literal notranslate"><span class="pre">communication_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CollectiveCommunication.NCCL)</span></code>.</p></li>
<li><p>Cast the variables to <code class="docutils literal notranslate"><span class="pre">tf.float</span></code> if possible. The official ResNet model includes <a class="reference external" href="https://github.com/tensorflow/models/blob/8367cf6dabe11adf7628541706b660821f397dce/official/resnet/resnet_model.py#L466">an example</a> of how this can be done.</p></li>
</ul>
</div>
<div class="section" id="Fault-tolerance">
<h3>Fault tolerance<a class="headerlink" href="#Fault-tolerance" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In synchronous training, the cluster would fail if one of the workers fails and no failure-recovery mechanism exists. Using Keras with <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> comes with the advantage of fault tolerance in cases where workers die or are otherwise unstable. You do this by preserving training state in the distributed file system of your choice, such that upon restart of the instance that previously failed or preempted, the training state is recovered.</p>
<p>When a worker becomes unavailable, other workers will fail (possibly after a timeout). In such cases, the unavailable worker needs to be restarted, as well as other workers that have failed.</p>
<p>Note: Previously, the <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callback provided a mechanism to restore training state upon restart from job failure for multi-worker training. The TensorFlow team are introducing a new <code class="docutils literal notranslate"><span class="pre">`BackupAndRestore</span></code> &lt;#scrollTo=kmH8uCUhfn4w&gt;`__ callback, to also add the support to single worker training for a consistent experience, and removed fault tolerance functionality from existing <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callback. From now on, applications that rely on this behavior should migrate to the new
callback.</p>
<div class="section" id="ModelCheckpoint-callback">
<h4>ModelCheckpoint callback<a class="headerlink" href="#ModelCheckpoint-callback" title="Enlazar permanentemente con este título">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callback no longer provides fault tolerance functionality, please use <code class="docutils literal notranslate"><span class="pre">`BackupAndRestore</span></code> &lt;#scrollTo=kmH8uCUhfn4w&gt;`__ callback instead.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callback can still be used to save checkpoints. But with this, if training was interrupted or successfully finished, in order to continue training from the checkpoint, the user is responsible to load the model manually.</p>
<p>Optionally the user can choose to save and restore model/weights outside <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callback.</p>
</div>
</div>
<div class="section" id="Model-saving-and-loading">
<h3>Model saving and loading<a class="headerlink" href="#Model-saving-and-loading" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To save your model using <code class="docutils literal notranslate"><span class="pre">model.save</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.saved_model.save</span></code>, the destination for saving needs to be different for each worker. On the non-chief workers, you will need to save the model to a temporary directory, and on the chief, you will need to save to the provided model directory. The temporary directories on the worker need to be unique to prevent errors resulting from multiple workers trying to write to the same location. The model saved in all the directories are identical and
typically only the model saved by the chief should be referenced for restoring or serving. You should have some cleanup logic that deletes the temporary directories created by the workers once your training has completed.</p>
<p>The reason you need to save on the chief and workers at the same time is because you might be aggregating variables during checkpointing which requires both the chief and workers to participate in the allreduce communication protocol. On the other hand, letting chief and workers save to the same model directory will result in errors due to contention.</p>
<p>With <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>, the program is run on every worker, and in order to know whether the current worker is chief, it takes advantage of the cluster resolver object that has attributes <code class="docutils literal notranslate"><span class="pre">task_type</span></code> and <code class="docutils literal notranslate"><span class="pre">task_id</span></code>. <code class="docutils literal notranslate"><span class="pre">task_type</span></code> tells you what the current job is (e.g. “worker”), and <code class="docutils literal notranslate"><span class="pre">task_id</span></code> tells you the identifier of the worker. The worker with id 0 is designated as the chief worker.</p>
<p>In the code snippet below, <code class="docutils literal notranslate"><span class="pre">write_filepath</span></code> provides the file path to write, which depends on the worker id. In the case of chief (worker with id 0), it writes to the original file path; for others, it creates a temporary directory (with id in the directory path) to write in:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;/tmp/keras-model&#39;</span>

<span class="k">def</span> <span class="nf">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="c1"># Note: there are two possible `TF_CONFIG` configuration.</span>
  <span class="c1">#   1) In addition to `worker` tasks, a `chief` task type is use;</span>
  <span class="c1">#      in this case, this function should be modified to</span>
  <span class="c1">#      `return task_type == &#39;chief&#39;`.</span>
  <span class="c1">#   2) Only `worker` task type is used; in this case, worker 0 is</span>
  <span class="c1">#      regarded as the chief. The implementation demonstrated here</span>
  <span class="c1">#      is for this case.</span>
  <span class="c1"># For the purpose of this colab section, we also add `task_type is None`</span>
  <span class="c1"># case because it is effectively run with only single worker.</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;worker&#39;</span> <span class="ow">and</span> <span class="n">task_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">task_type</span> <span class="ow">is</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">_get_temp_dir</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">base_dirpath</span> <span class="o">=</span> <span class="s1">&#39;workertemp_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span>
  <span class="n">temp_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">base_dirpath</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">temp_dir</span>

<span class="k">def</span> <span class="nf">write_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">dirpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="n">base</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
    <span class="n">dirpath</span> <span class="o">=</span> <span class="n">_get_temp_dir</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">base</span><span class="p">)</span>

<span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">task_type</span><span class="p">,</span>
                      <span class="n">strategy</span><span class="o">.</span><span class="n">cluster_resolver</span><span class="o">.</span><span class="n">task_id</span><span class="p">)</span>
<span class="n">write_model_path</span> <span class="o">=</span> <span class="n">write_filepath</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>With that, you’re now ready to save:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">multi_worker_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">write_model_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>As described above, later on the model should only be loaded from the path chief saved to, so let’s remove the temporary ones the non-chief workers saved:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">write_model_path</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Now, when it’s time to load, let’s use convenient <code class="docutils literal notranslate"><span class="pre">tf.keras.models.load_model</span></code> API, and continue with further work. Here, assume only using single worker to load and continue training, in which case you do not call <code class="docutils literal notranslate"><span class="pre">tf.keras.models.load_model</span></code> within another <code class="docutils literal notranslate"><span class="pre">strategy.scope()</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="c1"># Now that the model is restored, and can continue with the training.</span>
<span class="n">loaded_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">single_worker_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Checkpoint-saving-and-restoring">
<h3>Checkpoint saving and restoring<a class="headerlink" href="#Checkpoint-saving-and-restoring" title="Enlazar permanentemente con este título">¶</a></h3>
<p>On the other hand, checkpointing allows you to save model’s weights and restore them without having to save the whole model. Here, you’ll create one <code class="docutils literal notranslate"><span class="pre">tf.train.Checkpoint</span></code> that tracks the model, which is managed by a <code class="docutils literal notranslate"><span class="pre">tf.train.CheckpointManager</span></code> so that only the latest checkpoint is preserved.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="s1">&#39;/tmp/ckpt&#39;</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">multi_worker_model</span><span class="p">)</span>
<span class="n">write_checkpoint_dir</span> <span class="o">=</span> <span class="n">write_filepath</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">)</span>
<span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="n">write_checkpoint_dir</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Once the <code class="docutils literal notranslate"><span class="pre">CheckpointManager</span></code> is set up, you’re now ready to save, and remove the checkpoints non-chief workers saved.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">_is_chief</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="n">task_id</span><span class="p">):</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">write_checkpoint_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, when you need to restore, you can find the latest checkpoint saved using the convenient <code class="docutils literal notranslate"><span class="pre">tf.train.latest_checkpoint</span></code> function. After restoring the checkpoint, you can continue with training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">latest_checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
<span class="n">checkpoint</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">latest_checkpoint</span><span class="p">)</span>
<span class="n">multi_worker_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">multi_worker_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="BackupAndRestore-callback">
<h4>BackupAndRestore callback<a class="headerlink" href="#BackupAndRestore-callback" title="Enlazar permanentemente con este título">¶</a></h4>
<p>BackupAndRestore callback provides fault tolerance functionality, by backing up the model and current epoch number in a temporary checkpoint file under <code class="docutils literal notranslate"><span class="pre">backup_dir</span></code> argument to <code class="docutils literal notranslate"><span class="pre">BackupAndRestore</span></code>. This is done at the end of each epoch.</p>
<p>Once jobs get interrupted and restart, the callback restores the last checkpoint, and training continues from the beginning of the interrupted epoch. Any partial training already done in the unfinished epoch before interruption will be thrown away, so that it doesn’t affect the final model state.</p>
<p>To use it, provide an instance of <code class="docutils literal notranslate"><span class="pre">tf.keras.callbacks.experimental.BackupAndRestore</span></code> at the <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> call.</p>
<p>With MultiWorkerMirroredStrategy, if a worker gets interrupted, the whole cluster pauses until the interrupted worker is restarted. Other workers will also restart, and the interrupted worker rejoins the cluster. Then, every worker reads the checkpoint file that was previously saved and picks up its former state, thereby allowing the cluster to get back in sync. Then the training continues.</p>
<p><code class="docutils literal notranslate"><span class="pre">BackupAndRestore</span></code> callback uses <code class="docutils literal notranslate"><span class="pre">CheckpointManager</span></code> to save and restore the training state, which generates a file called checkpoint that tracks existing checkpoints together with the latest one. For this reason, <code class="docutils literal notranslate"><span class="pre">backup_dir</span></code> should not be re-used to store other checkpoints in order to avoid name collision.</p>
<p>Currently, <code class="docutils literal notranslate"><span class="pre">BackupAndRestore</span></code> callback supports single worker with no strategy, MirroredStrategy, and multi-worker with MultiWorkerMirroredStrategy. Below are two examples for both multi-worker training and single worker training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Multi-worker training with MultiWorkerMirroredStrategy.</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">BackupAndRestore</span><span class="p">(</span><span class="n">backup_dir</span><span class="o">=</span><span class="s1">&#39;/tmp/backup&#39;</span><span class="p">)]</span>
<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
  <span class="n">multi_worker_model</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">build_and_compile_cnn_model</span><span class="p">()</span>
<span class="n">multi_worker_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">multi_worker_dataset</span><span class="p">,</span>
                       <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                       <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span>
                       <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>If you inspect the directory of <code class="docutils literal notranslate"><span class="pre">backup_dir</span></code> you specified in <code class="docutils literal notranslate"><span class="pre">BackupAndRestore</span></code>, you may notice some temporarily generated checkpoint files. Those files are needed for recovering the previously lost instances, and they will be removed by the library at the end of <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> upon successful exiting of your training.</p>
<p>Note: Currently BackupAndRestore only supports eager mode. In graph mode, consider using <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#model_saving_and_loading">Save/Restore Model</a> mentioned above, and by providing <code class="docutils literal notranslate"><span class="pre">initial_epoch</span></code> in <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code>.</p>
</div>
</div>
</div>
<div class="section" id="See-also">
<h2>See also<a class="headerlink" href="#See-also" title="Enlazar permanentemente con este título">¶</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">Distributed Training in TensorFlow</a> guide provides an overview of the available distribution strategies.</p></li>
<li><p><a class="reference external" href="https://github.com/tensorflow/models/tree/master/official">Official models</a>, many of which can be configured to run multiple distribution strategies.</p></li>
<li><p>The <a class="reference internal" href="../../guide/function.html"><span class="doc">Performance section</span></a> in the guide provides information about other strategies and <a class="reference internal" href="../../guide/profiler.html"><span class="doc">tools</span></a> you can use to optimize the performance of your TensorFlow models.</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Construido con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>