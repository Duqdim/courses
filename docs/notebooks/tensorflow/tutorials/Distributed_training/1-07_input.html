

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Distributed Input &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Distributed Input</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Distributed_training/1-07_input.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Distributed-Input">
<h1>Distributed Input<a class="headerlink" href="#Distributed-Input" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>3b6d13edb2db40abb36a3650a0d0637b|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>ccef85f3a0e24bbebc12995ac38e31a7|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>d431ee1cee0d45779e7a39f98312841b|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>0e7f7e1ca34742a38c1043571ac2e22f|Download notebook</p>
</td></table><p>The <a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">tf.distribute</a> APIs provide an easy way for users to scale their training from a single machine to multiple machines. When scaling their model, users also have to distribute their input across multiple devices. <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> provides APIs using which you can automatically distribute your input across devices.</p>
<p>This guide will show you the different ways in which you can create distributed dataset and iterators using <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> APIs. Additionally, the following topics will be covered: - Usage, sharding and batching options when using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.distribute_datasets_from_function</span></code>. - Different ways in which you can iterate over the distributed dataset. - Differences between
<code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code>/<code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.distribute_datasets_from_function</span></code> APIs and <code class="docutils literal notranslate"><span class="pre">tf.data</span></code> APIs as well any limitations that users may come across in their usage.</p>
<p>This guide does not cover usage of distributed input with Keras APIs.</p>
<div class="section" id="Distributed-Datasets">
<h2>Distributed Datasets<a class="headerlink" href="#Distributed-Datasets" title="Enlazar permanentemente con este título">¶</a></h2>
<p>To use <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> APIs to scale, it is recommended that users use <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> to represent their input. <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> has been made to work efficiently with <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> (for example, automatic prefetch of data onto each accelerator device) with performance optimizations being regularly incorporated into the implementation. If you have a use case for using something other than <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>, please refer a later <a class="reference external" href="%22tensorinputs%22">section</a> in this guide. In a
non distributed training loop, users first create a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance and then iterate over the elements. For example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Helper libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="c1"># Create a tf.data.Dataset object.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
  <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>
  <span class="k">return</span> <span class="n">labels</span> <span class="o">-</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">features</span>

<span class="c1"># Iterate over the dataset using the for..in construct.</span>
<span class="k">for</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

</pre></div>
</div>
</div>
<p>To allow users to use <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> strategy with minimal changes to a user’s existing code, two APIs were introduced which would distribute a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance and return a distributed dataset object. A user could then iterate over this distributed dataset instance and train their model as before. Let us now look at the two APIs - <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.distribute_datasets_from_function</span></code> in more detail:</p>
<div class="section" id="tf.distribute.Strategy.experimental_distribute_dataset">
<h3><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code><a class="headerlink" href="#tf.distribute.Strategy.experimental_distribute_dataset" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="section" id="Usage">
<h4>Usage<a class="headerlink" href="#Usage" title="Enlazar permanentemente con este título">¶</a></h4>
<p>This API takes a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance as input and returns a <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code> instance. You should batch the input dataset with a value that is equal to the global batch size. This global batch size is the number of samples that you want to process across all devices in 1 step. You can iterate over this distributed dataset in a Pythonic fashion or create an iterator using <code class="docutils literal notranslate"><span class="pre">iter</span></code>. The returned object is not a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance and does not support any
other APIs that transform or inspect the dataset in any way. This is the recommended API if you don’t have specific ways in which you want to shard your input over different replicas.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>
<span class="c1"># Distribute input using the `experimental_distribute_dataset`.</span>
<span class="n">dist_dataset</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="c1"># 1 global batch of data fed to the model in 1 step.</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dist_dataset</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Properties">
<h4>Properties<a class="headerlink" href="#Properties" title="Enlazar permanentemente con este título">¶</a></h4>
<div class="section" id="Batching">
<h5>Batching<a class="headerlink" href="#Batching" title="Enlazar permanentemente con este título">¶</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> rebatches the input <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance with a new batch size that is equal to the global batch size divided by the number of replicas in sync. The number of replicas in sync is equal to the number of devices that are taking part in the gradient allreduce during training. When a user calls <code class="docutils literal notranslate"><span class="pre">next</span></code> on the distributed iterator, a per replica batch size of data is returned on each replica. The rebatched dataset cardinality will always be a multiple of the number of
replicas. Here are a couple of examples: * <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.range(6).batch(4,</span> <span class="pre">drop_remainder=False)</span></code> * Without distribution: * Batch 1: [0, 1, 2, 3] * Batch 2: [4, 5] * With distribution over 2 replicas. The last batch ([4, 5]) is split between 2 replicas.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>* Batch 1:
  * Replica 1:[0, 1]
  * Replica 2:[2, 3]
* Batch 2:
  * Replica 2: [4]
  * Replica 2: [5]
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.range(4).batch(4)</span></code></p>
<ul>
<li><p>Without distribution:</p>
<ul>
<li><p>Batch 1: [[0], [1], [2], [3]]</p></li>
</ul>
</li>
<li><p>With distribution over 5 replicas:</p>
<ul>
<li><p>Batch 1:</p>
<ul>
<li><p>Replica 1: [0]</p></li>
<li><p>Replica 2: [1]</p></li>
<li><p>Replica 3: [2]</p></li>
<li><p>Replica 4: [3]</p></li>
<li><p>Replica 5: []</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.range(8).batch(4)</span></code></p>
<ul>
<li><p>Without distribution:</p>
<ul>
<li><p>Batch 1: [0, 1, 2, 3]</p></li>
<li><p>Batch 2: [4, 5, 6, 7]</p></li>
</ul>
</li>
<li><p>With distribution over 3 replicas:</p>
<ul>
<li><p>Batch 1:</p>
<ul>
<li><p>Replica 1: [0, 1]</p></li>
<li><p>Replica 2: [2, 3]</p></li>
<li><p>Replica 3: []</p></li>
</ul>
</li>
<li><p>Batch 2:</p>
<ul>
<li><p>Replica 1: [4, 5]</p></li>
<li><p>Replica 2: [6, 7]</p></li>
<li><p>Replica 3: []</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Note: The above examples only illustrate how a global batch is split on different replicas. It is not advisable to depend on the actual values that might end up on each replica as it can change depending on the implementation.</p>
<p>Rebatching the dataset has a space complexity that increases linearly with the number of replicas. This means that for the multi worker training use case the input pipeline can run into OOM errors.</p>
</div>
<div class="section" id="Sharding">
<h5>Sharding<a class="headerlink" href="#Sharding" title="Enlazar permanentemente con este título">¶</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> also autoshards the input dataset in multi worker training with <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> and <code class="docutils literal notranslate"><span class="pre">TPUStrategy</span></code>. Each dataset is created on the CPU device of the worker. Autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.AutoShardPolicy</span></code> is set). This is to ensure that at each step, a global batch size of non overlapping dataset elements will be processed by each worker.
Autosharding has a couple of different options that can be specified using <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.DistributeOptions</span></code>. Note that there is no autosharding in multi worker training with <code class="docutils literal notranslate"><span class="pre">ParameterServerStrategy</span></code>, and more information on dataset creation with this strategy can be found in the <a class="reference external" href="parameter_server_training.ipynb">Parameter Server Strategy tutorial</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="n">options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
<span class="n">options</span><span class="o">.</span><span class="n">experimental_distribute</span><span class="o">.</span><span class="n">auto_shard_policy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AutoShardPolicy</span><span class="o">.</span><span class="n">DATA</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>There are three different options that you can set for the <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.AutoShardPolicy</span></code>:</p>
<ul class="simple">
<li><p>AUTO: This is the default option which means an attempt will be made to shard by FILE. The attempt to shard by FILE fails if a file-based dataset is not detected. <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> will then fall back to sharding by DATA. Note that if the input dataset is file-based but the number of files is less than the number of workers, an <code class="docutils literal notranslate"><span class="pre">InvalidArgumentError</span></code> will be raised. If this happens, explicitly set the policy to <code class="docutils literal notranslate"><span class="pre">AutoShardPolicy.DATA</span></code>, or split your input source into smaller files such
that number of files is greater than number of workers.</p></li>
<li><p>FILE: This is the option if you want to shard the input files over all the workers. You should use this option if the number of input files is much larger than the number of workers and the data in the files is evenly distributed. The downside of this option is having idle workers if the data in the files is not evenly distributed. If the number of files is less than the number of workers, an <code class="docutils literal notranslate"><span class="pre">InvalidArgumentError</span></code> will be raised. If this happens, explicitly set the policy to
<code class="docutils literal notranslate"><span class="pre">AutoShardPolicy.DATA</span></code>. For example, let us distribute 2 files over 2 workers with 1 replica each. File 1 contains [0, 1, 2, 3, 4, 5] and File 2 contains [6, 7, 8, 9, 10, 11]. Let the total number of replicas in sync be 2 and global batch size be 4.</p>
<ul>
<li><p>Worker 0:</p>
<ul>
<li><p>Batch 1 = Replica 1: [0, 1]</p></li>
<li><p>Batch 2 = Replica 1: [2, 3]</p></li>
<li><p>Batch 3 = Replica 1: [4]</p></li>
<li><p>Batch 4 = Replica 1: [5]</p></li>
</ul>
</li>
<li><p>Worker 1:</p>
<ul>
<li><p>Batch 1 = Replica 2: [6, 7]</p></li>
<li><p>Batch 2 = Replica 2: [8, 9]</p></li>
<li><p>Batch 3 = Replica 2: [10]</p></li>
<li><p>Batch 4 = Replica 2: [11]</p></li>
</ul>
</li>
</ul>
</li>
<li><p>DATA: This will autoshard the elements across all the workers. Each of the workers will read the entire dataset and only process the shard assigned to it. All other shards will be discarded. This is generally used if the number of input files is less than the number of workers and you want better sharding of data across all workers. The downside is that the entire dataset will be read on each worker. For example, let us distribute 1 files over 2 workers. File 1 contains [0, 1, 2, 3, 4, 5, 6,
7, 8, 9, 10, 11]. Let the total number of replicas in sync be 2.</p>
<ul>
<li><p>Worker 0:</p>
<ul>
<li><p>Batch 1 = Replica 1: [0, 1]</p></li>
<li><p>Batch 2 = Replica 1: [4, 5]</p></li>
<li><p>Batch 3 = Replica 1: [8, 9]</p></li>
</ul>
</li>
<li><p>Worker 1:</p>
<ul>
<li><p>Batch 1 = Replica 2: [2, 3]</p></li>
<li><p>Batch 2 = Replica 2: [6, 7]</p></li>
<li><p>Batch 3 = Replica 2: [10, 11]</p></li>
</ul>
</li>
</ul>
</li>
<li><p>OFF: If you turn off autosharding, each worker will process all the data. For example, let us distribute 1 files over 2 workers. File 1 contains [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Let the total number of replicas in sync be 2. Then each worker will see the following distribution:</p>
<ul>
<li><p>Worker 0:</p>
<ul>
<li><p>Batch 1 = Replica 1: [0, 1]</p></li>
<li><p>Batch 2 = Replica 1: [2, 3]</p></li>
<li><p>Batch 3 = Replica 1: [4, 5]</p></li>
<li><p>Batch 4 = Replica 1: [6, 7]</p></li>
<li><p>Batch 5 = Replica 1: [8, 9]</p></li>
<li><p>Batch 6 = Replica 1: [10, 11]</p></li>
</ul>
</li>
<li><p>Worker 1:</p>
<ul>
<li><p>Batch 1 = Replica 2: [0, 1]</p></li>
<li><p>Batch 2 = Replica 2: [2, 3]</p></li>
<li><p>Batch 3 = Replica 2: [4, 5]</p></li>
<li><p>Batch 4 = Replica 2: [6, 7]</p></li>
<li><p>Batch 5 = Replica 2: [8, 9]</p></li>
<li><p>Batch 6 = Replica 2: [10, 11]</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="Prefetching">
<h5>Prefetching<a class="headerlink" href="#Prefetching" title="Enlazar permanentemente con este título">¶</a></h5>
<p>By default, <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> adds a prefetch transformation at the end of the user provided <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance. The argument to the prefetch transformation which is <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code> is equal to the number of replicas in sync.</p>
</div>
</div>
</div>
<div class="section" id="tf.distribute.Strategy.distribute_datasets_from_function">
<h3><code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.distribute_datasets_from_function</span></code><a class="headerlink" href="#tf.distribute.Strategy.distribute_datasets_from_function" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="section" id="id9">
<h4>Usage<a class="headerlink" href="#id9" title="Enlazar permanentemente con este título">¶</a></h4>
<p>This API takes an input function and returns a <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code> instance. The input function that users pass in has a <code class="docutils literal notranslate"><span class="pre">tf.distribute.InputContext</span></code> argument and should return a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance. With this API, <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> does not make any further changes to the user’s <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance returned from the input function. It is the responsibility of the user to batch and shard the dataset. <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> calls the input function on the CPU
device of each of the workers. Apart from allowing users to specify their own batching and sharding logic, this API also demonstrates better scalability and performance compared to <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code> when used for multi worker training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">dataset_fn</span><span class="p">(</span><span class="n">input_context</span><span class="p">):</span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_context</span><span class="o">.</span><span class="n">get_per_replica_batch_size</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
    <span class="n">input_context</span><span class="o">.</span><span class="n">num_input_pipelines</span><span class="p">,</span> <span class="n">input_context</span><span class="o">.</span><span class="n">input_pipeline_id</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># This prefetches 2 batches per device.</span>
  <span class="k">return</span> <span class="n">dataset</span>

<span class="n">dist_dataset</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">distribute_datasets_from_function</span><span class="p">(</span><span class="n">dataset_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id10">
<h4>Properties<a class="headerlink" href="#id10" title="Enlazar permanentemente con este título">¶</a></h4>
<div class="section" id="id11">
<h5>Batching<a class="headerlink" href="#id11" title="Enlazar permanentemente con este título">¶</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance that is the return value of the input function should be batched using the per replica batch size. The per replica batch size is the global batch size divided by the number of replicas that are taking part in sync training. This is because <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> calls the input function on the CPU device of each of the workers. The dataset that is created on a given worker should be ready to use by all the replicas on that worker.</p>
</div>
<div class="section" id="id12">
<h5>Sharding<a class="headerlink" href="#id12" title="Enlazar permanentemente con este título">¶</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.distribute.InputContext</span></code> object that is implicitly passed as an argument to the user’s input function is created by <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> under the hood. It has information about the number of workers, current worker id etc. This input function can handle sharding as per policies set by the user using these properties that are part of the <code class="docutils literal notranslate"><span class="pre">tf.distribute.InputContext</span></code> object.</p>
</div>
<div class="section" id="id13">
<h5>Prefetching<a class="headerlink" href="#id13" title="Enlazar permanentemente con este título">¶</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> does not add a prefetch transformation at the end of the <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> returned by the user provided input function.</p>
<p>Note: Both <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.distribute_datasets_from_function</span></code> return <strong>``tf.distribute.DistributedDataset`` instances that are not of type ``tf.data.Dataset``</strong>. You can iterate over these instances (as shown in the Distributed Iterators section) and use the <code class="docutils literal notranslate"><span class="pre">element_spec</span></code> property.</p>
</div>
</div>
</div>
</div>
<div class="section" id="Distributed-Iterators">
<h2>Distributed Iterators<a class="headerlink" href="#Distributed-Iterators" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Similar to non-distributed <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instances, you will need to create an iterator on the <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code> instances to iterate over it and access the elements in the <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code>. The following are the ways in which you can create an <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator</span></code> and use it to train your model:</p>
<div class="section" id="Usages">
<h3>Usages<a class="headerlink" href="#Usages" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="section" id="Use-a-Pythonic-for-loop-construct">
<h4>Use a Pythonic for loop construct<a class="headerlink" href="#Use-a-Pythonic-for-loop-construct" title="Enlazar permanentemente con este título">¶</a></h4>
<p>You can use a user friendly Pythonic loop to iterate over the <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code>. The elements returned from the <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator</span></code> can be a single <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or a <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedValues</span></code> which contains a value per replica. Placing the loop inside a <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> will give a performance boost. However, <code class="docutils literal notranslate"><span class="pre">break</span></code> and <code class="docutils literal notranslate"><span class="pre">return</span></code> are currently not supported for a loop over a <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code> that is placed inside of a
<code class="docutils literal notranslate"><span class="pre">tf.function</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>
<span class="n">dist_dataset</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
  <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>
  <span class="k">return</span> <span class="n">labels</span> <span class="o">-</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">features</span>

<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dist_dataset</span><span class="p">:</span>
  <span class="c1"># train_step trains the model using the dataset elements</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss is &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Use-iter-to-create-an-explicit-iterator">
<h4>Use <code class="docutils literal notranslate"><span class="pre">iter</span></code> to create an explicit iterator<a class="headerlink" href="#Use-iter-to-create-an-explicit-iterator" title="Enlazar permanentemente con este título">¶</a></h4>
<p>To iterate over the elements in a <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedDataset</span></code> instance, you can create a <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator</span></code> using the <code class="docutils literal notranslate"><span class="pre">iter</span></code> API on it. With an explicit iterator, you can iterate for a fixed number of steps. In order to get the next element from an <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator</span></code> instance <code class="docutils literal notranslate"><span class="pre">dist_iterator</span></code>, you can call <code class="docutils literal notranslate"><span class="pre">next(dist_iterator)</span></code>, <code class="docutils literal notranslate"><span class="pre">dist_iterator.get_next()</span></code>, or <code class="docutils literal notranslate"><span class="pre">dist_iterator.get_next_as_optional()</span></code>. The former two are essentially the same:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">dist_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dist_dataset</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="p">):</span>
    <span class="c1"># train_step trains the model using the dataset elements</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">dist_iterator</span><span class="p">),))</span>
    <span class="c1"># which is the same as</span>
    <span class="c1"># loss = mirrored_strategy.run(train_step, args=(dist_iterator.get_next(),))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss is &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">next()</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator.get_next()</span></code>, if the <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator</span></code> has reached its end, an OutOfRange error will be thrown. The client can catch the error on python side and continue doing other work such as checkpointing and evaluation. However, this will not work if you are using a host training loop (i.e., run multiple steps per <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>), which looks like:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>@tf.function
def train_fn(iterator):
  for _ in tf.range(steps_per_loop):
    strategy.run(step_fn, args=(next(iterator),))
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train_fn</span></code> contains multiple steps by wrapping the step body inside a <code class="docutils literal notranslate"><span class="pre">tf.range</span></code>. In this case, different iterations in the loop with no dependency could start in parallel, so an OutOfRange error can be triggered in later iterations before the computation of previous iterations finishes. Once an OutOfRange error is thrown, all the ops in the function will be terminated right away. If this is some case that you would like to avoid, an alternative that does not throw an OutOfRange error is
<code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator.get_next_as_optional()</span></code>. <code class="docutils literal notranslate"><span class="pre">get_next_as_optional</span></code> returns a <code class="docutils literal notranslate"><span class="pre">tf.experimental.Optional</span></code> which contains the next element or no value if the <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedIterator</span></code> has reached to an end.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># You can break the loop with get_next_as_optional by checking if the Optional contains value</span>
<span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">steps_per_loop</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;GPU:0&quot;</span><span class="p">,</span> <span class="s2">&quot;CPU:0&quot;</span><span class="p">])</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>
<span class="n">distributed_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_fn</span><span class="p">(</span><span class="n">distributed_iterator</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">steps_per_loop</span><span class="p">):</span>
    <span class="n">optional_data</span> <span class="o">=</span> <span class="n">distributed_iterator</span><span class="o">.</span><span class="n">get_next_as_optional</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">optional_data</span><span class="o">.</span><span class="n">has_value</span><span class="p">():</span>
      <span class="k">break</span>
    <span class="n">per_replica_results</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">optional_data</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),))</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">experimental_local_results</span><span class="p">(</span><span class="n">per_replica_results</span><span class="p">))</span>
<span class="n">train_fn</span><span class="p">(</span><span class="n">distributed_iterator</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="Using-element_spec-property">
<h2>Using <code class="docutils literal notranslate"><span class="pre">element_spec</span></code> property<a class="headerlink" href="#Using-element_spec-property" title="Enlazar permanentemente con este título">¶</a></h2>
<p>If you pass the elements of a distributed dataset to a <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> and want a <code class="docutils literal notranslate"><span class="pre">tf.TypeSpec</span></code> guarantee, you can specify the <code class="docutils literal notranslate"><span class="pre">input_signature</span></code> argument of the <code class="docutils literal notranslate"><span class="pre">tf.function</span></code>. The output of a distributed dataset is <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedValues</span></code> which can represent the input to a single device or multiple devices. To get the <code class="docutils literal notranslate"><span class="pre">tf.TypeSpec</span></code> corresponding to this distributed value you can use the <code class="docutils literal notranslate"><span class="pre">element_spec</span></code> property of the distributed dataset or distributed iterator object.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">global_batch_size</span><span class="p">)</span>
<span class="n">dist_dataset</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="p">[</span><span class="n">dist_dataset</span><span class="o">.</span><span class="n">element_spec</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">per_replica_inputs</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">step_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">inputs</span>

  <span class="k">return</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">step_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">per_replica_inputs</span><span class="p">,))</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dist_dataset</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps_per_epoch</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">))</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Partial-Batches">
<h2>Partial Batches<a class="headerlink" href="#Partial-Batches" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Partial batches are encountered when <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instances that users create may contain batch sizes that are not evenly divisible by the number of replicas or when the cardinality of the dataset instance is not divisible by the batch size. This means that when the dataset is distributed over multiple replicas, the <code class="docutils literal notranslate"><span class="pre">next</span></code> call on some iterators will result in an OutOfRangeError. To handle this use case, <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> returns dummy batches of batch size 0 on replicas that do not
have any more data to process.</p>
<p>For the single worker case, if data is not returned by the <code class="docutils literal notranslate"><span class="pre">next</span></code> call on the iterator, dummy batches of 0 batch size are created and used along with the real data in the dataset. In the case of partial batches, the last global batch of data will contain real data alongside dummy batches of data. The stopping condition for processing data now checks if any of the replicas have data. If there is no data on any of the replicas, an OutOfRange error is thrown.</p>
<p>For the multi worker case, the boolean value representing presence of data on each of the workers is aggregated using cross replica communication and this is used to identify if all the workers have finished processing the distributed dataset. Since this involves cross worker communication there is some performance penalty involved.</p>
</div>
<div class="section" id="Caveats">
<h2>Caveats<a class="headerlink" href="#Caveats" title="Enlazar permanentemente con este título">¶</a></h2>
<ul class="simple">
<li><p>When using <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy.experimental_distribute_dataset</span></code> APIs with a multiple worker setup, users pass a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> that reads from files. If the <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.AutoShardPolicy</span></code> is set to <code class="docutils literal notranslate"><span class="pre">AUTO</span></code> or <code class="docutils literal notranslate"><span class="pre">FILE</span></code>, the actual per step batch size may be smaller than the user defined global batch size. This can happen when the remaining elements in the file are less than the global batch size. Users can either exhaust the dataset without depending on the number of
steps to run or set <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.AutoShardPolicy</span></code> to <code class="docutils literal notranslate"><span class="pre">DATA</span></code> to work around it.</p></li>
<li><p>Stateful dataset transformations are currently not supported with <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> and any stateful ops that the dataset may have are currently ignored. For example, if your dataset has a <code class="docutils literal notranslate"><span class="pre">map_fn</span></code> that uses <code class="docutils literal notranslate"><span class="pre">tf.random.uniform</span></code> to rotate an image, then you have a dataset graph that depends on state (i.e the random seed) on the local machine where the python process is being executed.</p></li>
<li><p>Experimental <code class="docutils literal notranslate"><span class="pre">tf.data.experimental.OptimizationOptions</span></code> that are disabled by default can in certain contexts – such as when used together with <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> – cause a performance degradation. You should only enable them after you validate that they benefit the performance of your workload in a distribute setting.</p></li>
<li><p>Please refer to <a class="reference external" href="https://www.tensorflow.org/guide/data_performance">this guide</a> for how to optimize your input pipeline with <code class="docutils literal notranslate"><span class="pre">tf.data</span></code> in general. A few additional tips:</p></li>
<li><p>If you have multiple workers and are using <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.list_files</span></code> to create a dataset from all files matching one or more glob patterns, remember to set the <code class="docutils literal notranslate"><span class="pre">seed</span></code> argument or set <code class="docutils literal notranslate"><span class="pre">shuffle=False</span></code> so that each worker shard the file consistently.</p></li>
<li><p>If your input pipeline includes both shuffling the data on record level and parsing the data, unless the unparsed data is significantly larger than the parsed data (which is usually not the case), shuffle first and then parse, as shown in the following example. This may benefit memory usage and performance.</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>d = tf.data.Dataset.list_files(pattern, shuffle=False)
d = d.shard(num_workers, worker_index)
d = d.repeat(num_epochs)
d = d.shuffle(shuffle_buffer_size)
d = d.interleave(tf.data.TFRecordDataset,
                 cycle_length=num_readers, block_length=1)
d = d.map(parser_fn, num_parallel_calls=num_map_threads)
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.shuffle(buffer_size,</span> <span class="pre">seed=None,</span> <span class="pre">reshuffle_each_iteration=None)</span></code> maintain an internal buffer of <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code> elements, and thus reducing <code class="docutils literal notranslate"><span class="pre">buffer_size</span></code> could aleviate OOM issue.</p></li>
</ul>
<ul class="simple">
<li><p>The order in which the data is processed by the workers when using <code class="docutils literal notranslate"><span class="pre">tf.distribute.experimental_distribute_dataset</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.distribute.distribute_datasets_from_function</span></code> is not guaranteed. This is typically required if you are using <code class="docutils literal notranslate"><span class="pre">tf.distribute</span></code> to scale prediction. You can however insert an index for each element in the batch and order outputs accordingly. The following snippet is an example of how to order outputs.</p></li>
</ul>
<p>Note: <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy()</span></code> is used here for the sake of convenience. We only need to reorder inputs when we are using multiple workers and <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> is used to distribute training on a single worker.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
<span class="n">dataset_size</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">)</span><span class="o">.</span><span class="n">enumerate</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">dist_dataset</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">inputs</span>
  <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="n">outputs</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="n">dist_dataset</span><span class="p">:</span>
  <span class="n">output_index</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_local_results</span><span class="p">(</span><span class="n">output_index</span><span class="p">))</span>
  <span class="n">rindices</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
    <span class="n">rindices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_local_results</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>
  <span class="n">routputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">routputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">rindices</span><span class="p">,</span> <span class="n">routputs</span><span class="p">):</span>
    <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<blockquote>
<div><p>## How do I distribute my data if I am not using a canonical tf.data.Dataset instance?</p>
</div></blockquote>
<p>Sometimes users cannot use a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> to represent their input and subsequently the above mentioned APIs to distribute the dataset to multiple devices. In such cases you can use raw tensors or inputs from a generator.</p>
<div class="section" id="Use-experimental_distribute_values_from_function-for-arbitrary-tensor-inputs">
<h3>Use experimental_distribute_values_from_function for arbitrary tensor inputs<a class="headerlink" href="#Use-experimental_distribute_values_from_function-for-arbitrary-tensor-inputs" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">strategy.run</span></code> accepts <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedValues</span></code> which is the output of <code class="docutils literal notranslate"><span class="pre">next(iterator)</span></code>. To pass the tensor values, use <code class="docutils literal notranslate"><span class="pre">experimental_distribute_values_from_function</span></code> to construct <code class="docutils literal notranslate"><span class="pre">tf.distribute.DistributedValues</span></code> from raw tensors.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
<span class="n">worker_devices</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">extended</span><span class="o">.</span><span class="n">worker_devices</span>

<span class="k">def</span> <span class="nf">value_fn</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">distributed_values</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_distribute_values_from_function</span><span class="p">(</span><span class="n">value_fn</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">distributed_values</span><span class="p">,))</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Use-tf.data.Dataset.from_generator-if-your-input-is-from-a-generator">
<h3>Use tf.data.Dataset.from_generator if your input is from a generator<a class="headerlink" href="#Use-tf.data.Dataset.from_generator-if-your-input-is-from-a-generator" title="Enlazar permanentemente con este título">¶</a></h3>
<p>If you have a generator function that you want to use, you can create a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> instance using the <code class="docutils literal notranslate"><span class="pre">from_generator</span></code> API.</p>
<p>Note: This is currently not supported for <code class="docutils literal notranslate"><span class="pre">tf.distribute.TPUStrategy</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mirrored_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">input_gen</span><span class="p">():</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># use Dataset.from_generator</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
    <span class="n">input_gen</span><span class="p">,</span> <span class="n">output_types</span><span class="o">=</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">output_shapes</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="mi">4</span><span class="p">]))</span>
<span class="n">dist_dataset</span> <span class="o">=</span> <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">experimental_distribute_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dist_dataset</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
  <span class="n">mirrored_strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">),))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>