

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Playing CartPole with the Actor-Critic Method &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" />
    <link rel="next" title="Actor Critic Method" href="../../keras.io/rl/0-00_actor_critic_cartpole.html" />
    <link rel="prev" title="Tokenizing with TF Text" href="../Load_and_process_data/1-10_tokenizers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/content.html">Sesiones</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/course-info.html">Información del curso</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/programming-labs.html">Laboratorios de programación</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/complement.html">Material Complementario</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a> &raquo;</li>
        
          <li><a href="../../../../redes-neuronales-con-tensorflow/content.html">Sesiones</a> &raquo;</li>
        
      <li>Playing CartPole with the Actor-Critic Method</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Reinforcement_learning/1-01_actor_critic.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Playing-CartPole-with-the-Actor-Critic-Method">
<h1>Playing CartPole with the Actor-Critic Method<a class="headerlink" href="#Playing-CartPole-with-the-Actor-Critic-Method" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left">  <td>

|710c4aff3a0147bebe3cd19984d8814d| View on TensorFlow.org</td>  <td>

|6080a8a29d6047849d74ca1b4cffdf8b| Run in Google Colab</td>  <td>

|da1d52d4d70844539a3d575db3abf34e| View source on GitHub</td><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>034af594ba454853a5c78c023757dad0|Download notebook</p>
</td></table><p>This tutorial demonstrates how to implement the <a class="reference external" href="https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf">Actor-Critic</a> method using TensorFlow to train an agent on the <a class="reference external" href="https://gym.openai.com/">Open AI Gym</a> CartPole-V0 environment. The reader is assumed to have some familiarity with <a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">policy gradient methods</a> of reinforcement learning.</p>
<p><strong>Actor-Critic methods</strong></p>
<p>Actor-Critic methods are <a class="reference external" href="https://en.wikipedia.org/wiki/Temporal_difference_learning">temporal difference (TD) learning</a> methods that represent the policy function independent of the value function.</p>
<p>A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state. A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.</p>
<p>In the Actor-Critic method, the policy is referred to as the <em>actor</em> that proposes a set of possible actions given a state, and the estimated value function is referred to as the <em>critic</em>, which evaluates actions taken by the <em>actor</em> based on the given policy.</p>
<p>In this tutorial, both the <em>Actor</em> and <em>Critic</em> will be represented using one neural network with two outputs.</p>
<p><strong>CartPole-v0</strong></p>
<p>In the <a class="reference external" href="https://gym.openai.com/envs/CartPole-v0">CartPole-v0 environment</a>, a pole is attached to a cart moving along a frictionless track. The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. A reward of +1 is given for every time step the pole remains upright. An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.</p>
<center><figure><figcaption><p>Trained actor-critic model in Cartpole-v0 environment</p>
</figcaption></figure></center><p>The problem is considered “solved” when the average total reward for the episode reaches 195 over 100 consecutive trials.</p>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Import necessary packages and configure global settings.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>pip install gym
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-bash notranslate"><div class="highlight"><pre>
<span></span>%%bash
<span class="c1"># Install additional packages for visualization</span>
sudo apt-get install -y xvfb python-opengl &gt; /dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
pip install pyvirtualdisplay &gt; /dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
pip install git+https://github.com/tensorflow/docs &gt; /dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statistics</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span>


<span class="c1"># Create the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">)</span>

<span class="c1"># Set seed for experiment reproducibility</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Small epsilon value for stabilizing division operations</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Model">
<h2>Model<a class="headerlink" href="#Model" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The <em>Actor</em> and <em>Critic</em> will be modeled using one neural network that generates the action probabilities and critic value respectively. This tutorial uses model subclassing to define the model.</p>
<p>During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value <span class="math notranslate nohighlight">\(V\)</span>, which models the state-dependent <a class="reference external" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions">value function</a>. The goal is to train a model that chooses actions based on a policy <span class="math notranslate nohighlight">\(\pi\)</span> that maximizes expected <a class="reference external" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return">return</a>.</p>
<p>For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.</p>
<p>Refer to <a class="reference external" href="http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf">OpenAI Gym’s CartPole-v0 wiki page</a> for more information.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Combined actor-critic network.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">num_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
      <span class="n">num_hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">common</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_hidden_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_actions</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">common</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>  <span class="c1"># 2</span>
<span class="n">num_hidden_units</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">num_hidden_units</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Enlazar permanentemente con este título">¶</a></h2>
<p>To train the agent, you will follow these steps:</p>
<ol class="arabic simple">
<li><p>Run the agent on the environment to collect training data per episode.</p></li>
<li><p>Compute expected return at each time step.</p></li>
<li><p>Compute the loss for the combined actor-critic model.</p></li>
<li><p>Compute gradients and update network parameters.</p></li>
<li><p>Repeat 1-4 until either success criterion or max episodes has been reached.</p></li>
</ol>
<div class="section" id="1.-Collecting-training-data">
<h3>1. Collecting training data<a class="headerlink" href="#1.-Collecting-training-data" title="Enlazar permanentemente con este título">¶</a></h3>
<p>As in supervised learning, in order to train the actor-critic model, you need to have training data. However, in order to collect such data, the model would need to be “run” in the environment.</p>
<p>Training data is collected for each episode. Then at each time step, the model’s forward pass will be run on the environment’s state in order to generate action probabilities and the critic value based on the current policy parameterized by the model’s weights.</p>
<p>The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated.</p>
<p>This process is implemented in the <code class="docutils literal notranslate"><span class="pre">run_episode</span></code> function, which uses TensorFlow operations so that it can later be compiled into a TensorFlow graph for faster training. Note that <code class="docutils literal notranslate"><span class="pre">tf.TensorArray</span></code>s were used to support Tensor iteration on variable length arrays.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Wrap OpenAI Gym&#39;s `env.step` call as an operation in a TensorFlow function.</span>
<span class="c1"># This would allow it to be included in a callable TensorFlow graph.</span>

<span class="k">def</span> <span class="nf">env_step</span><span class="p">(</span><span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Returns state, reward and done flag given an action.&quot;&quot;&quot;</span>

  <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
          <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
          <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">tf_env_step</span><span class="p">(</span><span class="n">action</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">numpy_function</span><span class="p">(</span><span class="n">env_step</span><span class="p">,</span> <span class="p">[</span><span class="n">action</span><span class="p">],</span>
                           <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span>
    <span class="n">initial_state</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Runs a single episode to collect training data.&quot;&quot;&quot;</span>

  <span class="n">action_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_size</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="n">initial_state_shape</span> <span class="o">=</span> <span class="n">initial_state</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">initial_state</span>

  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
    <span class="c1"># Convert state into a batched tensor (batch size = 1)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Run the model and to get action probabilities and critic value</span>
    <span class="n">action_logits_t</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="c1"># Sample next action from the action probability distribution</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">action_logits_t</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">action_probs_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">action_logits_t</span><span class="p">)</span>

    <span class="c1"># Store critic values</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

    <span class="c1"># Store log probability of the action chosen</span>
    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">action_probs</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">action_probs_t</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>

    <span class="c1"># Apply action to the environment to get next state and reward</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">tf_env_step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">initial_state_shape</span><span class="p">)</span>

    <span class="c1"># Store reward</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">):</span>
      <span class="k">break</span>

  <span class="n">action_probs</span> <span class="o">=</span> <span class="n">action_probs</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">action_probs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rewards</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="2.-Computing-expected-returns">
<h3>2. Computing expected returns<a class="headerlink" href="#2.-Computing-expected-returns" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The sequence of rewards for each timestep <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\{r_{t}\}^{T}_{t=1}\)</span> collected during one episode is converted into a sequence of expected returns <span class="math notranslate nohighlight">\(\{G_{t}\}^{T}_{t=1}\)</span> in which the sum of rewards is taken from the current timestep <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(T\)</span> and each reward is multiplied with an exponentially decaying discount factor <span class="math notranslate nohighlight">\(\gamma\)</span>:</p>
<div class="math notranslate nohighlight">
\[G_{t} = \sum^{T}_{t'=t} \gamma^{t'-t}r_{t'}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\gamma\in(0,1)\)</span>, rewards further out from the current timestep are given less weight.</p>
<p>Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.</p>
<p>To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">get_expected_return</span><span class="p">(</span>
    <span class="n">rewards</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">standardize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Compute expected returns per timestep.&quot;&quot;&quot;</span>

  <span class="n">n</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">rewards</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">returns</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorArray</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

  <span class="c1"># Start from the end of `rewards` and accumulate reward sums</span>
  <span class="c1"># into the `returns` array</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">discounted_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
  <span class="n">discounted_sum_shape</span> <span class="o">=</span> <span class="n">discounted_sum</span><span class="o">.</span><span class="n">shape</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">discounted_sum</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">discounted_sum</span>
    <span class="n">discounted_sum</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">discounted_sum_shape</span><span class="p">)</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="n">returns</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">discounted_sum</span><span class="p">)</span>
  <span class="n">returns</span> <span class="o">=</span> <span class="n">returns</span><span class="o">.</span><span class="n">stack</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">standardize</span><span class="p">:</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">((</span><span class="n">returns</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">returns</span><span class="p">))</span> <span class="o">/</span>
               <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">returns</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="3.-The-actor-critic-loss">
<h3>3. The actor-critic loss<a class="headerlink" href="#3.-The-actor-critic-loss" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:</p>
<div class="math notranslate nohighlight">
\[L = L_{actor} + L_{critic}\]</div>
<div class="section" id="Actor-loss">
<h4>Actor loss<a class="headerlink" href="#Actor-loss" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The actor loss is based on <a class="reference external" href="https://www.youtube.com/watch?v=EKqxumCuAAY&amp;t=62m23s">policy gradients with the critic as a state dependent baseline</a> and computed with single-sample (per-episode) estimates.</p>
<div class="math notranslate nohighlight">
\[L_{actor} = -\sum^{T}_{t=1} log\pi_{\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\pi}_{\theta}(s_{t})]\]</div>
<p>where: - <span class="math notranslate nohighlight">\(T\)</span>: the number of timesteps per episode, which can vary per episode - <span class="math notranslate nohighlight">\(s_{t}\)</span>: the state at timestep <span class="math notranslate nohighlight">\(t\)</span> - <span class="math notranslate nohighlight">\(a_{t}\)</span>: chosen action at timestep <span class="math notranslate nohighlight">\(t\)</span> given state <span class="math notranslate nohighlight">\(s\)</span> - <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span>: is the policy (actor) parameterized by <span class="math notranslate nohighlight">\(\theta\)</span> - <span class="math notranslate nohighlight">\(V^{\pi}_{\theta}\)</span>: is the value function (critic) also parameterized by <span class="math notranslate nohighlight">\(\theta\)</span> - <span class="math notranslate nohighlight">\(G = G_{t}\)</span>: the expected return for a given state, action pair at timestep <span class="math notranslate nohighlight">\(t\)</span></p>
<p>A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.</p>
<div class="section" id="Advantage">
<h5>Advantage<a class="headerlink" href="#Advantage" title="Enlazar permanentemente con este título">¶</a></h5>
<p>The <span class="math notranslate nohighlight">\(G - V\)</span> term in our <span class="math notranslate nohighlight">\(L_{actor}\)</span> formulation is called the <a class="reference external" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions">advantage</a>, which indicates how much better an action is given a particular state over a random action selected according to the policy <span class="math notranslate nohighlight">\(\pi\)</span> for that state.</p>
<p>While it’s possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic <span class="math notranslate nohighlight">\(V\)</span> as a baseline is that it trained to be as close as possible to <span class="math notranslate nohighlight">\(G\)</span>, leading to a lower variance.</p>
<p>In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.</p>
<p>For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective <span class="math notranslate nohighlight">\(J\)</span>. With the critic, it may turn out that there’s no advantage (<span class="math notranslate nohighlight">\(G - V = 0\)</span>) and thus no benefit gained in increasing the actions’ probabilities and the algorithm would set the gradients to zero.</p>
</div>
</div>
<div class="section" id="Critic-loss">
<h4>Critic loss<a class="headerlink" href="#Critic-loss" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Training <span class="math notranslate nohighlight">\(V\)</span> to be as close possible to <span class="math notranslate nohighlight">\(G\)</span> can be set up as a regression problem with the following loss function:</p>
<div class="math notranslate nohighlight">
\[L_{critic} = L_{\delta}(G, V^{\pi}_{\theta})\]</div>
<p>where <span class="math notranslate nohighlight">\(L_{\delta}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a>, which is less sensitive to outliers in data than squared-error loss.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">huber_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Huber</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span>
    <span class="n">action_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">values</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">returns</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Computes the combined actor-critic loss.&quot;&quot;&quot;</span>

  <span class="n">advantage</span> <span class="o">=</span> <span class="n">returns</span> <span class="o">-</span> <span class="n">values</span>

  <span class="n">action_log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">action_probs</span><span class="p">)</span>
  <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">action_log_probs</span> <span class="o">*</span> <span class="n">advantage</span><span class="p">)</span>

  <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">huber_loss</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">critic_loss</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="4.-Defining-the-training-step-to-update-parameters">
<h3>4. Defining the training step to update parameters<a class="headerlink" href="#4.-Defining-the-training-step-to-update-parameters" title="Enlazar permanentemente con este título">¶</a></h3>
<p>All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> context to enable automatic differentiation.</p>
<p>This tutorial uses the Adam optimizer to apply the gradients to the model parameters.</p>
<p>The sum of the undiscounted rewards, <code class="docutils literal notranslate"><span class="pre">episode_reward</span></code>, is also computed in this step. This value will be used later on to evaluate if the success criterion is met.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.function</span></code> context is applied to the <code class="docutils literal notranslate"><span class="pre">train_step</span></code> function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>


<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span>
    <span class="n">initial_state</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Runs a model training step.&quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

    <span class="c1"># Run the model for one episode to collect training data</span>
    <span class="n">action_probs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span>
        <span class="n">initial_state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">max_steps_per_episode</span><span class="p">)</span>

    <span class="c1"># Calculate expected returns</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="n">get_expected_return</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

    <span class="c1"># Convert training data to appropriate TF tensor shapes</span>
    <span class="n">action_probs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">returns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">action_probs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">returns</span><span class="p">]]</span>

    <span class="c1"># Calculating loss values to update our network</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span>

  <span class="c1"># Compute the gradients from the loss</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

  <span class="c1"># Apply the gradients to the model&#39;s parameters</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

  <span class="n">episode_reward</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">episode_reward</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="5.-Run-the-training-loop">
<h3>5. Run the training loop<a class="headerlink" href="#5.-Run-the-training-loop" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.</p>
<p>A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency.</p>
<p>Depending on your runtime, training can finish in less than a minute.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>

<span class="n">min_episodes_criterion</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">max_episodes</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Cartpole-v0 is considered solved if average reward is &gt;= 195 over 100</span>
<span class="c1"># consecutive trials</span>
<span class="n">reward_threshold</span> <span class="o">=</span> <span class="mi">195</span>
<span class="n">running_reward</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Discount factor for future rewards</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="c1"># Keep last episodes reward</span>
<span class="n">episodes_reward</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">min_episodes_criterion</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">trange</span><span class="p">(</span><span class="n">max_episodes</span><span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">initial_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_step</span><span class="p">(</span>
        <span class="n">initial_state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">max_steps_per_episode</span><span class="p">))</span>

    <span class="n">episodes_reward</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_reward</span><span class="p">)</span>
    <span class="n">running_reward</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episodes_reward</span><span class="p">)</span>

    <span class="n">t</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Episode </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span>
        <span class="n">episode_reward</span><span class="o">=</span><span class="n">episode_reward</span><span class="p">,</span> <span class="n">running_reward</span><span class="o">=</span><span class="n">running_reward</span><span class="p">)</span>

    <span class="c1"># Show average episode reward every 10 episodes</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">pass</span> <span class="c1"># print(f&#39;Episode {i}: average reward: {avg_reward}&#39;)</span>

    <span class="k">if</span> <span class="n">running_reward</span> <span class="o">&gt;</span> <span class="n">reward_threshold</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">min_episodes_criterion</span><span class="p">:</span>
        <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Solved at episode </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">: average reward: </span><span class="si">{</span><span class="n">running_reward</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Visualization">
<h2>Visualization<a class="headerlink" href="#Visualization" title="Enlazar permanentemente con este título">¶</a></h2>
<p>After training, it would be good to visualize how the model performs in the environment. You can run the cells below to generate a GIF animation of one episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment’s images correctly in Colab.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Render an episode and save as a GIF file</span>

<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">ipythondisplay</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">pyvirtualdisplay</span> <span class="kn">import</span> <span class="n">Display</span>


<span class="n">display</span> <span class="o">=</span> <span class="n">Display</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">300</span><span class="p">))</span>
<span class="n">display</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">render_episode</span><span class="p">(</span><span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="n">screen</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">)</span>
  <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">screen</span><span class="p">)</span>

  <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">im</span><span class="p">]</span>

  <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">action_probs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">action_probs</span><span class="p">))</span>

    <span class="n">state</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Render screen every 10 steps</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">screen</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">)</span>
      <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">screen</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="k">return</span> <span class="n">images</span>


<span class="c1"># Save GIF image</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">render_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">max_steps_per_episode</span><span class="p">)</span>
<span class="n">image_file</span> <span class="o">=</span> <span class="s1">&#39;cartpole-v0.gif&#39;</span>
<span class="c1"># loop=0: loop forever, duration=1: play each frame for 1ms</span>
<span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
    <span class="n">image_file</span><span class="p">,</span> <span class="n">save_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">append_images</span><span class="o">=</span><span class="n">images</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">loop</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow_docs.vis.embed</span> <span class="k">as</span> <span class="nn">embed</span>
<span class="n">embed</span><span class="o">.</span><span class="n">embed_file</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Next-steps">
<h2>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial demonstrated how to implement the actor-critic method using Tensorflow.</p>
<p>As a next step, you could try training a model on a different environment in OpenAI Gym.</p>
<p>For additional information regarding actor-critic methods and the Cartpole-v0 problem, you may refer to the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://hal.inria.fr/hal-00840470/document">Actor Critic Method</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=EKqxumCuAAY&amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;index=7&amp;t=0s">Actor Critic Lecture (CAL)</a></p></li>
<li><p><a class="reference external" href="http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf">Cartpole learning control problem [Barto, et al. 1983]</a></p></li>
</ul>
<p>For more reinforcement learning examples in TensorFlow, you can check the following resources: - <a class="reference external" href="https://keras.io/examples/rl/">Reinforcement learning code examples (keras.io)</a> - <a class="reference external" href="https://www.tensorflow.org/agents">TF-Agents reinforcement learning library</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>