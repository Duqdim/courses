

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Subword tokenizers &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" />
    <link rel="next" title="TFRecord and tf.train.Example" href="1-09_tfrecord.html" />
    <link rel="prev" title="TF.Text" href="1-07_TF.Text.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de grandes datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-financiera/index.html">Analítica Financiera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/content.html">Sesiones</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/course-info.html">Información del curso</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/programming-labs.html">Laboratorios de programación</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes-neuronales-con-tensorflow/complement.html">Material Complementario</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../redes-neuronales-con-tensorflow/index.html">Redes Neuronales Artificiales</a> &raquo;</li>
        
          <li><a href="../../../../redes-neuronales-con-tensorflow/content.html">Sesiones</a> &raquo;</li>
        
      <li>Subword tokenizers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Load_and_process_data/1-08_subwords_tokenizer.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Subword-tokenizers">
<h1>Subword tokenizers<a class="headerlink" href="#Subword-tokenizers" title="Enlazar permanentemente con este título">¶</a></h1>
<p>This tutorial demonstrates how to generate a subword vocabulary from a dataset, and use it to build a <code class="docutils literal notranslate"><span class="pre">text.BertTokenizer</span></code> from the vocabulary.</p>
<p>The main advantage of a subword tokenizer is that it interpolates between word-based and character-based tokenization. Common words get a slot in the vocabulary, but the tokenizer can fall back to word pieces and individual characters for unknown words.</p>
<p>Objective: At the end of this tutorial you’ll have built a complete end-to-end wordpiece tokenizer and detokenizer from scratch, and saved it as a <code class="docutils literal notranslate"><span class="pre">saved_model</span></code> that you can load and use in this <a class="reference external" href="https://tensorflow.org/tutorials/text/transformer">translation tutorial</a>.</p>
<div class="section" id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">tensorflow_text</span></code> package includes TensorFlow implementations of many common tokenizers. This includes three subword-style tokenizers:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">text.BertTokenizer</span></code> - The <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> class is a higher level interface. It includes BERT’s token splitting algorithm and a <code class="docutils literal notranslate"><span class="pre">WordPieceTokenizer</span></code>. It takes <strong>sentences</strong> as input and returns <strong>token-IDs</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">text.WordpeiceTokenizer</span></code> - The <code class="docutils literal notranslate"><span class="pre">WordPieceTokenizer</span></code> class is a lower level interface. It only implements the <a class="reference external" href="#applying_wordpiece">WordPiece algorithm</a>. You must standardize and split the text into words before calling it. It takes <strong>words</strong> as input and returns token-IDs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">text.SentencepieceTokenizer</span></code> - The <code class="docutils literal notranslate"><span class="pre">SentencepieceTokenizer</span></code> requires a more complex setup. Its initializer requires a pre-trained sentencepiece model. See the <a class="reference external" href="https://github.com/google/sentencepiece#train-sentencepiece-model">google/sentencepiece repository</a> for instructions on how to build one of these models. It can accept <strong>sentences</strong> as input when tokenizing.</p></li>
</ul>
<p>This tutorial builds a Wordpiece vocabulary in a top down manner, starting from existing words. This process doesn’t work for Japanese, Chinese, or Korean since these languages don’t have clear multi-character units. To tokenize these languages conside using <code class="docutils literal notranslate"><span class="pre">text.SentencepieceTokenizer</span></code>, <code class="docutils literal notranslate"><span class="pre">text.UnicodeCharTokenizer</span></code> or <a class="reference external" href="https://tfhub.dev/google/zh_segmentation/1">this approach</a>.</p>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>pip install -q tensorflow_datasets
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 2.5.0.dev20210325 =&gt; avoid binary incompatibilities until 2.5 is out.</span>

<span class="c1"># `BertTokenizer.detokenize` is not in `tf-text` stable yet (currently 2.4.3).</span>
<span class="o">!</span>pip install -q <span class="nv">tensorflow_text_nightly</span><span class="o">==</span><span class="m">2</span>.5.0.dev20210325
<span class="c1"># tf-text-nightly resquires tf-nightly</span>
<span class="o">!</span>pip install -q tf-nightly<span class="o">==</span><span class="m">2</span>.5.0.dev20210325
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">tensorflow_text</span> <span class="k">as</span> <span class="nn">text</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">get_logger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s1">&#39;ERROR&#39;</span><span class="p">)</span>
<span class="n">pwd</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Download-the-dataset">
<h2>Download the dataset<a class="headerlink" href="#Download-the-dataset" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Fetch the Portuguese/English translation dataset from <a class="reference external" href="https://tensorflow.org/datasets">tfds</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">examples</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;ted_hrlr_translate/pt_to_en&#39;</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>This dataset produces Portuguese/English sentence pairs:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Portuguese: &quot;</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;English:   &quot;</span><span class="p">,</span> <span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Note a few things about the example sentences above: * They’re lower case. * There are spaces around the punctuation. * It’s not clear if or what unicode normalization is being used.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_en</span> <span class="o">=</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">:</span> <span class="n">en</span><span class="p">)</span>
<span class="n">train_pt</span> <span class="o">=</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">:</span> <span class="n">pt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generate-the-vocabulary">
<h2>Generate the vocabulary<a class="headerlink" href="#Generate-the-vocabulary" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This section generates a wordpiece vocabulary from a dataset. If you already have a vocabulary file and just want to see how to build a <code class="docutils literal notranslate"><span class="pre">text.BertTokenizer</span></code> or <code class="docutils literal notranslate"><span class="pre">text.Wordpiece</span></code> tokenizer with it then you can skip ahead to the <a class="reference external" href="#build_the_tokenizer">Build the tokenizer</a> section.</p>
<p>Note: The vocabulary generation code used in this tutorial is optimized for <strong>simplicity</strong>. If you need a more scalable solution consider using the Apache Beam implementation available in <a class="reference external" href="https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py">tools/wordpiece_vocab/generate_vocab.py</a></p>
<p>The vocabulary generation code is included in the <code class="docutils literal notranslate"><span class="pre">tensorflow_text</span></code> pip package. It is not imported by default , you need to manually import it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">tensorflow_text.tools.wordpiece_vocab</span> <span class="kn">import</span> <span class="n">bert_vocab_from_dataset</span> <span class="k">as</span> <span class="n">bert_vocab</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">bert_vocab.bert_vocab_from_dataset</span></code> function will generate the vocabulary.</p>
<p>There are many arguments you can set to adjust its behavior. For this tutorial, you’ll mostly use the defaults. If you want to learn more about the options, first read about <a class="reference external" href="#algorithm">the algorithm</a>, and then have a look at <a class="reference external" href="https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py">the code</a>.</p>
<p>This takes about 2 minutes.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">bert_tokenizer_params</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">reserved_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[START]&quot;</span><span class="p">,</span> <span class="s2">&quot;[END]&quot;</span><span class="p">]</span>

<span class="n">bert_vocab_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="c1"># The target vocabulary size</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">8000</span><span class="p">,</span>
    <span class="c1"># Reserved tokens that must be included in the vocabulary</span>
    <span class="n">reserved_tokens</span><span class="o">=</span><span class="n">reserved_tokens</span><span class="p">,</span>
    <span class="c1"># Arguments for `text.BertTokenizer`</span>
    <span class="n">bert_tokenizer_params</span><span class="o">=</span><span class="n">bert_tokenizer_params</span><span class="p">,</span>
    <span class="c1"># Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`</span>
    <span class="n">learn_params</span><span class="o">=</span><span class="p">{},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">pt_vocab</span> <span class="o">=</span> <span class="n">bert_vocab</span><span class="o">.</span><span class="n">bert_vocab_from_dataset</span><span class="p">(</span>
    <span class="n">train_pt</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="o">**</span><span class="n">bert_vocab_args</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Here are some slices of the resulting vocabulary.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">pt_vocab</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pt_vocab</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">110</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pt_vocab</span><span class="p">[</span><span class="mi">1000</span><span class="p">:</span><span class="mi">1010</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pt_vocab</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<p>Write a vocabulary file:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">write_vocab_file</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">write_vocab_file</span><span class="p">(</span><span class="s1">&#39;pt_vocab.txt&#39;</span><span class="p">,</span> <span class="n">pt_vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Use that function to generate a vocabulary from the english data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">en_vocab</span> <span class="o">=</span> <span class="n">bert_vocab</span><span class="o">.</span><span class="n">bert_vocab_from_dataset</span><span class="p">(</span>
    <span class="n">train_en</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="o">**</span><span class="n">bert_vocab_args</span>
<span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">en_vocab</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">en_vocab</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">110</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">en_vocab</span><span class="p">[</span><span class="mi">1000</span><span class="p">:</span><span class="mi">1010</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">en_vocab</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<p>Here are the two vocabulary files:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">write_vocab_file</span><span class="p">(</span><span class="s1">&#39;en_vocab.txt&#39;</span><span class="p">,</span> <span class="n">en_vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>ls *.txt
</pre></div>
</div>
</div>
</div>
<div class="section" id="Build-the-tokenizer">
<h2>Build the tokenizer<a class="headerlink" href="#Build-the-tokenizer" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">text.BertTokenizer</span></code> can be initialized by passing the vocabulary file’s path as the first argument (see the section on <a class="reference external" href="#tf.lookup">tf.lookup</a> for other options):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pt_tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="s1">&#39;pt_vocab.txt&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">bert_tokenizer_params</span><span class="p">)</span>
<span class="n">en_tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="s1">&#39;en_vocab.txt&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">bert_tokenizer_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now you can use it to encode some text. Take a batch of 3 examples from the english data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">pt_examples</span><span class="p">,</span> <span class="n">en_examples</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">en_examples</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Run it through the <code class="docutils literal notranslate"><span class="pre">BertTokenizer.tokenize</span></code> method. Initially, this returns a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> with axes <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">word,</span> <span class="pre">word-piece)</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Tokenize the examples -&gt; (batch, word, word-piece)</span>
<span class="n">token_batch</span> <span class="o">=</span> <span class="n">en_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">en_examples</span><span class="p">)</span>
<span class="c1"># Merge the word and word-piece axes -&gt; (batch, tokens)</span>
<span class="n">token_batch</span> <span class="o">=</span> <span class="n">token_batch</span><span class="o">.</span><span class="n">merge_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">token_batch</span><span class="o">.</span><span class="n">to_list</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>If you replace the token IDs with their text representations (using <code class="docutils literal notranslate"><span class="pre">tf.gather</span></code>) you can see that in the first example the words <code class="docutils literal notranslate"><span class="pre">&quot;searchability&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;serendipity&quot;</span></code> have been decomposed into <code class="docutils literal notranslate"><span class="pre">&quot;search</span> <span class="pre">##ability&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;s</span> <span class="pre">##ere</span> <span class="pre">##nd</span> <span class="pre">##ip</span> <span class="pre">##ity&quot;</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Lookup each token id in the vocabulary.</span>
<span class="n">txt_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">en_vocab</span><span class="p">,</span> <span class="n">token_batch</span><span class="p">)</span>
<span class="c1"># Join with spaces.</span>
<span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">reduce_join</span><span class="p">(</span><span class="n">txt_tokens</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To re-assemble words from the extracted tokens, use the <code class="docutils literal notranslate"><span class="pre">BertTokenizer.detokenize</span></code> method:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">words</span> <span class="o">=</span> <span class="n">en_tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">token_batch</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">reduce_join</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<blockquote>
<div><p>Note: <code class="docutils literal notranslate"><span class="pre">BertTokenizer.tokenize</span></code>/<code class="docutils literal notranslate"><span class="pre">BertTokenizer.detokenize</span></code> does not round trip losslessly. The result of <code class="docutils literal notranslate"><span class="pre">detokenize</span></code> will not, in general, have the same content or offsets as the input to <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>. This is because of the “basic tokenization” step, that splits the strings into words before applying the <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code>, includes irreversible steps like lower-casing and splitting on punctuation. <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code> on the other hand <strong>is</strong> reversible.</p>
</div></blockquote>
</div>
<div class="section" id="Customization-and-export">
<h2>Customization and export<a class="headerlink" href="#Customization-and-export" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial builds the text tokenizer and detokenizer used by the <a class="reference external" href="https://tensorflow.org/tutorials/text/transformer">Transformer</a> tutorial. This section adds methods and processing steps to simplify that tutorial, and exports the tokenizers using <code class="docutils literal notranslate"><span class="pre">tf.saved_model</span></code> so they can be imported by the other tutorials.</p>
<div class="section" id="Custom-tokenization">
<h3>Custom tokenization<a class="headerlink" href="#Custom-tokenization" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The downstream tutorials both expect the tokenized text to include <code class="docutils literal notranslate"><span class="pre">[START]</span></code> and <code class="docutils literal notranslate"><span class="pre">[END]</span></code> tokens.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">reserved_tokens</span></code> reserve space at the beginning of the vocabulary, so <code class="docutils literal notranslate"><span class="pre">[START]</span></code> and <code class="docutils literal notranslate"><span class="pre">[END]</span></code> have the same indexes for both languages:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">START</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">reserved_tokens</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;[START]&quot;</span><span class="p">)</span>
<span class="n">END</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">reserved_tokens</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;[END]&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">add_start_end</span><span class="p">(</span><span class="n">ragged</span><span class="p">):</span>
  <span class="n">count</span> <span class="o">=</span> <span class="n">ragged</span><span class="o">.</span><span class="n">bounding_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">starts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">count</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">START</span><span class="p">)</span>
  <span class="n">ends</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">count</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">END</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">starts</span><span class="p">,</span> <span class="n">ragged</span><span class="p">,</span> <span class="n">ends</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">words</span> <span class="o">=</span> <span class="n">en_tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">add_start_end</span><span class="p">(</span><span class="n">token_batch</span><span class="p">))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">reduce_join</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Custom-detokenization">
<h3>Custom detokenization<a class="headerlink" href="#Custom-detokenization" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Before exporting the tokenizers there are a couple of things you can cleanup for the downstream tutorials:</p>
<ol class="arabic simple">
<li><p>They want to generate clean text output, so drop reserved tokens like <code class="docutils literal notranslate"><span class="pre">[START]</span></code>, <code class="docutils literal notranslate"><span class="pre">[END]</span></code> and <code class="docutils literal notranslate"><span class="pre">[PAD]</span></code>.</p></li>
<li><p>They’re interested in complete strings, so apply a string join along the <code class="docutils literal notranslate"><span class="pre">words</span></code> axis of the result.</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">cleanup_text</span><span class="p">(</span><span class="n">reserved_tokens</span><span class="p">,</span> <span class="n">token_txt</span><span class="p">):</span>
  <span class="c1"># Drop the reserved tokens, except for &quot;[UNK]&quot;.</span>
  <span class="n">bad_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">reserved_tokens</span> <span class="k">if</span> <span class="n">tok</span> <span class="o">!=</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">]</span>
  <span class="n">bad_token_re</span> <span class="o">=</span> <span class="s2">&quot;|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">bad_tokens</span><span class="p">)</span>

  <span class="n">bad_cells</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">regex_full_match</span><span class="p">(</span><span class="n">token_txt</span><span class="p">,</span> <span class="n">bad_token_re</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">token_txt</span><span class="p">,</span> <span class="o">~</span><span class="n">bad_cells</span><span class="p">)</span>

  <span class="c1"># Join them into strings.</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">reduce_join</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">en_examples</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">token_batch</span> <span class="o">=</span> <span class="n">en_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">en_examples</span><span class="p">)</span><span class="o">.</span><span class="n">merge_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">en_tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">token_batch</span><span class="p">)</span>
<span class="n">words</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">cleanup_text</span><span class="p">(</span><span class="n">reserved_tokens</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Export">
<h3>Export<a class="headerlink" href="#Export" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The following code block builds a <code class="docutils literal notranslate"><span class="pre">CustomTokenizer</span></code> class to contain the <code class="docutils literal notranslate"><span class="pre">text.BertTokenizer</span></code> instances, the custom logic, and the <code class="docutils literal notranslate"><span class="pre">&#64;tf.function</span></code> wrappers required for export.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">CustomTokenizer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="n">lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reserved_tokens</span> <span class="o">=</span> <span class="n">reserved_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_path</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">Asset</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span><span class="o">.</span><span class="n">read_text</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1">## Create the signatures for export:</span>

    <span class="c1"># Include a tokenize signature for a batch of strings.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">))</span>

    <span class="c1"># Include `detokenize` and `lookup` signatures for:</span>
    <span class="c1">#   * `Tensors` with shapes [tokens] and [batch, tokens]</span>
    <span class="c1">#   * `RaggedTensors` with shape [batch, tokens]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">detokenize</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">detokenize</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>

    <span class="c1"># These `get_*` methods take no arguments</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_vocab_path</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_reserved_tokens</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strings</span><span class="p">):</span>
    <span class="n">enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">strings</span><span class="p">)</span>
    <span class="c1"># Merge the `word` and `word-piece` axes.</span>
    <span class="n">enc</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">merge_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">enc</span> <span class="o">=</span> <span class="n">add_start_end</span><span class="p">(</span><span class="n">enc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">enc</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenized</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">tokenized</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cleanup_text</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reserved_tokens</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">lookup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">)</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">get_vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">get_vocab_path</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_path</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">get_reserved_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reserved_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Build a <code class="docutils literal notranslate"><span class="pre">CustomTokenizer</span></code> for each language:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="n">tokenizers</span><span class="o">.</span><span class="n">pt</span> <span class="o">=</span> <span class="n">CustomTokenizer</span><span class="p">(</span><span class="n">reserved_tokens</span><span class="p">,</span> <span class="s1">&#39;pt_vocab.txt&#39;</span><span class="p">)</span>
<span class="n">tokenizers</span><span class="o">.</span><span class="n">en</span> <span class="o">=</span> <span class="n">CustomTokenizer</span><span class="p">(</span><span class="n">reserved_tokens</span><span class="p">,</span> <span class="s1">&#39;en_vocab.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Export the tokenizers as a <code class="docutils literal notranslate"><span class="pre">saved_model</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;ted_hrlr_translate_pt_en_converter&#39;</span>
<span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tokenizers</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Reload the <code class="docutils literal notranslate"><span class="pre">saved_model</span></code> and test the methods:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">reloaded_tokenizers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">reloaded_tokenizers</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">reloaded_tokenizers</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s1">&#39;Hello TensorFlow!&#39;</span><span class="p">])</span>
<span class="n">tokens</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">text_tokens</span> <span class="o">=</span> <span class="n">reloaded_tokenizers</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">text_tokens</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">round_trip</span> <span class="o">=</span> <span class="n">reloaded_tokenizers</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">round_trip</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Archive it for the <a class="reference external" href="https://tensorflow.org/tutorials/text/transformer">translation tutorials</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>zip -r <span class="o">{</span>model_name<span class="o">}</span>.zip <span class="o">{</span>model_name<span class="o">}</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>du -h *.zip
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Optional:-The-algorithm">
<h2>Optional: The algorithm<a class="headerlink" href="#Optional:-The-algorithm" title="Enlazar permanentemente con este título">¶</a></h2>
<p>It’s worth noting here that there are two versions of the WordPiece algorithm: Bottom-up and top-down. In both cases goal is the same: “Given a training corpus and a number of desired tokens D, the optimization problem is to select D wordpieces such that the resulting corpus is minimal in the number of wordpieces when segmented according to the chosen wordpiece model.”</p>
<p>The original <a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">bottom-up WordPiece algorithm</a>, is based on <a class="reference external" href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10">byte-pair encoding</a>. Like BPE, It starts with the alphabet, and iteratively combines common bigrams to form word-pieces and words.</p>
<p>TensorFlow Text’s vocabulary generator follows the top-down implementation from <a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>. Starting with words and breaking them down into smaller components until they hit the frequency threshold, or can’t be broken down further. The next section describes this in detail. For Japanese, Chinese and Korean this top-down approach doesn’t work since there are no explicit word units to start with. For those you need a <a class="reference external" href="https://tfhub.dev/google/zh_segmentation/1">different
approach</a>.</p>
<div class="section" id="Choosing-the-vocabulary">
<h3>Choosing the vocabulary<a class="headerlink" href="#Choosing-the-vocabulary" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The top-down WordPiece generation algorithm takes in a set of (word, count) pairs and a threshold <code class="docutils literal notranslate"><span class="pre">T</span></code>, and returns a vocabulary <code class="docutils literal notranslate"><span class="pre">V</span></code>.</p>
<p>The algorithm is iterative. It is run for <code class="docutils literal notranslate"><span class="pre">k</span></code> iterations, where typically <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">4</span></code>, but only the first two are really important. The third and fourth (and beyond) are just identical to the second. Note that each step of the binary search runs the algorithm from scratch for <code class="docutils literal notranslate"><span class="pre">k</span></code> iterations.</p>
<p>The iterations described below:</p>
<div class="section" id="First-iteration">
<h4>First iteration<a class="headerlink" href="#First-iteration" title="Enlazar permanentemente con este título">¶</a></h4>
<ol class="arabic simple">
<li><p>Iterate over every word and count pair in the input, denoted as <code class="docutils literal notranslate"><span class="pre">(w,</span> <span class="pre">c)</span></code>.</p></li>
<li><p>For each word <code class="docutils literal notranslate"><span class="pre">w</span></code>, generate every substring, denoted as <code class="docutils literal notranslate"><span class="pre">s</span></code>. E.g., for the word <code class="docutils literal notranslate"><span class="pre">human</span></code>, we generate <code class="docutils literal notranslate"><span class="pre">{h,</span> <span class="pre">hu,</span> <span class="pre">hum,</span> <span class="pre">huma,</span> <span class="pre">human,</span> <span class="pre">##u,</span> <span class="pre">##um,</span> <span class="pre">##uma,</span> <span class="pre">##uman,</span> <span class="pre">##m,</span> <span class="pre">##ma,</span> <span class="pre">##man,</span> <span class="pre">#a,</span> <span class="pre">##an,</span> <span class="pre">##n}</span></code>.</p></li>
<li><p>Maintain a substring-to-count hash map, and increment the count of each <code class="docutils literal notranslate"><span class="pre">s</span></code> by <code class="docutils literal notranslate"><span class="pre">c</span></code>. E.g., if we have <code class="docutils literal notranslate"><span class="pre">(human,</span> <span class="pre">113)</span></code> and <code class="docutils literal notranslate"><span class="pre">(humas,</span> <span class="pre">3)</span></code> in our input, the count of <code class="docutils literal notranslate"><span class="pre">s</span> <span class="pre">=</span> <span class="pre">huma</span></code> will be <code class="docutils literal notranslate"><span class="pre">113+3=116</span></code>.</p></li>
<li><p>Once we’ve collected the counts of every substring, iterate over the <code class="docutils literal notranslate"><span class="pre">(s,</span> <span class="pre">c)</span></code> pairs <em>starting with the longest ``s`` first</em>.</p></li>
<li><p>Keep any <code class="docutils literal notranslate"><span class="pre">s</span></code> that has a <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">&gt;</span> <span class="pre">T</span></code>. E.g., if <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">=</span> <span class="pre">100</span></code> and we have <code class="docutils literal notranslate"><span class="pre">(pers,</span> <span class="pre">231);</span> <span class="pre">(dogs,</span> <span class="pre">259);</span> <span class="pre">(##rint;</span> <span class="pre">76)</span></code>, then we would keep <code class="docutils literal notranslate"><span class="pre">pers</span></code> and <code class="docutils literal notranslate"><span class="pre">dogs</span></code>.</p></li>
<li><p>When an <code class="docutils literal notranslate"><span class="pre">s</span></code> is kept, subtract off its count from all of its prefixes. This is the reason for sorting all of the <code class="docutils literal notranslate"><span class="pre">s</span></code> by length in step 4. This is a critical part of the algorithm, because otherwise words would be double counted. For example, let’s say that we’ve kept <code class="docutils literal notranslate"><span class="pre">human</span></code> and we get to <code class="docutils literal notranslate"><span class="pre">(huma,</span> <span class="pre">116)</span></code>. We know that <code class="docutils literal notranslate"><span class="pre">113</span></code> of those <code class="docutils literal notranslate"><span class="pre">116</span></code> came from <code class="docutils literal notranslate"><span class="pre">human</span></code>, and <code class="docutils literal notranslate"><span class="pre">3</span></code> came from <code class="docutils literal notranslate"><span class="pre">humas</span></code>. However, now that <code class="docutils literal notranslate"><span class="pre">human</span></code> is in our vocabulary, we know we will never segment <code class="docutils literal notranslate"><span class="pre">human</span></code> into
<code class="docutils literal notranslate"><span class="pre">huma</span> <span class="pre">##n</span></code>. So once <code class="docutils literal notranslate"><span class="pre">human</span></code> has been kept, then <code class="docutils literal notranslate"><span class="pre">huma</span></code> only has an <em>effective</em> count of <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p></li>
</ol>
<p>This algorithm will generate a set of word pieces <code class="docutils literal notranslate"><span class="pre">s</span></code> (many of which will be whole words <code class="docutils literal notranslate"><span class="pre">w</span></code>), which we <em>could</em> use as our WordPiece vocabulary.</p>
<p>However, there is a problem: This algorithm will severely overgenerate word pieces. The reason is that we only subtract off counts of prefix tokens. Therefore, if we keep the word <code class="docutils literal notranslate"><span class="pre">human</span></code>, we will subtract off the count for <code class="docutils literal notranslate"><span class="pre">h,</span> <span class="pre">hu,</span> <span class="pre">hu,</span> <span class="pre">huma</span></code>, but not for <code class="docutils literal notranslate"><span class="pre">##u,</span> <span class="pre">##um,</span> <span class="pre">##uma,</span> <span class="pre">##uman</span></code> and so on. So we might generate both <code class="docutils literal notranslate"><span class="pre">human</span></code> and <code class="docutils literal notranslate"><span class="pre">##uman</span></code> as word pieces, even though <code class="docutils literal notranslate"><span class="pre">##uman</span></code> will never be applied.</p>
<p>So why not subtract off the counts for every <em>substring</em>, not just every <em>prefix</em>? Because then we could end up subtracting off the counts multiple times. Let’s say that we’re processing <code class="docutils literal notranslate"><span class="pre">s</span></code> of length 5 and we keep both <code class="docutils literal notranslate"><span class="pre">(##denia,</span> <span class="pre">129)</span></code> and <code class="docutils literal notranslate"><span class="pre">(##eniab,</span> <span class="pre">137)</span></code>, where <code class="docutils literal notranslate"><span class="pre">65</span></code> of those counts came from the word <code class="docutils literal notranslate"><span class="pre">undeniable</span></code>. If we subtract off from <em>every</em> substring, we would subtract <code class="docutils literal notranslate"><span class="pre">65</span></code> from the substring <code class="docutils literal notranslate"><span class="pre">##enia</span></code> twice, even though we should only subtract once. However, if we only
subtract off from prefixes, it will correctly only be subtracted once.</p>
</div>
<div class="section" id="Second-(and-third-…)-iteration">
<h4>Second (and third …) iteration<a class="headerlink" href="#Second-(and-third-…)-iteration" title="Enlazar permanentemente con este título">¶</a></h4>
<p>To solve the overgeneration issue mentioned above, we perform multiple iterations of the algorithm.</p>
<p>Subsequent iterations are identical to the first, with one important distinction: In step 2, instead of considering <em>every</em> substring, we apply the WordPiece tokenization algorithm using the vocabulary from the previous iteration, and only consider substrings which <em>start</em> on a split point.</p>
<p>For example, let’s say that we’re performing step 2 of the algorithm and encounter the word <code class="docutils literal notranslate"><span class="pre">undeniable</span></code>. In the first iteration, we would consider every substring, e.g., <code class="docutils literal notranslate"><span class="pre">{u,</span> <span class="pre">un,</span> <span class="pre">und,</span> <span class="pre">...,</span> <span class="pre">undeniable,</span> <span class="pre">##n,</span> <span class="pre">##nd,</span> <span class="pre">...,</span> <span class="pre">##ndeniable,</span> <span class="pre">...}</span></code>.</p>
<p>Now, for the second iteration, we will only consider a subset of these. Let’s say that after the first iteration, the relevant word pieces are:</p>
<p><code class="docutils literal notranslate"><span class="pre">un,</span> <span class="pre">##deni,</span> <span class="pre">##able,</span> <span class="pre">##ndeni,</span> <span class="pre">##iable</span></code></p>
<p>The WordPiece algorithm will segment this into <code class="docutils literal notranslate"><span class="pre">un</span> <span class="pre">##deni</span> <span class="pre">##able</span></code> (see the section <a class="reference external" href="#applying-wordpiece">Applying WordPiece</a> for more information). In this case, we will only consider substrings that <em>start</em> at a segmentation point. We will still consider every possible <em>end</em> position. So during the second iteration, the set of <code class="docutils literal notranslate"><span class="pre">s</span></code> for <code class="docutils literal notranslate"><span class="pre">undeniable</span></code> is:</p>
<p><code class="docutils literal notranslate"><span class="pre">{u,</span> <span class="pre">un,</span> <span class="pre">und,</span> <span class="pre">unden,</span> <span class="pre">undeni,</span> <span class="pre">undenia,</span> <span class="pre">undeniab,</span> <span class="pre">undeniabl,</span> <span class="pre">undeniable,</span> <span class="pre">##d,</span> <span class="pre">##de,</span> <span class="pre">##den,</span> <span class="pre">##deni,</span> <span class="pre">##denia,</span> <span class="pre">##deniab,</span> <span class="pre">##deniabl</span> <span class="pre">,</span> <span class="pre">##deniable,</span> <span class="pre">##a,</span> <span class="pre">##ab,</span> <span class="pre">##abl,</span> <span class="pre">##able}</span></code></p>
<p>The algorithm is otherwise identical. In this example, in the first iteration, the algorithm produces the suprious tokens <code class="docutils literal notranslate"><span class="pre">##ndeni</span></code> and <code class="docutils literal notranslate"><span class="pre">##iable</span></code>. Now, these tokens are never considered, so they will not be generated by the second iteration. We perform several iterations just to make sure the results converge (although there is no literal convergence guarantee).</p>
</div>
</div>
<div class="section" id="Applying-WordPiece">
<h3>Applying WordPiece<a class="headerlink" href="#Applying-WordPiece" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Once a WordPiece vocabulary has been generated, we need to be able to apply it to new data. The algorithm is a simple greedy longest-match-first application.</p>
<p>For example, consider segmenting the word <code class="docutils literal notranslate"><span class="pre">undeniable</span></code>.</p>
<p>We first lookup <code class="docutils literal notranslate"><span class="pre">undeniable</span></code> in our WordPiece dictionary, and if it’s present, we’re done. If not, we decrement the end point by one character, and repeat, e.g., <code class="docutils literal notranslate"><span class="pre">undeniabl</span></code>.</p>
<p>Eventually, we will either find a subtoken in our vocabulary, or get down to a single character subtoken. (In general, we assume that every character is in our vocabulary, although this might not be the case for rare Unicode characters. If we encounter a rare Unicode character that’s not in the vocabulary we simply map the entire word to <code class="docutils literal notranslate"><span class="pre">&lt;unk&gt;</span></code>).</p>
<p>In this case, we find <code class="docutils literal notranslate"><span class="pre">un</span></code> in our vocabulary. So that’s our first word piece. Then we jump to the end of <code class="docutils literal notranslate"><span class="pre">un</span></code> and repeat the processing, e.g., try to find <code class="docutils literal notranslate"><span class="pre">##deniable</span></code>, then <code class="docutils literal notranslate"><span class="pre">##deniabl</span></code>, etc. This is repeated until we’ve segmented the entire word.</p>
</div>
<div class="section" id="Intuition">
<h3>Intuition<a class="headerlink" href="#Intuition" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Intuitively, WordPiece tokenization is trying to satisfy two different objectives:</p>
<ol class="arabic simple">
<li><p>Tokenize the data into the <em>least</em> number of pieces as possible. It is important to keep in mind that the WordPiece algorithm does not “want” to split words. Otherwise, it would just split every word into its characters, e.g., <code class="docutils literal notranslate"><span class="pre">human</span> <span class="pre">-&gt;</span> <span class="pre">{h,</span> <span class="pre">##u,</span> <span class="pre">##m,</span> <span class="pre">##a,</span> <span class="pre">#n}</span></code>. This is one critical thing that makes WordPiece different from morphological splitters, which will split linguistic morphemes even for common words (e.g., <code class="docutils literal notranslate"><span class="pre">unwanted</span> <span class="pre">-&gt;</span> <span class="pre">{un,</span> <span class="pre">want,</span> <span class="pre">ed}</span></code>).</p></li>
<li><p>When a word does have to be split into pieces, split it into pieces that have maximal counts in the training data. For example, the reason why the word <code class="docutils literal notranslate"><span class="pre">undeniable</span></code> would be split into <code class="docutils literal notranslate"><span class="pre">{un,</span> <span class="pre">##deni,</span> <span class="pre">##able}</span></code> rather than alternatives like <code class="docutils literal notranslate"><span class="pre">{unde,</span> <span class="pre">##niab,</span> <span class="pre">##le}</span></code> is that the counts for <code class="docutils literal notranslate"><span class="pre">un</span></code> and <code class="docutils literal notranslate"><span class="pre">##able</span></code> in particular will be very high, since these are common prefixes and suffixes. Even though the count for <code class="docutils literal notranslate"><span class="pre">##le</span></code> must be higher than <code class="docutils literal notranslate"><span class="pre">##able</span></code>, the low counts of <code class="docutils literal notranslate"><span class="pre">unde</span></code> and
<code class="docutils literal notranslate"><span class="pre">##niab</span></code> will make this a less “desirable” tokenization to the algorithm.</p></li>
</ol>
</div>
</div>
<div class="section" id="Optional:-tf.lookup">
<h2>Optional: tf.lookup<a class="headerlink" href="#Optional:-tf.lookup" title="Enlazar permanentemente con este título">¶</a></h2>
<p>If you need access to, or more control over the vocabulary it’s worth noting that you can build the lookup table yourself and pass that to <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code>.</p>
<p>When you pass a string, <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> does the following:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pt_lookup</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">StaticVocabularyTable</span><span class="p">(</span>
    <span class="n">num_oov_buckets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">TextFileInitializer</span><span class="p">(</span>
        <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;pt_vocab.txt&#39;</span><span class="p">,</span>
        <span class="n">key_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span>
        <span class="n">key_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">TextFileIndex</span><span class="o">.</span><span class="n">WHOLE_LINE</span><span class="p">,</span>
        <span class="n">value_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
        <span class="n">value_index</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">TextFileIndex</span><span class="o">.</span><span class="n">LINE_NUMBER</span><span class="p">))</span>
<span class="n">pt_tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">pt_lookup</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now you have direct access to the lookup table used in the tokenizer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pt_lookup</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s1">&#39;é&#39;</span><span class="p">,</span> <span class="s1">&#39;um&#39;</span><span class="p">,</span> <span class="s1">&#39;uma&#39;</span><span class="p">,</span> <span class="s1">&#39;para&#39;</span><span class="p">,</span> <span class="s1">&#39;não&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<p>You don’t need to use a vocabulary file, <code class="docutils literal notranslate"><span class="pre">tf.lookup</span></code> has other initializer options. If you have the vocabulary in memory you can use <code class="docutils literal notranslate"><span class="pre">lookup.KeyValueTensorInitializer</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pt_lookup</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">StaticVocabularyTable</span><span class="p">(</span>
    <span class="n">num_oov_buckets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">KeyValueTensorInitializer</span><span class="p">(</span>
        <span class="n">keys</span><span class="o">=</span><span class="n">pt_vocab</span><span class="p">,</span>
        <span class="n">values</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pt_vocab</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)))</span>
<span class="n">pt_tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">pt_lookup</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>