

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tokenizing with TF Text &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tokenizing with TF Text</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Load_and_process_data/1-10_tokenizers.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Tokenizing-with-TF-Text">
<h1>Tokenizing with TF Text<a class="headerlink" href="#Tokenizing-with-TF-Text" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>ef9ba2abfaeb4df79d8b3cd7734e708c|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>38788c5447794751ab9a9dcaaf625205|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>edb4d63869c543149b72fc5dc5aac50f|View on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>498bfe5d46914e9d86cd77ec013333bb|Download notebook</p>
</td></table><div class="section" id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and/or punctuation. The <code class="docutils literal notranslate"><span class="pre">tensorflow_text</span></code> package provides a number of tokenizers available for preprocessing text required by your text-based models. By performing the tokenization in the TensorFlow graph, you will not need to worry about differences between the training and inference workflows and managing preprocessing scripts.</p>
<p>This guide discusses the many tokenization options provided by TensorFlow Text, when you might want to use one option over another, and how these tokenizers are called from within your model.</p>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[82]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>pip install -q tensorflow-text-nightly
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
     |████████████████████████████████| 4.4MB 5.7MB/s

</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_text</span> <span class="k">as</span> <span class="nn">tf_text</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Splitter-API">
<h2>Splitter API<a class="headerlink" href="#Splitter-API" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The main interfaces are <code class="docutils literal notranslate"><span class="pre">Splitter</span></code> and <code class="docutils literal notranslate"><span class="pre">SplitterWithOffsets</span></code> which have single methods <code class="docutils literal notranslate"><span class="pre">split</span></code> and <code class="docutils literal notranslate"><span class="pre">split_with_offsets</span></code>. The <code class="docutils literal notranslate"><span class="pre">SplitterWithOffsets</span></code> variant (which extends <code class="docutils literal notranslate"><span class="pre">Splitter</span></code>) includes an option for getting byte offsets. This allows the caller to know which bytes in the original string the created token was created from.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> and <code class="docutils literal notranslate"><span class="pre">TokenizerWithOffsets</span></code> are specialized versions of the <code class="docutils literal notranslate"><span class="pre">Splitter</span></code> that provide the convenience methods <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> and <code class="docutils literal notranslate"><span class="pre">tokenize_with_offsets</span></code> respectively.</p>
<p>Generally, for any N-dimensional input, the returned tokens are in a N+1-dimensional <a class="reference external" href="https://www.tensorflow.org/guide/ragged_tensor">RaggedTensor</a> with the inner-most dimension of tokens mapping to the original individual strings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Splitter</span> <span class="p">{</span>
  <span class="nd">@abstractmethod</span>
  <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">SplitterWithOffsets</span><span class="p">(</span><span class="n">Splitter</span><span class="p">)</span> <span class="p">{</span>
  <span class="nd">@abstractmethod</span>
  <span class="k">def</span> <span class="nf">split_with_offsets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>There is also a <code class="docutils literal notranslate"><span class="pre">Detokenizer</span></code> interface. Any tokenizer implementing this interface can accept a N-dimensional ragged tensor of tokens, and normally returns a N-1-dimensional tensor or ragged tensor that has the given tokens assembled together.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Detokenizer</span> <span class="p">{</span>
  <span class="nd">@abstractmethod</span>
  <span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="Tokenizers">
<h2>Tokenizers<a class="headerlink" href="#Tokenizers" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Below is the suite of tokenizers provided by TensorFlow Text. String inputs are assumed to be UTF-8. Please review the <a class="reference external" href="https://www.tensorflow.org/tutorials/load_data/unicode">Unicode guide</a> for converting strings to UTF-8.</p>
<div class="section" id="Whole-word-tokenizers">
<h3>Whole word tokenizers<a class="headerlink" href="#Whole-word-tokenizers" title="Enlazar permanentemente con este título">¶</a></h3>
<p>These tokenizers attempt to split a string by words, and is the most intuitive way to split text.</p>
<div class="section" id="WhitespaceTokenizer">
<h4>WhitespaceTokenizer<a class="headerlink" href="#WhitespaceTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> is the most basic tokenizer which splits strings on ICU defined whitespace characters (eg. space, tab, new line). This is often good for quickly building out prototype models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;What&#39;, b&#39;you&#39;, b&#39;know&#39;, b&#39;you&#39;, b&#34;can&#39;t&#34;, b&#39;explain,&#39;, b&#39;but&#39;, b&#39;you&#39;, b&#39;feel&#39;, b&#39;it.&#39;]]
</pre></div></div>
</div>
<p>You may notice a shortcome of this tokenizer is that punctuation is included with the word to make up a token. To split the words and punctuation into separate tokens, the <code class="docutils literal notranslate"><span class="pre">UnicodeScriptTokenizer</span></code> should be used.</p>
</div>
<div class="section" id="UnicodeScriptTokenizer">
<h4>UnicodeScriptTokenizer<a class="headerlink" href="#UnicodeScriptTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">UnicodeScriptTokenizer</span></code> splits strings based on Unicode script boundaries. The script codes used correspond to International Components for Unicode (ICU) UScriptCode values. See: <a class="reference external" href="http://icu-project.org/apiref/icu4c/uscript_8h.html">http://icu-project.org/apiref/icu4c/uscript_8h.html</a></p>
<p>In practice, this is similar to the <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> with the most apparent difference being that it will split punctuation (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language texts from each other. Note that this will also split contraction words into separate tokens.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">UnicodeScriptTokenizer</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;What&#39;, b&#39;you&#39;, b&#39;know&#39;, b&#39;you&#39;, b&#39;can&#39;, b&#34;&#39;&#34;, b&#39;t&#39;, b&#39;explain&#39;, b&#39;,&#39;, b&#39;but&#39;, b&#39;you&#39;, b&#39;feel&#39;, b&#39;it&#39;, b&#39;.&#39;]]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Subword-tokenizers">
<h3>Subword tokenizers<a class="headerlink" href="#Subword-tokenizers" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Subword tokenizers can be used with a smaller vocabulary, and allow the model to have some information about novel words from the subwords that make create it.</p>
<p>We briefly discuss the Subword tokenization options below, but the <a class="reference external" href="https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer">Subword Tokenization tutorial</a> goes more in depth and also explains how to generate the vocab files.</p>
<div class="section" id="WordpieceTokenizer">
<h4>WordpieceTokenizer<a class="headerlink" href="#WordpieceTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>WordPiece tokenization is a data-driven tokenization scheme which generates a set of sub-tokens. These sub tokens may correspond to linguistic morphemes, but this is often not the case.</p>
<p>The WordpieceTokenizer expects the input to already be split into tokens. Because of this prerequisite, you will often want to split using the <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> or <code class="docutils literal notranslate"><span class="pre">UnicodeScriptTokenizer</span></code> beforehand.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;What&#39;, b&#39;you&#39;, b&#39;know&#39;, b&#39;you&#39;, b&#34;can&#39;t&#34;, b&#39;explain,&#39;, b&#39;but&#39;, b&#39;you&#39;, b&#39;feel&#39;, b&#39;it.&#39;]]
</pre></div></div>
</div>
<p>After the string is split into tokens, the <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code> can be used to split into subtokens.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true&quot;</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="s2">&quot;vocab.txt&quot;</span>
<span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">subtokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">UnicodeScriptTokenizer</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
<span class="n">subtokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subtokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[[b&#39;What&#39;], [b&#39;you&#39;], [b&#39;know&#39;], [b&#39;you&#39;], [b&#34;can&#39;t&#34;], [b&#39;explain,&#39;], [b&#39;but&#39;], [b&#39;you&#39;], [b&#39;feel&#39;], [b&#39;it.&#39;]]]
</pre></div></div>
</div>
</div>
<div class="section" id="BertTokenizer">
<h4>BertTokenizer<a class="headerlink" href="#BertTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The BertTokenizer mirrors the original implemenation of tokenization from the BERT paper. This is backed by the WordpieceTokenizer, but also performs additional tasks such as normalization and tokenizing to words first.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">token_out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[[b&#39;what&#39;], [b&#39;you&#39;], [b&#39;know&#39;], [b&#39;you&#39;], [b&#39;can&#39;], [b&#34;&#39;&#34;], [b&#39;t&#39;], [b&#39;explain&#39;], [b&#39;,&#39;], [b&#39;but&#39;], [b&#39;you&#39;], [b&#39;feel&#39;], [b&#39;it&#39;], [b&#39;.&#39;]]]
</pre></div></div>
</div>
</div>
<div class="section" id="SentencepieceTokenizer">
<h4>SentencepieceTokenizer<a class="headerlink" href="#SentencepieceTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The SentencepieceTokenizer is a sub-token tokenizer that is highly configurable. This is backed by the Sentencepiece library. Like the BertTokenizer, it can include normalization and token splitting before splitting into sub-tokens.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true&quot;</span>
<span class="n">sp_model</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">SentencepieceTokenizer</span><span class="p">(</span><span class="n">sp_model</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;\xe2\x96\x81What&#39;, b&#39;\xe2\x96\x81you&#39;, b&#39;\xe2\x96\x81know&#39;, b&#39;\xe2\x96\x81you&#39;, b&#39;\xe2\x96\x81can&#39;, b&#34;&#39;&#34;, b&#39;t&#39;, b&#39;\xe2\x96\x81explain&#39;, b&#39;,&#39;, b&#39;\xe2\x96\x81but&#39;, b&#39;\xe2\x96\x81you&#39;, b&#39;\xe2\x96\x81feel&#39;, b&#39;\xe2\x96\x81it&#39;, b&#39;.&#39;]]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Other-splitters">
<h3>Other splitters<a class="headerlink" href="#Other-splitters" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="section" id="UnicodeCharTokenizer">
<h4>UnicodeCharTokenizer<a class="headerlink" href="#UnicodeCharTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>This splits a string into UTF-8 characters. It is useful for CJK languages that do not have spaces between words.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]
</pre></div></div>
</div>
<p>The output is Unicode codepoints. This can be also useful for creating character ngrams, such as bigrams. To convert back into UTF-8 characters.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">characters</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">unicode_encode</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;UTF-8&quot;</span><span class="p">)</span>
<span class="n">bigrams</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">ngrams</span><span class="p">(</span><span class="n">characters</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">reduction_type</span><span class="o">=</span><span class="n">tf_text</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">STRING_JOIN</span><span class="p">,</span> <span class="n">string_separator</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bigrams</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;Wh&#39;, b&#39;ha&#39;, b&#39;at&#39;, b&#39;t &#39;, b&#39; y&#39;, b&#39;yo&#39;, b&#39;ou&#39;, b&#39;u &#39;, b&#39; k&#39;, b&#39;kn&#39;, b&#39;no&#39;, b&#39;ow&#39;, b&#39;w &#39;, b&#39; y&#39;, b&#39;yo&#39;, b&#39;ou&#39;, b&#39;u &#39;, b&#39; c&#39;, b&#39;ca&#39;, b&#39;an&#39;, b&#34;n&#39;&#34;, b&#34;&#39;t&#34;, b&#39;t &#39;, b&#39; e&#39;, b&#39;ex&#39;, b&#39;xp&#39;, b&#39;pl&#39;, b&#39;la&#39;, b&#39;ai&#39;, b&#39;in&#39;, b&#39;n,&#39;, b&#39;, &#39;, b&#39; b&#39;, b&#39;bu&#39;, b&#39;ut&#39;, b&#39;t &#39;, b&#39; y&#39;, b&#39;yo&#39;, b&#39;ou&#39;, b&#39;u &#39;, b&#39; f&#39;, b&#39;fe&#39;, b&#39;ee&#39;, b&#39;el&#39;, b&#39;l &#39;, b&#39; i&#39;, b&#39;it&#39;, b&#39;t.&#39;]]
</pre></div></div>
</div>
</div>
<div class="section" id="HubModuleTokenizer">
<h4>HubModuleTokenizer<a class="headerlink" href="#HubModuleTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>This is a wrapper around models deployed to TF Hub to make the calls easier since TF Hub currently does not support ragged tensors. Having a model perform tokenization is particularly useful for CJK languages when you want to split into words, but do not have spaces to provide a heuristic guide. At this time, we have a single segmentation model for Chinese.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">MODEL_HANDLE</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/google/zh_segmentation/1&quot;</span>
<span class="n">segmenter</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">HubModuleTokenizer</span><span class="p">(</span><span class="n">MODEL_HANDLE</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">segmenter</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;新华社北京&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;\xe6\x96\xb0\xe5\x8d\x8e\xe7\xa4\xbe&#39;, b&#39;\xe5\x8c\x97\xe4\xba\xac&#39;]]
</pre></div></div>
</div>
<p>It may be difficult to view the results of the UTF-8 encoded byte strings. Decode the list values to make viewing easier.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">decode_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">decode_list</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;UTF-8&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">decode_utf8_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">decode_list</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">to_list</span><span class="p">()))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">decode_utf8_tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[&#39;新华社&#39;, &#39;北京&#39;]]
</pre></div></div>
</div>
</div>
<div class="section" id="SplitMergeTokenizer">
<h4>SplitMergeTokenizer<a class="headerlink" href="#SplitMergeTokenizer" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">SplitMergeTokenizer</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">SplitMergeFromLogitsTokenizer</span></code> have a targeted purpose of splitting a string based on provided values that indicate where the string should be split. This is useful when building your own segmentation models like the previous Segmentation example.</p>
<p>For the <code class="docutils literal notranslate"><span class="pre">SplitMergeTokenizer</span></code>, a value of 0 is used to indicate the start of a new string, and the value of 1 indicates the character is part of the current string.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[65]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">strings</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;新华社北京&quot;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">SplitMergeTokenizer</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">strings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decode_utf8_tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[&#39;新华社&#39;, &#39;北京&#39;]]
</pre></div></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">SplitMergeFromLogitsTokenizer</span></code> is similar, but it instead accepts logit value pairs from a neural network that predict if each character should be split into a new string or merged into the current one.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">strings</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;新华社北京&quot;</span><span class="p">]]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[[[</span><span class="mf">5.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">12.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]]]</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">SplitMergeFromLogitsTokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">strings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decode_utf8_tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[&#39;新华社&#39;, &#39;北京&#39;]]
</pre></div></div>
</div>
</div>
<div class="section" id="RegexSplitter">
<h4>RegexSplitter<a class="headerlink" href="#RegexSplitter" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">RegexSplitter</span></code> is able to segment strings at arbitrary breakpoints defined by a provided regular expression.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[84]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">splitter</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">RegexSplitter</span><span class="p">(</span><span class="s2">&quot;\s&quot;</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">splitter</span><span class="o">.</span><span class="n">split</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">],</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;What&#39;, b&#39;you&#39;, b&#39;know&#39;, b&#39;you&#39;, b&#34;can&#39;t&#34;, b&#39;explain,&#39;, b&#39;but&#39;, b&#39;you&#39;, b&#39;feel&#39;, b&#39;it.&#39;]]
</pre></div></div>
</div>
</div>
</div>
</div>
<div class="section" id="Offsets">
<h2>Offsets<a class="headerlink" href="#Offsets" title="Enlazar permanentemente con este título">¶</a></h2>
<p>When tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements <code class="docutils literal notranslate"><span class="pre">TokenizerWithOffsets</span></code> has a <em>tokenize_with_offsets</em> method that will return the byte offsets along with the tokens. The start_offsets lists the bytes in the original string each token starts at, and the end_offsets lists the bytes immediately after the point where each token ends. To refrase, the start offsets are inclusive and the end
offsets are exclusive.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[70]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">UnicodeScriptTokenizer</span><span class="p">()</span>
<span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">start_offsets</span><span class="p">,</span> <span class="n">end_offsets</span><span class="p">)</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize_with_offsets</span><span class="p">([</span><span class="s1">&#39;Everything not saved will be lost.&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">start_offsets</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">end_offsets</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;Everything&#39;, b&#39;not&#39;, b&#39;saved&#39;, b&#39;will&#39;, b&#39;be&#39;, b&#39;lost&#39;, b&#39;.&#39;]]
[[0, 11, 15, 21, 26, 29, 33]]
[[10, 14, 20, 25, 28, 33, 34]]
</pre></div></div>
</div>
</div>
<div class="section" id="Detokenization">
<h2>Detokenization<a class="headerlink" href="#Detokenization" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Tokenizers which implement the <code class="docutils literal notranslate"><span class="pre">Detokenizer</span></code> provide a <code class="docutils literal notranslate"><span class="pre">detokenize</span></code> method which attempts to combine the strings. This has the chance of being lossy, so the detokenized string may not always match exactly the original, pre-tokenized string.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;What you know you can&#39;t explain, but you feel it.&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
<span class="n">strings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">strings</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]
[b&#34;What you know you can&#39;t explain, but you feel it.&#34;]
</pre></div></div>
</div>
</div>
<div class="section" id="TF-Data">
<h2>TF Data<a class="headerlink" href="#TF-Data" title="Enlazar permanentemente con este título">¶</a></h2>
<p>TF Data is a powerful API for creating an input pipeline for training models. Tokenizers work as expected with the API.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[79]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">docs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">([[</span><span class="s1">&#39;Never tell me the odds.&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;It&#39;s a trap!&quot;</span><span class="p">]])</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="n">tokenized_docs</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[b&#39;Never&#39;, b&#39;tell&#39;, b&#39;me&#39;, b&#39;the&#39;, b&#39;odds.&#39;]]
[[b&#34;It&#39;s&#34;, b&#39;a&#39;, b&#39;trap!&#39;]]
</pre></div></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>