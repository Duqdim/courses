

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Load text &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" />
    <link rel="next" title="Unicode strings" href="1-06_unicode.html" />
    <link rel="prev" title="Load a pandas.DataFrame" href="1-04_pandas_dataframe.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-01">Sesión 01</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-02">Sesión 02</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-03">Sesión 03</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-04">Sesión 04</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-05">Sesión 05</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-06">Sesión 06</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-07">Sesión 07</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-08">Sesión 08</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-09">Sesión 09</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-10">Sesión 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-11">Sesión 11</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-12">Sesión 12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-13">Sesión 13</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-14">Sesión 14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-15">Sesión 15</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-16">Sesión 16</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a> &raquo;</li>
        
      <li>Load text</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Load_and_process_data/1-05_text.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Load-text">
<h1>Load text<a class="headerlink" href="#Load-text" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>f51c8fc485e643c290222be56feebbd1|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>ac8f1ac631954bf2beeecfd66bec53ee|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>08681ef4750547d4b9d0738ca9a6f11e|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>85990fc62a8d4cae9cf952192b175fb7|Download notebook</p>
</td></table><p>This tutorial demonstrates two ways to load and preprocess text.</p>
<ul class="simple">
<li><p>First, you will use Keras utilities and layers. If you are new to TensorFlow, you should start with these.</p></li>
<li><p>Next, you will use lower-level utilities like <code class="docutils literal notranslate"><span class="pre">tf.data.TextLineDataset</span></code> to load text files, and <code class="docutils literal notranslate"><span class="pre">tf.text</span></code> to preprocess the data for finer-grain control.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Be sure you&#39;re using the stable versions of both tf and tf-text, for binary compatibility.</span>
<span class="o">!</span>pip install -q -U tensorflow
<span class="o">!</span>pip install -q -U tensorflow-text
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental.preprocessing</span> <span class="kn">import</span> <span class="n">TextVectorization</span>

<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">tensorflow_text</span> <span class="k">as</span> <span class="nn">tf_text</span>
</pre></div>
</div>
</div>
<div class="section" id="Example-1:-Predict-the-tag-for-a-Stack-Overflow-question">
<h2>Example 1: Predict the tag for a Stack Overflow question<a class="headerlink" href="#Example-1:-Predict-the-tag-for-a-Stack-Overflow-question" title="Enlazar permanentemente con este título">¶</a></h2>
<p>As a first example, you will download a dataset of programming questions from Stack Overflow. Each question (“How do I sort a dictionary by value?”) is labeled with exactly one tag (<code class="docutils literal notranslate"><span class="pre">Python</span></code>, <code class="docutils literal notranslate"><span class="pre">CSharp</span></code>, <code class="docutils literal notranslate"><span class="pre">JavaScript</span></code>, or <code class="docutils literal notranslate"><span class="pre">Java</span></code>). Your task is to develop a model that predicts the tag for a question. This is an example of multi-class classification, an important and widely applicable kind of machine learning problem.</p>
<div class="section" id="Download-and-explore-the-dataset">
<h3>Download and explore the dataset<a class="headerlink" href="#Download-and-explore-the-dataset" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next, you will download the dataset, and explore the directory structure.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data_url</span> <span class="o">=</span> <span class="s1">&#39;https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz&#39;</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span>
    <span class="s1">&#39;stack_overflow_16k.tar.gz&#39;</span><span class="p">,</span>
    <span class="n">data_url</span><span class="p">,</span>
    <span class="n">untar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cache_dir</span><span class="o">=</span><span class="s1">&#39;stack_overflow&#39;</span><span class="p">,</span>
    <span class="n">cache_subdir</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">dataset_dir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">list</span><span class="p">(</span><span class="n">dataset_dir</span><span class="o">.</span><span class="n">iterdir</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_dir</span> <span class="o">=</span> <span class="n">dataset_dir</span><span class="o">/</span><span class="s1">&#39;train&#39;</span>
<span class="nb">list</span><span class="p">(</span><span class="n">train_dir</span><span class="o">.</span><span class="n">iterdir</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">train/csharp</span></code>, <code class="docutils literal notranslate"><span class="pre">train/java</span></code>, <code class="docutils literal notranslate"><span class="pre">train/python</span></code> and <code class="docutils literal notranslate"><span class="pre">train/javascript</span></code> directories contain many text files, each of which is a Stack Overflow question. Print a file and inspect the data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sample_file</span> <span class="o">=</span> <span class="n">train_dir</span><span class="o">/</span><span class="s1">&#39;python/1755.txt&#39;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">sample_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-the-dataset">
<h3>Load the dataset<a class="headerlink" href="#Load-the-dataset" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next, you will load the data off disk and prepare it into a format suitable for training. To do so, you will use <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory">text_dataset_from_directory</a> utility to create a labeled <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>. If you’re new to <a class="reference external" href="https://www.tensorflow.org/guide/data">tf.data</a>, it’s a powerful collection of tools for building input pipelines.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">preprocessing.text_dataset_from_directory</span></code> expects a directory structure as follows.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>train/
...csharp/
......1.txt
......2.txt
...java/
......1.txt
......2.txt
...javascript/
......1.txt
......2.txt
...python/
......1.txt
......2.txt
</pre></div>
</div>
<p>When running a machine learning experiment, it is a best practice to divide your dataset into three splits: <a class="reference external" href="https://developers.google.com/machine-learning/glossary#training_set">train</a>, <a class="reference external" href="https://developers.google.com/machine-learning/glossary#validation_set">validation</a>, and <a class="reference external" href="https://developers.google.com/machine-learning/glossary#test-set">test</a>. The Stack Overflow dataset has already been divided into train and test, but it lacks a validation set. Create a validation set using an
80:20 split of the training data by using the <code class="docutils literal notranslate"><span class="pre">validation_split</span></code> argument below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">raw_train_ds</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">subset</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>As you can see above, there are 8,000 examples in the training folder, of which you will use 80% (or 6,400) for training. As you will see in a moment, you can train a model by passing a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> directly to <code class="docutils literal notranslate"><span class="pre">model.fit</span></code>. First, iterate over the dataset and print out a few examples, to get a feel for the data.</p>
<p>Note: To increase the difficulty of the classification problem, the dataset author replaced occurrences of the words <em>Python</em>, <em>CSharp</em>, <em>JavaScript</em>, or <em>Java</em> in the programming question with the word <em>blank</em>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="ow">in</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Question: &quot;</span><span class="p">,</span> <span class="n">text_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label:&quot;</span><span class="p">,</span> <span class="n">label_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>The labels are <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code> or <code class="docutils literal notranslate"><span class="pre">3</span></code>. To see which of these correspond to which string label, you can check the <code class="docutils literal notranslate"><span class="pre">class_names</span></code> property on the dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_train_ds</span><span class="o">.</span><span class="n">class_names</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;corresponds to&quot;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next, you will create a validation and test dataset. You will use the remaining 1,600 reviews from the training set for validation.</p>
<p>Note: When using the <code class="docutils literal notranslate"><span class="pre">validation_split</span></code> and <code class="docutils literal notranslate"><span class="pre">subset</span></code> arguments, make sure to either specify a random seed, or to pass <code class="docutils literal notranslate"><span class="pre">shuffle=False</span></code>, so that the validation and training splits have no overlap.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">raw_val_ds</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">subset</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">test_dir</span> <span class="o">=</span> <span class="n">dataset_dir</span><span class="o">/</span><span class="s1">&#39;test&#39;</span>
<span class="n">raw_test_ds</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="n">test_dir</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Prepare-the-dataset-for-training">
<h3>Prepare the dataset for training<a class="headerlink" href="#Prepare-the-dataset-for-training" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Note: The Preprocessing APIs used in this section are experimental in TensorFlow 2.3 and subject to change.</p>
<p>Next, you will standardize, tokenize, and vectorize the data using the <code class="docutils literal notranslate"><span class="pre">preprocessing.TextVectorization</span></code> layer. * Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset.</p>
<ul class="simple">
<li><p>Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words by splitting on whitespace).</p></li>
<li><p>Vectorization refers to converting tokens into numbers so they can be fed into a neural network.</p></li>
</ul>
<p>All of these tasks can be accomplished with this layer. You can learn more about each of these in the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization">API doc</a>.</p>
<ul class="simple">
<li><p>The default standardization converts text to lowercase and removes punctuation.</p></li>
<li><p>The default tokenizer splits on whitespace.</p></li>
<li><p>The default vectorization mode is <code class="docutils literal notranslate"><span class="pre">int</span></code>. This outputs integer indices (one per token). This mode can be used to build models that take word order into account. You can also use other modes, like <code class="docutils literal notranslate"><span class="pre">binary</span></code>, to build bag-of-word models.</p></li>
</ul>
<p>You will build two modes to learn more about these. First, you will use the <code class="docutils literal notranslate"><span class="pre">binary</span></code> model to build a bag-of-words model. Next, you will use the <code class="docutils literal notranslate"><span class="pre">int</span></code> mode with a 1D ConvNet.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">binary_vectorize_layer</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>For <code class="docutils literal notranslate"><span class="pre">int</span></code> mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length, which will cause the layer to pad or truncate sequences to exactly sequence_length values.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">int_vectorize_layer</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next, you will call <code class="docutils literal notranslate"><span class="pre">adapt</span></code> to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.</p>
<p>Note: it’s important to only use your training data when calling adapt (using the test set would leak information).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Make a text-only dataset (without labels), then call adapt</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">text</span><span class="p">)</span>
<span class="n">binary_vectorize_layer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
<span class="n">int_vectorize_layer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>See the result of using these layers to preprocess data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">binary_vectorize_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
  <span class="n">text</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">binary_vectorize_layer</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">label</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">int_vectorize_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
  <span class="n">text</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">int_vectorize_layer</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">label</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Retrieve a batch (of 32 reviews and labels) from the dataset</span>
<span class="n">text_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">raw_train_ds</span><span class="p">))</span>
<span class="n">first_question</span><span class="p">,</span> <span class="n">first_label</span> <span class="o">=</span> <span class="n">text_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">,</span> <span class="n">first_question</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label&quot;</span><span class="p">,</span> <span class="n">first_label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&#39;binary&#39; vectorized question:&quot;</span><span class="p">,</span>
      <span class="n">binary_vectorize_text</span><span class="p">(</span><span class="n">first_question</span><span class="p">,</span> <span class="n">first_label</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&#39;int&#39; vectorized question:&quot;</span><span class="p">,</span>
      <span class="n">int_vectorize_text</span><span class="p">(</span><span class="n">first_question</span><span class="p">,</span> <span class="n">first_label</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>As you can see above, <code class="docutils literal notranslate"><span class="pre">binary</span></code> mode returns an array denoting which tokens exist at least once in the input, while <code class="docutils literal notranslate"><span class="pre">int</span></code> mode replaces each token by an integer, thus preserving their order. You can lookup the token (string) that each integer corresponds to by calling <code class="docutils literal notranslate"><span class="pre">.get_vocabulary()</span></code> on the layer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1289 ---&gt; &quot;</span><span class="p">,</span> <span class="n">int_vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()[</span><span class="mi">1289</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;313 ---&gt; &quot;</span><span class="p">,</span> <span class="n">int_vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()[</span><span class="mi">313</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">int_vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">())))</span>
</pre></div>
</div>
</div>
<p>You are nearly ready to train your model. As a final preprocessing step, you will apply the <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layers you created earlier to the train, validation, and test dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">binary_train_ds</span> <span class="o">=</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">binary_vectorize_text</span><span class="p">)</span>
<span class="n">binary_val_ds</span> <span class="o">=</span> <span class="n">raw_val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">binary_vectorize_text</span><span class="p">)</span>
<span class="n">binary_test_ds</span> <span class="o">=</span> <span class="n">raw_test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">binary_vectorize_text</span><span class="p">)</span>

<span class="n">int_train_ds</span> <span class="o">=</span> <span class="n">raw_train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">int_vectorize_text</span><span class="p">)</span>
<span class="n">int_val_ds</span> <span class="o">=</span> <span class="n">raw_val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">int_vectorize_text</span><span class="p">)</span>
<span class="n">int_test_ds</span> <span class="o">=</span> <span class="n">raw_test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">int_vectorize_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Configure-the-dataset-for-performance">
<h3>Configure the dataset for performance<a class="headerlink" href="#Configure-the-dataset-for-performance" title="Enlazar permanentemente con este título">¶</a></h3>
<p>These are two important methods you should use when loading data to make sure that I/O does not become blocking.</p>
<p><code class="docutils literal notranslate"><span class="pre">.cache()</span></code> keeps data in memory after it’s loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.</p>
<p><code class="docutils literal notranslate"><span class="pre">.prefetch()</span></code> overlaps data preprocessing and model execution while training.</p>
<p>You can learn more about both methods, as well as how to cache data to disk in the <a class="reference external" href="https://www.tensorflow.org/guide/data_performance">data performance guide</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">AUTOTUNE</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span>

<span class="k">def</span> <span class="nf">configure_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">binary_train_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">binary_train_ds</span><span class="p">)</span>
<span class="n">binary_val_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">binary_val_ds</span><span class="p">)</span>
<span class="n">binary_test_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">binary_test_ds</span><span class="p">)</span>

<span class="n">int_train_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">int_train_ds</span><span class="p">)</span>
<span class="n">int_val_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">int_val_ds</span><span class="p">)</span>
<span class="n">int_test_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">int_test_ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Train-the-model">
<h3>Train the model<a class="headerlink" href="#Train-the-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>It’s time to create our neural network. For the <code class="docutils literal notranslate"><span class="pre">binary</span></code> vectorized data, train a simple bag-of-words linear model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">binary_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">)])</span>
<span class="n">binary_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">binary_train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">binary_val_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next, you will use the <code class="docutils literal notranslate"><span class="pre">int</span></code> vectorized layer to build a 1D ConvNet.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling1D</span><span class="p">(),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_labels</span><span class="p">)</span>
  <span class="p">])</span>
  <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.</span>
<span class="n">int_model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">VOCAB_SIZE</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">int_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">int_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">int_train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">int_val_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Compare the two models:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Linear model on binary vectorized data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">binary_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ConvNet model on int vectorized data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">int_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Evaluate both models on the test data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">binary_loss</span><span class="p">,</span> <span class="n">binary_accuracy</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">binary_test_ds</span><span class="p">)</span>
<span class="n">int_loss</span><span class="p">,</span> <span class="n">int_accuracy</span> <span class="o">=</span> <span class="n">int_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">int_test_ds</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Binary model accuracy: </span><span class="si">{:2.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">binary_accuracy</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Int model accuracy: </span><span class="si">{:2.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">int_accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Note: This example dataset represents a rather simple classification problem. More complex datasets and problems bring out subtle but significant differences in preprocessing strategies and model architectures. Be sure to try out different hyperparameters and epochs to compare various approaches.</p>
</div>
<div class="section" id="Export-the-model">
<h3>Export the model<a class="headerlink" href="#Export-the-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>In the code above, you applied the <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer to the dataset before feeding text to the model. If you want to make your model capable of processing raw strings (for example, to simplify deploying it), you can include the <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer inside your model. To do so, you can create a new model using the weights you just trained.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">export_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span><span class="n">binary_vectorize_layer</span><span class="p">,</span> <span class="n">binary_model</span><span class="p">,</span>
     <span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)])</span>

<span class="n">export_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="c1"># Test it with `raw_test_ds`, which yields raw strings</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">export_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">raw_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">{:2.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">binary_accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Now your model can take raw strings as input and predict a score for each label using <code class="docutils literal notranslate"><span class="pre">model.predict</span></code>. Define a function to find the label with the maximum score:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">get_string_labels</span><span class="p">(</span><span class="n">predicted_scores_batch</span><span class="p">):</span>
  <span class="n">predicted_int_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predicted_scores_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">raw_train_ds</span><span class="o">.</span><span class="n">class_names</span><span class="p">,</span> <span class="n">predicted_int_labels</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">predicted_labels</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Run-inference-on-new-data">
<h3>Run inference on new data<a class="headerlink" href="#Run-inference-on-new-data" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;how do I extract keys from a dict into a list?&quot;</span><span class="p">,</span>  <span class="c1"># python</span>
    <span class="s2">&quot;debug public static void main(string[] args) {...}&quot;</span><span class="p">,</span>  <span class="c1"># java</span>
<span class="p">]</span>
<span class="n">predicted_scores</span> <span class="o">=</span> <span class="n">export_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">get_string_labels</span><span class="p">(</span><span class="n">predicted_scores</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Question: &quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted label: &quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Including the text preprocessing logic inside your model enables you to export a model for production that simplifies deployment, and reduces the potential for <a class="reference external" href="https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew">train/test skew</a>.</p>
<p>There is a performance difference to keep in mind when choosing where to apply your <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer. Using it outside of your model enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So, if you’re training your model on the GPU, you probably want to go with this option to get the best performance while developing your model, then switch to including the TextVectorization layer inside your model when you’re ready to prepare for deployment.</p>
<p>Visit this <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/save_and_load">tutorial</a> to learn more about saving models.</p>
</div>
</div>
<div class="section" id="Example-2:-Predict-the-author-of-Illiad-translations">
<h2>Example 2: Predict the author of Illiad translations<a class="headerlink" href="#Example-2:-Predict-the-author-of-Illiad-translations" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The following provides an example of using <code class="docutils literal notranslate"><span class="pre">tf.data.TextLineDataset</span></code> to load examples from text files, and <code class="docutils literal notranslate"><span class="pre">tf.text</span></code> to preprocess the data. In this example, you will use three different English translations of the same work, Homer’s Illiad, and train a model to identify the translator given a single line of text.</p>
<div class="section" id="id9">
<h3>Download and explore the dataset<a class="headerlink" href="#id9" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The texts of the three translations are by:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/William_Cowper">William Cowper</a> — <a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt">text</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby">Edward, Earl of Derby</a> — <a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt">text</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29">Samuel Butler</a> — <a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt">text</a></p></li>
</ul>
<p>The text files used in this tutorial have undergone some typical preprocessing tasks like removing document header and footer, line numbers and chapter titles. Download these lightly munged files locally.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">DIRECTORY_URL</span> <span class="o">=</span> <span class="s1">&#39;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&#39;</span>
<span class="n">FILE_NAMES</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cowper.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;derby.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;butler.txt&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">FILE_NAMES</span><span class="p">:</span>
  <span class="n">text_dir</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="n">DIRECTORY_URL</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>

<span class="n">parent_dir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">text_dir</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
<span class="nb">list</span><span class="p">(</span><span class="n">parent_dir</span><span class="o">.</span><span class="n">iterdir</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id10">
<h3>Load the dataset<a class="headerlink" href="#id10" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You will use <code class="docutils literal notranslate"><span class="pre">TextLineDataset</span></code>, which is designed to create a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> from a text file in which each example is a line of text from the original file, whereas <code class="docutils literal notranslate"><span class="pre">text_dataset_from_directory</span></code> treats all contents of a file as a single example. <code class="docutils literal notranslate"><span class="pre">TextLineDataset</span></code> is useful for text data that is primarily line-based (for example, poetry or error logs).</p>
<p>Iterate through these files, loading each one into its own dataset. Each example needs to be individually labeled, so use <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.map</span></code> to apply a labeler function to each one. This will iterate over every example in the dataset, returning (<code class="docutils literal notranslate"><span class="pre">example,</span> <span class="pre">label</span></code>) pairs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">labeler</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">example</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">labeled_data_sets</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">FILE_NAMES</span><span class="p">):</span>
  <span class="n">lines_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TextLineDataset</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">parent_dir</span><span class="o">/</span><span class="n">file_name</span><span class="p">))</span>
  <span class="n">labeled_dataset</span> <span class="o">=</span> <span class="n">lines_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">ex</span><span class="p">:</span> <span class="n">labeler</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
  <span class="n">labeled_data_sets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labeled_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Next, you’ll combine these labeled datasets into a single dataset, and shuffle it.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">VALIDATION_SIZE</span> <span class="o">=</span> <span class="mi">5000</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">all_labeled_data</span> <span class="o">=</span> <span class="n">labeled_data_sets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">labeled_dataset</span> <span class="ow">in</span> <span class="n">labeled_data_sets</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
  <span class="n">all_labeled_data</span> <span class="o">=</span> <span class="n">all_labeled_data</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">labeled_dataset</span><span class="p">)</span>

<span class="n">all_labeled_data</span> <span class="o">=</span> <span class="n">all_labeled_data</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
    <span class="n">BUFFER_SIZE</span><span class="p">,</span> <span class="n">reshuffle_each_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Print out a few examples as before. The dataset hasn’t been batched yet, hence each entry in <code class="docutils literal notranslate"><span class="pre">all_labeled_data</span></code> corresponds to one data point:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">all_labeled_data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sentence: &quot;</span><span class="p">,</span> <span class="n">text</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id11">
<h3>Prepare the dataset for training<a class="headerlink" href="#id11" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Instead of using the Keras <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer to preprocess our text dataset, you will now use the <code class="docutils literal notranslate"><span class="pre">`tf.text</span></code> API &lt;<a class="reference external" href="https://www.tensorflow.org/tutorials/tensorflow_text/intro">https://www.tensorflow.org/tutorials/tensorflow_text/intro</a>&gt;`__ to standardize and tokenize the data, build a vocabulary and use <code class="docutils literal notranslate"><span class="pre">StaticVocabularyTable</span></code> to map tokens to integers to feed to the model.</p>
<p>While tf.text provides various tokenizers, you will use the <code class="docutils literal notranslate"><span class="pre">UnicodeScriptTokenizer</span></code> to tokenize our dataset. Define a function to convert the text to lower-case and tokenize it. You will use <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.map</span></code> to apply the tokenization to the dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">UnicodeScriptTokenizer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">unused_label</span><span class="p">):</span>
  <span class="n">lower_case</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">case_fold_utf8</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">lower_case</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenized_ds</span> <span class="o">=</span> <span class="n">all_labeled_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>You can iterate over the dataset and print out a few tokenized examples.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">text_batch</span> <span class="ow">in</span> <span class="n">tokenized_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokens: &quot;</span><span class="p">,</span> <span class="n">text_batch</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Next, you will build a vocabulary by sorting tokens by frequency and keeping the top <code class="docutils literal notranslate"><span class="pre">VOCAB_SIZE</span></code> tokens.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenized_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">tokenized_ds</span><span class="p">)</span>

<span class="n">vocab_dict</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">toks</span> <span class="ow">in</span> <span class="n">tokenized_ds</span><span class="o">.</span><span class="n">as_numpy_iterator</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">toks</span><span class="p">:</span>
    <span class="n">vocab_dict</span><span class="p">[</span><span class="n">tok</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">]</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[:</span><span class="n">VOCAB_SIZE</span><span class="p">]</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocab size: &quot;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five vocab entries:&quot;</span><span class="p">,</span> <span class="n">vocab</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>To convert the tokens into integers, use the <code class="docutils literal notranslate"><span class="pre">vocab</span></code> set to create a <code class="docutils literal notranslate"><span class="pre">StaticVocabularyTable</span></code>. You will map tokens to integers in the range [<code class="docutils literal notranslate"><span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">vocab_size</span> <span class="pre">+</span> <span class="pre">2</span></code>]. As with the <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer, <code class="docutils literal notranslate"><span class="pre">0</span></code> is reserved to denote padding and <code class="docutils literal notranslate"><span class="pre">1</span></code> is reserved to denote an out-of-vocabulary (OOV) token.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">keys</span> <span class="o">=</span> <span class="n">vocab</span>
<span class="n">values</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># reserve 0 for padding, 1 for OOV</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">KeyValueTensorInitializer</span><span class="p">(</span>
    <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">key_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">value_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

<span class="n">num_oov_buckets</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">vocab_table</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lookup</span><span class="o">.</span><span class="n">StaticVocabularyTable</span><span class="p">(</span><span class="n">init</span><span class="p">,</span> <span class="n">num_oov_buckets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Finally, define a fuction to standardize, tokenize and vectorize the dataset using the tokenizer and lookup table:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
  <span class="n">standardized</span> <span class="o">=</span> <span class="n">tf_text</span><span class="o">.</span><span class="n">case_fold_utf8</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
  <span class="n">tokenized</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">standardized</span><span class="p">)</span>
  <span class="n">vectorized</span> <span class="o">=</span> <span class="n">vocab_table</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">tokenized</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">vectorized</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
</div>
<p>You can try this on a single example to see the output:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">example_text</span><span class="p">,</span> <span class="n">example_label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">all_labeled_data</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sentence: &quot;</span><span class="p">,</span> <span class="n">example_text</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">vectorized_text</span><span class="p">,</span> <span class="n">example_label</span> <span class="o">=</span> <span class="n">preprocess_text</span><span class="p">(</span><span class="n">example_text</span><span class="p">,</span> <span class="n">example_label</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vectorized sentence: &quot;</span><span class="p">,</span> <span class="n">vectorized_text</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Now run the preprocess function on the dataset using <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.map</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">all_encoded_data</span> <span class="o">=</span> <span class="n">all_labeled_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">preprocess_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Split-the-dataset-into-train-and-test">
<h3>Split the dataset into train and test<a class="headerlink" href="#Split-the-dataset-into-train-and-test" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The Keras <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer also batches and pads the vectorized data. Padding is required because the examples inside of a batch need to be the same size and shape, but the examples in these datasets are not all the same size — each line of text has a different number of words. <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> supports splitting and padded-batching datasets:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">all_encoded_data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">VALIDATION_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">all_encoded_data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">VALIDATION_SIZE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_data</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, <code class="docutils literal notranslate"><span class="pre">validation_data</span></code> and <code class="docutils literal notranslate"><span class="pre">train_data</span></code> are not collections of (<code class="docutils literal notranslate"><span class="pre">example,</span> <span class="pre">label</span></code>) pairs, but collections of batches. Each batch is a pair of (<em>many examples</em>, <em>many labels</em>) represented as arrays. To illustrate:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sample_text</span><span class="p">,</span> <span class="n">sample_labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">validation_data</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Text batch shape: &quot;</span><span class="p">,</span> <span class="n">sample_text</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label batch shape: &quot;</span><span class="p">,</span> <span class="n">sample_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First text example: &quot;</span><span class="p">,</span> <span class="n">sample_text</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First label example: &quot;</span><span class="p">,</span> <span class="n">sample_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Since we use <code class="docutils literal notranslate"><span class="pre">0</span></code> for padding and <code class="docutils literal notranslate"><span class="pre">1</span></code> for out-of-vocabulary (OOV) tokens, the vocabulary size has increased by two.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">vocab_size</span> <span class="o">+=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<p>Configure the datasets for better performance as before.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id12">
<h3>Train the model<a class="headerlink" href="#id12" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You can train a model on this dataset as before.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss: &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">{:2.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id13">
<h3>Export the model<a class="headerlink" href="#id13" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To make our model capable to taking raw strings as input, you will create a <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer that performs the same steps as our custom preprocessing function. Since you already trained a vocabulary, you can use <code class="docutils literal notranslate"><span class="pre">set_vocaublary</span></code> instead of <code class="docutils literal notranslate"><span class="pre">adapt</span></code> which trains a new vocabulary.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">preprocess_layer</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">standardize</span><span class="o">=</span><span class="n">tf_text</span><span class="o">.</span><span class="n">case_fold_utf8</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>
<span class="n">preprocess_layer</span><span class="o">.</span><span class="n">set_vocabulary</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">export_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span><span class="n">preprocess_layer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
     <span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)])</span>

<span class="n">export_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create a test dataset of raw strings</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">all_labeled_data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">VALIDATION_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">test_ds</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">export_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss: &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">{:2.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>The loss and accuracy for the model on encoded validation set and the exported model on the raw validation set are the same, as expected.</p>
</div>
<div class="section" id="id14">
<h3>Run inference on new data<a class="headerlink" href="#id14" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Join&#39;d to th&#39; Ionians with their flowing robes,&quot;</span><span class="p">,</span>  <span class="c1"># Label: 1</span>
    <span class="s2">&quot;the allies, and his armour flashed about him so that he seemed to all&quot;</span><span class="p">,</span>  <span class="c1"># Label: 2</span>
    <span class="s2">&quot;And with loud clangor of his arms he fell.&quot;</span><span class="p">,</span>  <span class="c1"># Label: 0</span>
<span class="p">]</span>
<span class="n">predicted_scores</span> <span class="o">=</span> <span class="n">export_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predicted_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Question: &quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted label: &quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Downloading-more-datasets-using-TensorFlow-Datasets-(TFDS)">
<h2>Downloading more datasets using TensorFlow Datasets (TFDS)<a class="headerlink" href="#Downloading-more-datasets-using-TensorFlow-Datasets-(TFDS)" title="Enlazar permanentemente con este título">¶</a></h2>
<p>You can download many more datasets from <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/overview">TensorFlow Datasets</a>. As an example, you will download the <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/imdb_reviews">IMDB Large Movie Review dataset</a>, and use it to train a model for sentiment classification.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s1">&#39;imdb_reviews&#39;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle_files</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">val_ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s1">&#39;imdb_reviews&#39;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle_files</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Print a few examples.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">review_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="ow">in</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Review: &quot;</span><span class="p">,</span> <span class="n">review_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label: &quot;</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>You can now preprocess the data and train a model as before.</p>
<p>Note: You will use <code class="docutils literal notranslate"><span class="pre">losses.BinaryCrossentropy</span></code> instead of <code class="docutils literal notranslate"><span class="pre">losses.SparseCategoricalCrossentropy</span></code> for your model since this is a binary classification problem.</p>
<div class="section" id="id15">
<h3>Prepare the dataset for training<a class="headerlink" href="#id15" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">vectorize_layer</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>

<span class="c1"># Make a text-only dataset (without labels), then call adapt</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">text</span><span class="p">)</span>
<span class="n">vectorize_layer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">vectorize_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
  <span class="n">text</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">vectorize_layer</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">label</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">vectorize_text</span><span class="p">)</span>
<span class="n">val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">vectorize_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Configure datasets for performance as before</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">train_ds</span><span class="p">)</span>
<span class="n">val_ds</span> <span class="o">=</span> <span class="n">configure_dataset</span><span class="p">(</span><span class="n">val_ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id16">
<h3>Train the model<a class="headerlink" href="#id16" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">VOCAB_SIZE</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">val_ds</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss: &quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">{:2.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id17">
<h3>Export the model<a class="headerlink" href="#id17" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">export_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span><span class="n">vectorize_layer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
     <span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)])</span>

<span class="n">export_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 0 --&gt; negative review</span>
<span class="c1"># 1 --&gt; positive review</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;This is a fantastic movie.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;This is a bad movie.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;This movie was so bad that it was good.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I will never say yes to watching this movie.&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">predicted_scores</span> <span class="o">=</span> <span class="n">export_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">predicted_scores</span><span class="p">]</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Question: &quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted label: &quot;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial demonstrated several ways to load and preprocess text. As a next step, you can explore additional tutorials on the website, or download new datasets from <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/overview">TensorFlow Datasets</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>