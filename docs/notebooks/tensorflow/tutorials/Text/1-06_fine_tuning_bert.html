

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Fine-tuning a BERT model &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" />
    <link rel="next" title="Text generation with an RNN" href="1-07_text_generation.html" />
    <link rel="prev" title="Solve GLUE tasks using BERT on TPU" href="1-05_solve_glue_tasks_using_bert_on_tpu.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-01">Sesión 01</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-02">Sesión 02</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-03">Sesión 03</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-04">Sesión 04</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-05">Sesión 05</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-06">Sesión 06</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-07">Sesión 07</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-08">Sesión 08</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-09">Sesión 09</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-10">Sesión 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-11">Sesión 11</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-12">Sesión 12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-13">Sesión 13</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-14">Sesión 14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-15">Sesión 15</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-16">Sesión 16</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a> &raquo;</li>
        
      <li>Fine-tuning a BERT model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Text/1-06_fine_tuning_bert.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Fine-tuning-a-BERT-model">
<h1>Fine-tuning a BERT model<a class="headerlink" href="#Fine-tuning-a-BERT-model" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>14dd43799fed4db5873fe0292dcb264d|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>fbab9f38302f4297a6b51bb91556ab43|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>e4a0623f18444d35ac73910e7ae88009|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>7ef125d6358b43c988b67010d084e0f7|Download notebook</p>
</td><td><p><a href="#id9"><span class="problematic" id="id10">|</span></a>fcd551759a1143228a33f2be879472b7|See TF Hub model</p>
</td></table><p>In this example, we will work through fine-tuning a BERT model using the tensorflow-models PIP package.</p>
<p>The pretrained BERT model this tutorial is based on is also available on <a class="reference external" href="https://tensorflow.org/hub">TensorFlow Hub</a>, to see how to use it refer to the <a class="reference external" href="#hub_bert">Hub Appendix</a></p>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="section" id="Install-the-TensorFlow-Model-Garden-pip-package">
<h3>Install the TensorFlow Model Garden pip package<a class="headerlink" href="#Install-the-TensorFlow-Model-Garden-pip-package" title="Enlazar permanentemente con este título">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf-models-official</span></code> is the stable Model Garden package. Note that it may not include the latest changes in the <code class="docutils literal notranslate"><span class="pre">tensorflow_models</span></code> github repo. To include latest changes, you may install <code class="docutils literal notranslate"><span class="pre">tf-models-nightly</span></code>, which is the nightly Model Garden package created daily automatically.</p></li>
<li><p>pip will install all models and dependencies automatically.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>pip install -q tf-models-official<span class="o">==</span><span class="m">2</span>.4.0
</pre></div>
</div>
</div>
</div>
<div class="section" id="Imports">
<h3>Imports<a class="headerlink" href="#Imports" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">tensorflow_hub</span> <span class="k">as</span> <span class="nn">hub</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="n">tfds</span><span class="o">.</span><span class="n">disable_progress_bar</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">official.modeling</span> <span class="kn">import</span> <span class="n">tf_utils</span>
<span class="kn">from</span> <span class="nn">official</span> <span class="kn">import</span> <span class="n">nlp</span>
<span class="kn">from</span> <span class="nn">official.nlp</span> <span class="kn">import</span> <span class="n">bert</span>

<span class="c1"># Load the required submodules</span>
<span class="kn">import</span> <span class="nn">official.nlp.optimization</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.bert_models</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.configs</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.run_classifier</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.tokenization</span>
<span class="kn">import</span> <span class="nn">official.nlp.data.classifier_data_lib</span>
<span class="kn">import</span> <span class="nn">official.nlp.modeling.losses</span>
<span class="kn">import</span> <span class="nn">official.nlp.modeling.models</span>
<span class="kn">import</span> <span class="nn">official.nlp.modeling.networks</span>

</pre></div>
</div>
</div>
</div>
<div class="section" id="Resources">
<h3>Resources<a class="headerlink" href="#Resources" title="Enlazar permanentemente con este título">¶</a></h3>
<p>This directory contains the configuration, vocabulary, and a pre-trained checkpoint used in this tutorial:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">gs_folder_bert</span> <span class="o">=</span> <span class="s2">&quot;gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12&quot;</span>
<span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>You can get a pre-trained BERT encoder from <a class="reference external" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2">TensorFlow Hub</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hub_url_bert</span> <span class="o">=</span> <span class="s2">&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3&quot;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="The-data">
<h2>The data<a class="headerlink" href="#The-data" title="Enlazar permanentemente con este título">¶</a></h2>
<p>For this example we used the <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/glue#gluemrpc">GLUE MRPC dataset from TFDS</a>.</p>
<p>This dataset is not set up so that it can be directly fed into the BERT model, so this section also handles the necessary preprocessing.</p>
<div class="section" id="Get-the-dataset-from-TensorFlow-Datasets">
<h3>Get the dataset from TensorFlow Datasets<a class="headerlink" href="#Get-the-dataset-from-TensorFlow-Datasets" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The Microsoft Research Paraphrase Corpus (Dolan &amp; Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.</p>
<ul class="simple">
<li><p>Number of labels: 2.</p></li>
<li><p>Size of training dataset: 3668.</p></li>
<li><p>Size of evaluation dataset: 408.</p></li>
<li><p>Maximum sequence length of training and evaluation dataset: 128.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">glue</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;glue/mrpc&#39;</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="c1"># It&#39;s small, load the whole dataset</span>
                       <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">list</span><span class="p">(</span><span class="n">glue</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">info</span></code> object describes the dataset and it’s features:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">info</span><span class="o">.</span><span class="n">features</span>
</pre></div>
</div>
</div>
<p>The two classes are:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">info</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">names</span>
</pre></div>
</div>
</div>
<p>Here is one example from the training set:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">glue_train</span> <span class="o">=</span> <span class="n">glue</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">glue_train</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">:</span><span class="s2">9s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="The-BERT-tokenizer">
<h3>The BERT tokenizer<a class="headerlink" href="#The-BERT-tokenizer" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To fine tune a pre-trained model you need to be sure that you’re using exactly the same tokenization, vocabulary, and index mapping as you used during training.</p>
<p>The BERT tokenizer used in this tutorial is written in pure Python (It’s not built out of TensorFlow ops). So you can’t just plug it into your model as a <code class="docutils literal notranslate"><span class="pre">keras.layer</span></code> like you can with <code class="docutils literal notranslate"><span class="pre">preprocessing.TextVectorization</span></code>.</p>
<p>The following code rebuilds the tokenizer that was used by the base model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Set up tokenizer to generate Tensorflow dataset</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">tokenization</span><span class="o">.</span><span class="n">FullTokenizer</span><span class="p">(</span>
    <span class="n">vocab_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">,</span> <span class="s2">&quot;vocab.txt&quot;</span><span class="p">),</span>
     <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocab size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Tokenize a sentence:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;Hello TensorFlow!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Preprocess-the-data">
<h3>Preprocess the data<a class="headerlink" href="#Preprocess-the-data" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The section manually preprocessed the dataset into the format expected by the model.</p>
<p>This dataset is small, so preprocessing can be done quickly and easily in memory. For larger datasets the <code class="docutils literal notranslate"><span class="pre">tf_models</span></code> library includes some tools for preprocessing and re-serializing a dataset. See <a class="reference external" href="#re_encoding_tools">Appendix: Re-encoding a large dataset</a> for details.</p>
<div class="section" id="Encode-the-sentences">
<h4>Encode the sentences<a class="headerlink" href="#Encode-the-sentences" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The model expects its two inputs sentences to be concatenated together. This input is expected to start with a <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> “This is a classification problem” token, and each sentence should end with a <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> “Separator” token:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Start by encoding all the sentences while appending a <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> token, and packing them into ragged-tensors:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
   <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
   <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">sentence1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="n">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">glue_train</span><span class="p">[</span><span class="s2">&quot;sentence1&quot;</span><span class="p">]])</span>
<span class="n">sentence2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="n">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">glue_train</span><span class="p">[</span><span class="s2">&quot;sentence2&quot;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sentence1 shape:&quot;</span><span class="p">,</span> <span class="n">sentence1</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sentence2 shape:&quot;</span><span class="p">,</span> <span class="n">sentence2</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>Now prepend a <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token, and concatenate the ragged tensors to form a single <code class="docutils literal notranslate"><span class="pre">input_word_ids</span></code> tensor for each example. <code class="docutils literal notranslate"><span class="pre">RaggedTensor.to_tensor()</span></code> zero pads to the longest sequence.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">])]</span><span class="o">*</span><span class="n">sentence1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="bp">cls</span><span class="p">,</span> <span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">input_word_ids</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Mask-and-input-type">
<h4>Mask and input type<a class="headerlink" href="#Mask-and-input-type" title="Enlazar permanentemente con este título">¶</a></h4>
<p>The model expects two additional inputs:</p>
<ul class="simple">
<li><p>The input mask</p></li>
<li><p>The input type</p></li>
</ul>
<p>The mask allows the model to cleanly differentiate between the content and the padding. The mask has the same shape as the <code class="docutils literal notranslate"><span class="pre">input_word_ids</span></code>, and contains a <code class="docutils literal notranslate"><span class="pre">1</span></code> anywhere the <code class="docutils literal notranslate"><span class="pre">input_word_ids</span></code> is not padding.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">input_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">input_mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The “input type” also has the same shape, but inside the non-padded region, contains a <code class="docutils literal notranslate"><span class="pre">0</span></code> or a <code class="docutils literal notranslate"><span class="pre">1</span></code> indicating which sentence the token is a part of.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">type_cls</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
<span class="n">type_s1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sentence1</span><span class="p">)</span>
<span class="n">type_s2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sentence2</span><span class="p">)</span>
<span class="n">input_type_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">type_cls</span><span class="p">,</span> <span class="n">type_s1</span><span class="p">,</span> <span class="n">type_s2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">input_type_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Put-it-all-together">
<h4>Put it all together<a class="headerlink" href="#Put-it-all-together" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Collect the above text parsing code into a single function, and apply it to each split of the <code class="docutils literal notranslate"><span class="pre">glue/mrpc</span></code> dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
   <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
   <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">bert_encode</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
  <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">[</span><span class="s2">&quot;sentence1&quot;</span><span class="p">])</span>

  <span class="n">sentence1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
      <span class="n">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">[</span><span class="s2">&quot;sentence1&quot;</span><span class="p">])])</span>
  <span class="n">sentence2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span>
      <span class="n">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
       <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">[</span><span class="s2">&quot;sentence2&quot;</span><span class="p">])])</span>

  <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">])]</span><span class="o">*</span><span class="n">sentence1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="bp">cls</span><span class="p">,</span> <span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">input_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">()</span>

  <span class="n">type_cls</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
  <span class="n">type_s1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sentence1</span><span class="p">)</span>
  <span class="n">type_s2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sentence2</span><span class="p">)</span>
  <span class="n">input_type_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">type_cls</span><span class="p">,</span> <span class="n">type_s1</span><span class="p">,</span> <span class="n">type_s2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">()</span>

  <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s1">&#39;input_word_ids&#39;</span><span class="p">:</span> <span class="n">input_word_ids</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(),</span>
      <span class="s1">&#39;input_mask&#39;</span><span class="p">:</span> <span class="n">input_mask</span><span class="p">,</span>
      <span class="s1">&#39;input_type_ids&#39;</span><span class="p">:</span> <span class="n">input_type_ids</span><span class="p">}</span>

  <span class="k">return</span> <span class="n">inputs</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">glue_train</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">glue</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">glue_train_labels</span> <span class="o">=</span> <span class="n">glue</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>

<span class="n">glue_validation</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">glue</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">glue_validation_labels</span> <span class="o">=</span> <span class="n">glue</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>

<span class="n">glue_test</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">glue</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">glue_test_labels</span>  <span class="o">=</span> <span class="n">glue</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">][</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>Each subset of the data has been converted to a dictionary of features, and a set of labels. Each feature in the input dictionary has the same shape, and the number of labels should match:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">glue_train</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">key</span><span class="si">:</span><span class="s1">15s</span><span class="si">}</span><span class="s1"> shape: </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;glue_train_labels shape: </span><span class="si">{</span><span class="n">glue_train_labels</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="The-model">
<h2>The model<a class="headerlink" href="#The-model" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="section" id="Build-the-model">
<h3>Build the model<a class="headerlink" href="#Build-the-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The first step is to download the configuration for the pre-trained model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="n">bert_config_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">,</span> <span class="s2">&quot;bert_config.json&quot;</span><span class="p">)</span>
<span class="n">config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">bert_config_file</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

<span class="n">bert_config</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">BertConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_dict</span><span class="p">)</span>

<span class="n">config_dict</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">config</span></code> defines the core BERT Model, which is a Keras model to predict the outputs of <code class="docutils literal notranslate"><span class="pre">num_classes</span></code> from the inputs with maximum sequence length <code class="docutils literal notranslate"><span class="pre">max_seq_length</span></code>.</p>
<p>This function returns both the encoder and the classifier.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">bert_classifier</span><span class="p">,</span> <span class="n">bert_encoder</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">bert_models</span><span class="o">.</span><span class="n">classifier_model</span><span class="p">(</span>
    <span class="n">bert_config</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The classifier has three inputs and one output:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">bert_classifier</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">48</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Run it on a test batch of data 10 examples from the training set. The output is the logits for the two classes:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">glue_batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">glue_train</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">bert_classifier</span><span class="p">(</span>
    <span class="n">glue_batch</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">TransformerEncoder</span></code> in the center of the classifier above <strong>is</strong> the <code class="docutils literal notranslate"><span class="pre">bert_encoder</span></code>.</p>
<p>Inspecting the encoder, we see its stack of <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> layers connected to those same three inputs:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">bert_encoder</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">48</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Restore-the-encoder-weights">
<h3>Restore the encoder weights<a class="headerlink" href="#Restore-the-encoder-weights" title="Enlazar permanentemente con este título">¶</a></h3>
<p>When built the encoder is randomly initialized. Restore the encoder’s weights from the checkpoint:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">bert_encoder</span><span class="p">)</span>
<span class="n">checkpoint</span><span class="o">.</span><span class="n">read</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">,</span> <span class="s1">&#39;bert_model.ckpt&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">assert_consumed</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Note: The pretrained <code class="docutils literal notranslate"><span class="pre">TransformerEncoder</span></code> is also available on <a class="reference external" href="https://tensorflow.org/hub">TensorFlow Hub</a>. See the <a class="reference external" href="#hub_bert">Hub appendix</a> for details.</p>
</div>
<div class="section" id="Set-up-the-optimizer">
<h3>Set up the optimizer<a class="headerlink" href="#Set-up-the-optimizer" title="Enlazar permanentemente con este título">¶</a></h3>
<p>BERT adopts the Adam optimizer with weight decay (aka “<a class="reference external" href="https://arxiv.org/abs/1711.05101">AdamW</a>”). It also employs a learning rate schedule that firstly warms up from 0 and then decays to 0.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Set up epochs and steps</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">eval_batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">train_data_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">glue_train_labels</span><span class="p">)</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_data_size</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_train_steps</span> <span class="o">=</span> <span class="n">steps_per_epoch</span> <span class="o">*</span> <span class="n">epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">epochs</span> <span class="o">*</span> <span class="n">train_data_size</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># creates an optimizer with learning rate schedule</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">optimization</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">(</span>
    <span class="mf">2e-5</span><span class="p">,</span> <span class="n">num_train_steps</span><span class="o">=</span><span class="n">num_train_steps</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This returns an <code class="docutils literal notranslate"><span class="pre">AdamWeightDecay</span></code> optimizer with the learning rate schedule set:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To see an example of how to customize the optimizer and it’s schedule, see the <a class="reference external" href="#optiizer_schedule">Optimizer schedule appendix</a>.</p>
</div>
<div class="section" id="Train-the-model">
<h3>Train the model<a class="headerlink" href="#Train-the-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The metric is accuracy and we use sparse categorical cross-entropy as loss.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">bert_classifier</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>

<span class="n">bert_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
      <span class="n">glue_train</span><span class="p">,</span> <span class="n">glue_train_labels</span><span class="p">,</span>
      <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">glue_validation</span><span class="p">,</span> <span class="n">glue_validation_labels</span><span class="p">),</span>
      <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
      <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now run the fine-tuned model on a custom example to see that it works.</p>
<p>Start by encoding some sentence pairs:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">my_examples</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span>
    <span class="n">glue_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;sentence1&#39;</span><span class="p">:[</span>
            <span class="s1">&#39;The rain in Spain falls mainly on the plain.&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Look I fine tuned BERT.&#39;</span><span class="p">],</span>
        <span class="s1">&#39;sentence2&#39;</span><span class="p">:[</span>
            <span class="s1">&#39;It mostly rains on the flat lands of Spain.&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Is it working? This does not match.&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The model should report class <code class="docutils literal notranslate"><span class="pre">1</span></code> “match” for the first example and class <code class="docutils literal notranslate"><span class="pre">0</span></code> “no-match” for the second:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">bert_classifier</span><span class="p">(</span><span class="n">my_examples</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">result</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">names</span><span class="p">)[</span><span class="n">result</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Save-the-model">
<h3>Save the model<a class="headerlink" href="#Save-the-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Often the goal of training a model is to <em>use</em> it for something, so export the model and then restore it to be sure that it works.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">export_dir</span><span class="o">=</span><span class="s1">&#39;./saved_model&#39;</span>
<span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">bert_classifier</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">reloaded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">export_dir</span><span class="p">)</span>
<span class="n">reloaded_result</span> <span class="o">=</span> <span class="n">reloaded</span><span class="p">([</span><span class="n">my_examples</span><span class="p">[</span><span class="s1">&#39;input_word_ids&#39;</span><span class="p">],</span>
                            <span class="n">my_examples</span><span class="p">[</span><span class="s1">&#39;input_mask&#39;</span><span class="p">],</span>
                            <span class="n">my_examples</span><span class="p">[</span><span class="s1">&#39;input_type_ids&#39;</span><span class="p">]],</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">original_result</span> <span class="o">=</span> <span class="n">bert_classifier</span><span class="p">(</span><span class="n">my_examples</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># The results are (nearly) identical:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">original_result</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reloaded_result</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Appendix">
<h2>Appendix<a class="headerlink" href="#Appendix" title="Enlazar permanentemente con este título">¶</a></h2>
<blockquote>
<div><p>### Re-encoding a large dataset</p>
</div></blockquote>
<p>This tutorial you re-encoded the dataset in memory, for clarity.</p>
<p>This was only possible because <code class="docutils literal notranslate"><span class="pre">glue/mrpc</span></code> is a very small dataset. To deal with larger datasets <code class="docutils literal notranslate"><span class="pre">tf_models</span></code> library includes some tools for processing and re-encoding a dataset for efficient training.</p>
<p>The first step is to describe which features of the dataset should be transformed:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">processor</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">classifier_data_lib</span><span class="o">.</span><span class="n">TfdsProcessor</span><span class="p">(</span>
    <span class="n">tfds_params</span><span class="o">=</span><span class="s2">&quot;dataset=glue/mrpc,text_key=sentence1,text_b_key=sentence2&quot;</span><span class="p">,</span>
    <span class="n">process_text_fn</span><span class="o">=</span><span class="n">bert</span><span class="o">.</span><span class="n">tokenization</span><span class="o">.</span><span class="n">convert_to_unicode</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Then apply the transformation to generate new TFRecord files.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Set up output of training and evaluation Tensorflow dataset</span>
<span class="n">train_data_output_path</span><span class="o">=</span><span class="s2">&quot;./mrpc_train.tf_record&quot;</span>
<span class="n">eval_data_output_path</span><span class="o">=</span><span class="s2">&quot;./mrpc_eval.tf_record&quot;</span>

<span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">eval_batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Generate and save training data into a tf record file</span>
<span class="n">input_meta_data</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">classifier_data_lib</span><span class="o">.</span><span class="n">generate_tf_record_from_data_file</span><span class="p">(</span>
      <span class="n">processor</span><span class="o">=</span><span class="n">processor</span><span class="p">,</span>
      <span class="n">data_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># It is `None` because data is from tfds, not local dir.</span>
      <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">train_data_output_path</span><span class="o">=</span><span class="n">train_data_output_path</span><span class="p">,</span>
      <span class="n">eval_data_output_path</span><span class="o">=</span><span class="n">eval_data_output_path</span><span class="p">,</span>
      <span class="n">max_seq_length</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Finally create <code class="docutils literal notranslate"><span class="pre">tf.data</span></code> input pipelines from those TFRecord files:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">training_dataset</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">run_classifier</span><span class="o">.</span><span class="n">get_dataset_fn</span><span class="p">(</span>
    <span class="n">train_data_output_path</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)()</span>

<span class="n">evaluation_dataset</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">run_classifier</span><span class="o">.</span><span class="n">get_dataset_fn</span><span class="p">(</span>
    <span class="n">eval_data_output_path</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">eval_batch_size</span><span class="p">,</span>
    <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">)()</span>

</pre></div>
</div>
</div>
<p>The resulting <code class="docutils literal notranslate"><span class="pre">tf.data.Datasets</span></code> return <code class="docutils literal notranslate"><span class="pre">(features,</span> <span class="pre">labels)</span></code> pairs, as expected by <code class="docutils literal notranslate"><span class="pre">keras.Model.fit</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">training_dataset</span><span class="o">.</span><span class="n">element_spec</span>
</pre></div>
</div>
</div>
<p>If you need to modify the data loading here is some code to get you started:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">create_classifier_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_training</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates input dataset from (tf)records files for train/eval.&quot;&quot;&quot;</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFRecordDataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">decode_record</span><span class="p">(</span><span class="n">record</span><span class="p">):</span>
    <span class="n">name_to_features</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([</span><span class="n">seq_length</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
      <span class="s1">&#39;input_mask&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([</span><span class="n">seq_length</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
      <span class="s1">&#39;segment_ids&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([</span><span class="n">seq_length</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
      <span class="s1">&#39;label_ids&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">record</span><span class="p">,</span> <span class="n">name_to_features</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_select_data_from_record</span><span class="p">(</span><span class="n">record</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;input_word_ids&#39;</span><span class="p">:</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span>
        <span class="s1">&#39;input_mask&#39;</span><span class="p">:</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;input_mask&#39;</span><span class="p">],</span>
        <span class="s1">&#39;input_type_ids&#39;</span><span class="p">:</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;segment_ids&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">&#39;label_ids&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">decode_record</span><span class="p">,</span>
                        <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
      <span class="n">_select_data_from_record</span><span class="p">,</span>
      <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="n">is_training</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Set up batch sizes</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">eval_batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Return Tensorflow dataset</span>
<span class="n">training_dataset</span> <span class="o">=</span> <span class="n">create_classifier_dataset</span><span class="p">(</span>
    <span class="n">train_data_output_path</span><span class="p">,</span>
    <span class="n">input_meta_data</span><span class="p">[</span><span class="s1">&#39;max_seq_length&#39;</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">evaluation_dataset</span> <span class="o">=</span> <span class="n">create_classifier_dataset</span><span class="p">(</span>
    <span class="n">eval_data_output_path</span><span class="p">,</span>
    <span class="n">input_meta_data</span><span class="p">[</span><span class="s1">&#39;max_seq_length&#39;</span><span class="p">],</span>
    <span class="n">eval_batch_size</span><span class="p">,</span>
    <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">training_dataset</span><span class="o">.</span><span class="n">element_spec</span>
</pre></div>
</div>
</div>
<div class="section" id="TFModels-BERT-on-TFHub">
<h3>TFModels BERT on TFHub<a class="headerlink" href="#TFModels-BERT-on-TFHub" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You can get <a class="reference external" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2">the BERT model</a> off the shelf from <a class="reference external" href="https://tensorflow.org/hub">TFHub</a>. It would not be hard to add a classification head on top of this <code class="docutils literal notranslate"><span class="pre">hub.KerasLayer</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Note: 350MB download.</span>
<span class="kn">import</span> <span class="nn">tensorflow_hub</span> <span class="k">as</span> <span class="nn">hub</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hub_model_name</span> <span class="o">=</span> <span class="s2">&quot;bert_en_uncased_L-12_H-768_A-12&quot;</span> <span class="c1">#@param [&quot;bert_en_uncased_L-24_H-1024_A-16&quot;, &quot;bert_en_wwm_cased_L-24_H-1024_A-16&quot;, &quot;bert_en_uncased_L-12_H-768_A-12&quot;, &quot;bert_en_wwm_uncased_L-24_H-1024_A-16&quot;, &quot;bert_en_cased_L-24_H-1024_A-16&quot;, &quot;bert_en_cased_L-12_H-768_A-12&quot;, &quot;bert_zh_L-12_H-768_A-12&quot;, &quot;bert_multi_cased_L-12_H-768_A-12&quot;]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hub_encoder</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;https://tfhub.dev/tensorflow/</span><span class="si">{</span><span class="n">hub_model_name</span><span class="si">}</span><span class="s2">/3&quot;</span><span class="p">,</span>
                             <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Hub encoder has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">hub_encoder</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span><span class="si">}</span><span class="s2"> trainable variables&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Test run it on a batch of data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">hub_encoder</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">input_word_ids</span><span class="o">=</span><span class="n">glue_train</span><span class="p">[</span><span class="s1">&#39;input_word_ids&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],</span>
        <span class="n">input_mask</span><span class="o">=</span><span class="n">glue_train</span><span class="p">[</span><span class="s1">&#39;input_mask&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],</span>
        <span class="n">input_type_ids</span><span class="o">=</span><span class="n">glue_train</span><span class="p">[</span><span class="s1">&#39;input_type_ids&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],),</span>
    <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pooled output shape:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;pooled_output&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence output shape:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;sequence_output&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>At this point it would be simple to add a classification head yourself.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">bert_models.classifier_model</span></code> function can also build a classifier onto the encoder from TensorFlow Hub:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hub_classifier</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">modeling</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">BertClassifier</span><span class="p">(</span>
    <span class="n">bert_encoder</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">(</span>
        <span class="n">stddev</span><span class="o">=</span><span class="mf">0.02</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>The one downside to loading this model from TFHub is that the structure of internal keras layers is not restored. So it’s more difficult to inspect or modify the model. The <code class="docutils literal notranslate"><span class="pre">BertEncoder</span></code> model is now a single layer:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">hub_classifier</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">try</span><span class="p">:</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">hub_encoder</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
  <span class="k">assert</span> <span class="kc">False</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Low-level-model-building">
<h3>Low level model building<a class="headerlink" href="#Low-level-model-building" title="Enlazar permanentemente con este título">¶</a></h3>
<p>If you need a more control over the construction of the model it’s worth noting that the <code class="docutils literal notranslate"><span class="pre">classifier_model</span></code> function used earlier is really just a thin wrapper over the <code class="docutils literal notranslate"><span class="pre">nlp.modeling.networks.BertEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">nlp.modeling.models.BertClassifier</span></code> classes. Just remember that if you start modifying the architecture it may not be correct or possible to reload the pre-trained checkpoint so you’ll need to retrain from scratch.</p>
<p>Build the encoder:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">bert_encoder_config</span> <span class="o">=</span> <span class="n">config_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># You need to rename a few fields to make this work:</span>
<span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;attention_dropout_rate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bert_encoder_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;attention_probs_dropout_prob&#39;</span><span class="p">)</span>
<span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_activation</span><span class="p">(</span><span class="n">bert_encoder_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;hidden_act&#39;</span><span class="p">))</span>
<span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;dropout_rate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bert_encoder_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;hidden_dropout_prob&#39;</span><span class="p">)</span>
<span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;initializer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">(</span>
          <span class="n">stddev</span><span class="o">=</span><span class="n">bert_encoder_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;initializer_range&#39;</span><span class="p">))</span>
<span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;max_sequence_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bert_encoder_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;max_position_embeddings&#39;</span><span class="p">)</span>
<span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;num_layers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bert_encoder_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;num_hidden_layers&#39;</span><span class="p">)</span>

<span class="n">bert_encoder_config</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">manual_encoder</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">modeling</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">BertEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">bert_encoder_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Restore the weights:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">manual_encoder</span><span class="p">)</span>
<span class="n">checkpoint</span><span class="o">.</span><span class="n">read</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">,</span> <span class="s1">&#39;bert_model.ckpt&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">assert_consumed</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Test run it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">manual_encoder</span><span class="p">(</span><span class="n">my_examples</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sequence output shape:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pooled output shape:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Wrap it in a classifier:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">manual_classifier</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">modeling</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">BertClassifier</span><span class="p">(</span>
        <span class="n">bert_encoder</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;dropout_rate&#39;</span><span class="p">],</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">bert_encoder_config</span><span class="p">[</span><span class="s1">&#39;initializer&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">manual_classifier</span><span class="p">(</span><span class="n">my_examples</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Optimizers-and-schedules">
<h3>Optimizers and schedules<a class="headerlink" href="#Optimizers-and-schedules" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The optimizer used to train the model was created using the <code class="docutils literal notranslate"><span class="pre">nlp.optimization.create_optimizer</span></code> function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">optimization</span><span class="o">.</span><span class="n">create_optimizer</span><span class="p">(</span>
    <span class="mf">2e-5</span><span class="p">,</span> <span class="n">num_train_steps</span><span class="o">=</span><span class="n">num_train_steps</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>That high level wrapper sets up the learning rate schedules and the optimizer.</p>
<p>The base learning rate schedule used here is a linear decay to zero over the training run:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">eval_batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">train_data_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">glue_train_labels</span><span class="p">)</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_data_size</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">num_train_steps</span> <span class="o">=</span> <span class="n">steps_per_epoch</span> <span class="o">*</span> <span class="n">epochs</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">decay_schedule</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">PolynomialDecay</span><span class="p">(</span>
      <span class="n">initial_learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
      <span class="n">decay_steps</span><span class="o">=</span><span class="n">num_train_steps</span><span class="p">,</span>
      <span class="n">end_learning_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">decay_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train_steps</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<p>This, in turn is wrapped in a <code class="docutils literal notranslate"><span class="pre">WarmUp</span></code> schedule that linearly increases the learning rate to the target value over the first 10% of training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">num_train_steps</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="n">warmup_schedule</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">optimization</span><span class="o">.</span><span class="n">WarmUp</span><span class="p">(</span>
        <span class="n">initial_learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
        <span class="n">decay_schedule_fn</span><span class="o">=</span><span class="n">decay_schedule</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">)</span>

<span class="c1"># The warmup overshoots, because it warms up to the `initial_learning_rate`</span>
<span class="c1"># following the original implementation. You can set</span>
<span class="c1"># `initial_learning_rate=decay_schedule(warmup_steps)` if you don&#39;t like the</span>
<span class="c1"># overshoot.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">warmup_schedule</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train_steps</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<p>Then create the <code class="docutils literal notranslate"><span class="pre">nlp.optimization.AdamWeightDecay</span></code> using that schedule, configured for the BERT model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">optimization</span><span class="o">.</span><span class="n">AdamWeightDecay</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">warmup_schedule</span><span class="p">,</span>
        <span class="n">weight_decay_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">exclude_from_weight_decay</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;LayerNorm&#39;</span><span class="p">,</span> <span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>