

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Text generation with an RNN &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" />
    <link rel="next" title="Neural machine translation with attention" href="1-08_nmt_with_attention.html" />
    <link rel="prev" title="Fine-tuning a BERT model" href="1-06_fine_tuning_bert.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-01">Sesión 01</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-02">Sesión 02</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-03">Sesión 03</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-04">Sesión 04</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-05">Sesión 05</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-06">Sesión 06</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-07">Sesión 07</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-08">Sesión 08</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-09">Sesión 09</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-10">Sesión 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-11">Sesión 11</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-12">Sesión 12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-13">Sesión 13</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-14">Sesión 14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-15">Sesión 15</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-16">Sesión 16</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a> &raquo;</li>
        
      <li>Text generation with an RNN</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Text/1-07_text_generation.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Text-generation-with-an-RNN">
<h1>Text generation with an RNN<a class="headerlink" href="#Text-generation-with-an-RNN" title="Enlazar permanentemente con este título">¶</a></h1>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>88850719d1224eb7a326dc844542a2ff|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>c0b4d132d17f4761b240ed65f4f659d6|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>ff61753c98d84f2ebe04e7172ab51cc2|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>cd1900995007443398856bc6130766e5|Download notebook</p>
</td></table><p>This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare’s writing from Andrej Karpathy’s <a class="reference external" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. Given a sequence of characters from this data (“Shakespear”), train a model to predict the next character in the sequence (“e”). Longer sequences of text can be generated by calling the model repeatedly.</p>
<p>Note: Enable GPU acceleration to execute this notebook faster. In Colab: <em>Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; GPU</em>.</p>
<p>This tutorial includes runnable code implemented using <a class="reference external" href="https://www.tensorflow.org/guide/keras/sequential_model">tf.keras</a> and <a class="reference external" href="https://www.tensorflow.org/guide/eager">eager execution</a>. The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt “Q”:</p>
<pre>
QUEENE:
I had thought thou hadst a Roman; for the oracle,
Thus by All bids the man against the word,
Which are so weak of care, by old care done;
Your children were in your holy love,
And the precipitation through the bleeding throne.

BISHOP OF ELY:
Marry, and will, my lord, to weep in such a one were prettiest;
Yet now I was adopted heir
Of the world's lamentable day,
To watch the next way with his father with his face?

ESCALUS:
The cause why then we are all resolved more sons.

VOLUMNIA:
O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,
And love and pale as any will to that word.

QUEEN ELIZABETH:
But how long have I heard the soul for this world,
And show his hands of life be proved to stand.

PETRUCHIO:
I say he look'd on, if I must be content
To stay him from the fatal of our country's bliss.
His lordship pluck'd from this sentence then for prey,
And then let us twain, being the moon,
were she such a case as fills m
</pre><p>While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:</p>
<ul class="simple">
<li><p>The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.</p></li>
<li><p>The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.</p></li>
<li><p>As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.</p></li>
</ul>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="section" id="Import-TensorFlow-and-other-libraries">
<h3>Import TensorFlow and other libraries<a class="headerlink" href="#Import-TensorFlow-and-other-libraries" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Download-the-Shakespeare-dataset">
<h3>Download the Shakespeare dataset<a class="headerlink" href="#Download-the-Shakespeare-dataset" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Change the following line to run this code on your own data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">path_to_file</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s1">&#39;shakespeare.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Read-the-data">
<h3>Read the data<a class="headerlink" href="#Read-the-data" title="Enlazar permanentemente con este título">¶</a></h3>
<p>First, look in the text:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Read, then decode for py2 compat.</span>
<span class="n">text</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">path_to_file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="c1"># length of text is the number of characters in it</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Length of text: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s1"> characters&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Take a look at the first 250 characters in text</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="mi">250</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># The unique characters in the file</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s1"> unique characters&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Process-the-text">
<h2>Process the text<a class="headerlink" href="#Process-the-text" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="section" id="Vectorize-the-text">
<h3>Vectorize the text<a class="headerlink" href="#Vectorize-the-text" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Before training, you need to convert the strings to a numerical representation.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">preprocessing.StringLookup</span></code> layer can convert each character into a numeric ID. It just needs the text to be split into tokens first.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">example_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;abcdefg&#39;</span><span class="p">,</span> <span class="s1">&#39;xyz&#39;</span><span class="p">]</span>

<span class="n">chars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">unicode_split</span><span class="p">(</span><span class="n">example_texts</span><span class="p">,</span> <span class="n">input_encoding</span><span class="o">=</span><span class="s1">&#39;UTF-8&#39;</span><span class="p">)</span>
<span class="n">chars</span>
</pre></div>
</div>
</div>
<p>Now create the <code class="docutils literal notranslate"><span class="pre">preprocessing.StringLookup</span></code> layer:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ids_from_chars</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StringLookup</span><span class="p">(</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>It converts form tokens to character IDs, padding with <code class="docutils literal notranslate"><span class="pre">0</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ids</span> <span class="o">=</span> <span class="n">ids_from_chars</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="n">ids</span>
</pre></div>
</div>
</div>
<p>Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use <code class="docutils literal notranslate"><span class="pre">preprocessing.StringLookup(...,</span> <span class="pre">invert=True)</span></code>.</p>
<p>Note: Here instead of passing the original vocabulary generated with <code class="docutils literal notranslate"><span class="pre">sorted(set(text))</span></code> use the <code class="docutils literal notranslate"><span class="pre">get_vocabulary()</span></code> method of the <code class="docutils literal notranslate"><span class="pre">preprocessing.StringLookup</span></code> layer so that the padding and <code class="docutils literal notranslate"><span class="pre">[UNK]</span></code> tokens are set the same way.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">chars_from_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">StringLookup</span><span class="p">(</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">ids_from_chars</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">(),</span> <span class="n">invert</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This layer recovers the characters from the vectors of IDs, and returns them as a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> of characters:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">chars</span> <span class="o">=</span> <span class="n">chars_from_ids</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
<span class="n">chars</span>
</pre></div>
</div>
</div>
<p>You can <code class="docutils literal notranslate"><span class="pre">tf.strings.reduce_join</span></code> to join the characters back into strings.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">reduce_join</span><span class="p">(</span><span class="n">chars</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">text_from_ids</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">reduce_join</span><span class="p">(</span><span class="n">chars_from_ids</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="The-prediction-task">
<h3>The prediction task<a class="headerlink" href="#The-prediction-task" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Given a character, or a sequence of characters, what is the most probable next character? This is the task you’re training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.</p>
<p>Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?</p>
</div>
<div class="section" id="Create-training-examples-and-targets">
<h3>Create training examples and targets<a class="headerlink" href="#Create-training-examples-and-targets" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Next divide the text into example sequences. Each input sequence will contain <code class="docutils literal notranslate"><span class="pre">seq_length</span></code> characters from the text.</p>
<p>For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.</p>
<p>So break the text into chunks of <code class="docutils literal notranslate"><span class="pre">seq_length+1</span></code>. For example, say <code class="docutils literal notranslate"><span class="pre">seq_length</span></code> is 4 and our text is “Hello”. The input sequence would be “Hell”, and the target sequence “ello”.</p>
<p>To do this first use the <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.from_tensor_slices</span></code> function to convert the text vector into a stream of character indices.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">all_ids</span> <span class="o">=</span> <span class="n">ids_from_chars</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">unicode_split</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="s1">&#39;UTF-8&#39;</span><span class="p">))</span>
<span class="n">all_ids</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ids_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">all_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">ids_dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chars_from_ids</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">seq_length</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">examples_per_epoch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">//</span><span class="p">(</span><span class="n">seq_length</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">batch</span></code> method lets you easily convert these individual characters to sequences of the desired size.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sequences</span> <span class="o">=</span> <span class="n">ids_dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">seq_length</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">chars_from_ids</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>It’s easier to see what this is doing if you join the tokens back into strings:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">text_from_ids</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>For training you’ll need a dataset of <code class="docutils literal notranslate"><span class="pre">(input,</span> <span class="pre">label)</span></code> pairs. Where <code class="docutils literal notranslate"><span class="pre">input</span></code> and <code class="docutils literal notranslate"><span class="pre">label</span></code> are sequences. At each time step the input is the current character and the label is the next character.</p>
<p>Here’s a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">split_input_target</span><span class="p">(</span><span class="n">sequence</span><span class="p">):</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">sequence</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target_text</span> <span class="o">=</span> <span class="n">sequence</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">target_text</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">split_input_target</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s2">&quot;Tensorflow&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sequences</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">split_input_target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">input_example</span><span class="p">,</span> <span class="n">target_example</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input :&quot;</span><span class="p">,</span> <span class="n">text_from_ids</span><span class="p">(</span><span class="n">input_example</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Target:&quot;</span><span class="p">,</span> <span class="n">text_from_ids</span><span class="p">(</span><span class="n">target_example</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Create-training-batches">
<h3>Create training batches<a class="headerlink" href="#Create-training-batches" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You used <code class="docutils literal notranslate"><span class="pre">tf.data</span></code> to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Batch size</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Buffer size to shuffle the dataset</span>
<span class="c1"># (TF data is designed to work with possibly infinite sequences,</span>
<span class="c1"># so it doesn&#39;t attempt to shuffle the entire sequence in memory. Instead,</span>
<span class="c1"># it maintains a buffer in which it shuffles elements).</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">))</span>

<span class="n">dataset</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Build-The-Model">
<h2>Build The Model<a class="headerlink" href="#Build-The-Model" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This section defines the model as a <code class="docutils literal notranslate"><span class="pre">keras.Model</span></code> subclass (For details see <a class="reference external" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a>).</p>
<p>This model has three layers:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Embedding</span></code>: The input layer. A trainable lookup table that will map each character-ID to a vector with <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code> dimensions;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.layers.GRU</span></code>: A type of RNN with size <code class="docutils literal notranslate"><span class="pre">units=rnn_units</span></code> (You can also use an LSTM layer here.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code>: The output layer, with <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Length of the vocabulary in chars</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="c1"># The embedding dimension</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">256</span>

<span class="c1"># Number of RNN units</span>
<span class="n">rnn_units</span> <span class="o">=</span> <span class="mi">1024</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">rnn_units</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">rnn_units</span><span class="p">,</span>
                                   <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">states</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_state</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">states</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span>
    <span class="c1"># Be sure the vocabulary size matches the `StringLookup` layers.</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ids_from_chars</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()),</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
    <span class="n">rnn_units</span><span class="o">=</span><span class="n">rnn_units</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:</p>
<p><img alt="A drawing of the data passing through the model" src="../../../../_images/text_generation_training.png" /></p>
<p>Note: For training you could use a <code class="docutils literal notranslate"><span class="pre">keras.Sequential</span></code> model here. To generate text later you’ll need to manage the RNN’s internal state. It’s simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the <a class="reference external" href="https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse">Keras RNN guide</a>.</p>
</div>
<div class="section" id="Try-the-model">
<h2>Try the model<a class="headerlink" href="#Try-the-model" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Now run the model to see that it behaves as expected.</p>
<p>First check the shape of the output:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">input_example_batch</span><span class="p">,</span> <span class="n">target_example_batch</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">example_batch_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_example_batch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">example_batch_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;# (batch_size, sequence_length, vocab_size)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In the above example the sequence length of the input is <code class="docutils literal notranslate"><span class="pre">100</span></code> but the model can be run on inputs of any length:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.</p>
<p>Note: It is important to <em>sample</em> from this distribution as taking the <em>argmax</em> of the distribution can easily get the model stuck in a loop.</p>
<p>Try it for the first example in the batch:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sampled_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">example_batch_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sampled_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">sampled_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>This gives us, at each timestep, a prediction of the next character index:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sampled_indices</span>
</pre></div>
</div>
</div>
<p>Decode these to see the text predicted by this untrained model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">text_from_ids</span><span class="p">(</span><span class="n">input_example_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Next Char Predictions:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">text_from_ids</span><span class="p">(</span><span class="n">sampled_indices</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Train-the-model">
<h2>Train the model<a class="headerlink" href="#Train-the-model" title="Enlazar permanentemente con este título">¶</a></h2>
<p>At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.</p>
<div class="section" id="Attach-an-optimizer,-and-a-loss-function">
<h3>Attach an optimizer, and a loss function<a class="headerlink" href="#Attach-an-optimizer,-and-a-loss-function" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The standard <code class="docutils literal notranslate"><span class="pre">tf.keras.losses.sparse_categorical_crossentropy</span></code> loss function works in this case because it is applied across the last dimension of the predictions.</p>
<p>Because your model returns logits, you need to set the <code class="docutils literal notranslate"><span class="pre">from_logits</span></code> flag.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">example_batch_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">target_example_batch</span><span class="p">,</span> <span class="n">example_batch_predictions</span><span class="p">)</span>
<span class="n">mean_loss</span> <span class="o">=</span> <span class="n">example_batch_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction shape: &quot;</span><span class="p">,</span> <span class="n">example_batch_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot; # (batch_size, sequence_length, vocab_size)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean loss:        &quot;</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A newly initialized model shouldn’t be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mean_loss</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Configure the training procedure using the <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.compile</span></code> method. Use <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code> with default arguments and the loss function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Configure-checkpoints">
<h3>Configure checkpoints<a class="headerlink" href="#Configure-checkpoints" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Use a <code class="docutils literal notranslate"><span class="pre">tf.keras.callbacks.ModelCheckpoint</span></code> to ensure that checkpoints are saved during training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Directory where the checkpoints will be saved</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="s1">&#39;./training_checkpoints&#39;</span>
<span class="c1"># Name of the checkpoint files</span>
<span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;ckpt_</span><span class="si">{epoch}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
    <span class="n">filepath</span><span class="o">=</span><span class="n">checkpoint_prefix</span><span class="p">,</span>
    <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Execute-the-training">
<h3>Execute the training<a class="headerlink" href="#Execute-the-training" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_callback</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Generate-text">
<h2>Generate text<a class="headerlink" href="#Generate-text" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The simplest way to generate text with this model is to run it in a loop, and keep track of the model’s internal state as you execute it.</p>
<p><img alt="To generate text the model’s output is fed back to the input" src="../../../../_images/text_generation_sampling.png" /></p>
<p>Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.</p>
<p>The following makes a single step prediction:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">OneStep</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">chars_from_ids</span><span class="p">,</span> <span class="n">ids_from_chars</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">chars_from_ids</span> <span class="o">=</span> <span class="n">chars_from_ids</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ids_from_chars</span> <span class="o">=</span> <span class="n">ids_from_chars</span>

    <span class="c1"># Create a mask to prevent &quot;&quot; or &quot;[UNK]&quot; from being generated.</span>
    <span class="n">skip_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_from_chars</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;[UNK]&#39;</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">sparse_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">SparseTensor</span><span class="p">(</span>
        <span class="c1"># Put a -inf at each bad index.</span>
        <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">skip_ids</span><span class="p">),</span>
        <span class="n">indices</span><span class="o">=</span><span class="n">skip_ids</span><span class="p">,</span>
        <span class="c1"># Match the shape to the vocabulary</span>
        <span class="n">dense_shape</span><span class="o">=</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">ids_from_chars</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">())])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prediction_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">to_dense</span><span class="p">(</span><span class="n">sparse_mask</span><span class="p">)</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">generate_one_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Convert strings to token IDs.</span>
    <span class="n">input_chars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">unicode_split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;UTF-8&#39;</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_from_chars</span><span class="p">(</span><span class="n">input_chars</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">()</span>

    <span class="c1"># Run the model.</span>
    <span class="c1"># predicted_logits.shape is [batch, char, next_char_logits]</span>
    <span class="n">predicted_logits</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">,</span>
                                          <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Only use the last prediction.</span>
    <span class="n">predicted_logits</span> <span class="o">=</span> <span class="n">predicted_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">predicted_logits</span> <span class="o">=</span> <span class="n">predicted_logits</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>
    <span class="c1"># Apply the prediction mask: prevent &quot;&quot; or &quot;[UNK]&quot; from being generated.</span>
    <span class="n">predicted_logits</span> <span class="o">=</span> <span class="n">predicted_logits</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">prediction_mask</span>

    <span class="c1"># Sample the output logits to generate token IDs.</span>
    <span class="n">predicted_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">predicted_logits</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">predicted_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">predicted_ids</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Convert from token ids to characters</span>
    <span class="n">predicted_chars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chars_from_ids</span><span class="p">(</span><span class="n">predicted_ids</span><span class="p">)</span>

    <span class="c1"># Return the characters and model state.</span>
    <span class="k">return</span> <span class="n">predicted_chars</span><span class="p">,</span> <span class="n">states</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">one_step_model</span> <span class="o">=</span> <span class="n">OneStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">chars_from_ids</span><span class="p">,</span> <span class="n">ids_from_chars</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Run it in a loop to generate some text. Looking at the generated text, you’ll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">states</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">next_char</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s1">&#39;ROMEO:&#39;</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">next_char</span><span class="p">]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">next_char</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">one_step_model</span><span class="o">.</span><span class="n">generate_one_step</span><span class="p">(</span><span class="n">next_char</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>
  <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_char</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">),</span> <span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Run time:&#39;</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The easiest thing you can do to improve the results is to train it for longer (try <code class="docutils literal notranslate"><span class="pre">EPOCHS</span> <span class="pre">=</span> <span class="pre">30</span></code>).</p>
<p>You can also experiment with a different start string, try adding another RNN layer to improve the model’s accuracy, or adjust the temperature parameter to generate more or less random predictions.</p>
<p>If you want the model to generate text <em>faster</em> the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">states</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">next_char</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s1">&#39;ROMEO:&#39;</span><span class="p">,</span> <span class="s1">&#39;ROMEO:&#39;</span><span class="p">,</span> <span class="s1">&#39;ROMEO:&#39;</span><span class="p">,</span> <span class="s1">&#39;ROMEO:&#39;</span><span class="p">,</span> <span class="s1">&#39;ROMEO:&#39;</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">next_char</span><span class="p">]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">next_char</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">one_step_model</span><span class="o">.</span><span class="n">generate_one_step</span><span class="p">(</span><span class="n">next_char</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>
  <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_char</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Run time:&#39;</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Export-the-generator">
<h2>Export the generator<a class="headerlink" href="#Export-the-generator" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This single-step model can easily be <a class="reference external" href="https://www.tensorflow.org/guide/saved_model">saved and restored</a>, allowing you to use it anywhere a <code class="docutils literal notranslate"><span class="pre">tf.saved_model</span></code> is accepted.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">one_step_model</span><span class="p">,</span> <span class="s1">&#39;one_step&#39;</span><span class="p">)</span>
<span class="n">one_step_reloaded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;one_step&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">states</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">next_char</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="s1">&#39;ROMEO:&#39;</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">next_char</span><span class="p">]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">next_char</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">one_step_reloaded</span><span class="o">.</span><span class="n">generate_one_step</span><span class="p">(</span><span class="n">next_char</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>
  <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_char</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Advanced:-Customized-Training">
<h2>Advanced: Customized Training<a class="headerlink" href="#Advanced:-Customized-Training" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.</p>
<p>So now that you’ve seen how to run the model manually next you’ll implement the training loop. This gives a starting point if, for example, you want to implement <em>curriculum learning</em> to help stabilize the model’s open-loop output.</p>
<p>The most important part of a custom training loop is the train step function.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> to track the gradients. You can learn more about this approach by reading the <a class="reference external" href="https://www.tensorflow.org/guide/eager">eager execution guide</a>.</p>
<p>The basic procedure is:</p>
<ol class="arabic simple">
<li><p>Execute the model and calculate the loss under a <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code>.</p></li>
<li><p>Calculate the updates and apply them to the model using the optimizer.</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">CustomTraining</span><span class="p">(</span><span class="n">MyModel</span><span class="p">):</span>
  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
      <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
          <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
      <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

      <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
</pre></div>
</div>
</div>
<p>The above implementation of the <code class="docutils literal notranslate"><span class="pre">train_step</span></code> method follows <cite>Keras’ ``train_step`</cite> conventions &lt;<a class="reference external" href="https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit">https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit</a>&gt;`__. This is optional, but it allows you to change the behavior of the train step and still use keras’ <code class="docutils literal notranslate"><span class="pre">Model.compile</span></code> and <code class="docutils literal notranslate"><span class="pre">Model.fit</span></code> methods.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">CustomTraining</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ids_from_chars</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()),</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
    <span class="n">rnn_units</span><span class="o">=</span><span class="n">rnn_units</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(),</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Or if you need more control, you can write your own complete custom training loop:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">mean</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">batch_n</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="n">logs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_step</span><span class="p">([</span><span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">])</span>
        <span class="n">mean</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">batch_n</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">template</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> Batch </span><span class="si">{</span><span class="n">batch_n</span><span class="si">}</span><span class="s2"> Loss </span><span class="si">{</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

    <span class="c1"># saving (checkpoint) the model every 5 epochs</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="n">checkpoint_prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> Loss: </span><span class="si">{</span><span class="n">mean</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Time taken for 1 epoch </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> sec&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="n">checkpoint_prefix</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>