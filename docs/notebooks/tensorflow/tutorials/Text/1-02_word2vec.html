

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Word2Vec &mdash; documentación de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Índice" href="../../../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../fundamentos-de-analitica/index.html">Fundamentos de Analítica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-de-grandes-datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica-predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../productos-de-datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Word2Vec</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/notebooks/tensorflow/tutorials/Text/1-02_word2vec.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Word2Vec">
<h1>Word2Vec<a class="headerlink" href="#Word2Vec" title="Enlazar permanentemente con este título">¶</a></h1>
<p>Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.</p>
<p>Note: This tutorial is based on <a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> and <a class="reference external" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>. It is not an exact implementation of the papers. Rather, it is intended to illustrate the key ideas.</p>
<p>These papers proposed two methods for learning representations of words:</p>
<ul class="simple">
<li><p><strong>Continuous Bag-of-Words Model</strong> which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.</p></li>
<li><p><strong>Continuous Skip-gram Model</strong> which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.</p></li>
</ul>
<p>You’ll use the skip-gram approach in this tutorial. First, you’ll explore skip-grams and other concepts using a single sentence for illustration. Next, you’ll train your own Word2Vec model on a small dataset. This tutorial also contains code to export the trained embeddings and visualize them in the <a class="reference external" href="http://projector.tensorflow.org/">TensorFlow Embedding Projector</a>.</p>
<div class="section" id="Skip-gram-and-Negative-Sampling">
<h2>Skip-gram and Negative Sampling<a class="headerlink" href="#Skip-gram-and-Negative-Sampling" title="Enlazar permanentemente con este título">¶</a></h2>
<p>While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of <code class="docutils literal notranslate"><span class="pre">(target_word,</span> <span class="pre">context_word)</span></code> where <code class="docutils literal notranslate"><span class="pre">context_word</span></code> appears in the neighboring context of <code class="docutils literal notranslate"><span class="pre">target_word</span></code>.</p>
<p>Consider the following sentence of 8 words. &gt; The wide road shimmered in the hot sun.</p>
<p>The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a <code class="docutils literal notranslate"><span class="pre">target_word</span></code> that can be considered <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">word</span></code>. Take a look at this table of skip-grams for target words based on different window sizes.</p>
<p>Note: For this tutorial, a window size of <em>n</em> implies n words on each side with a total window span of 2*n+1 words across a word.</p>
<p><img alt="word2vec_skipgrams" src="../../../../_images/word2vec_skipgram.png" /></p>
<p>The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words <em>w1, w2, … wT</em>, the objective can be written as the average log probability</p>
<p><img alt="word2vec_skipgram_objective" src="../../../../_images/word2vec_skipgram_objective.png" /></p>
<p>where <code class="docutils literal notranslate"><span class="pre">c</span></code> is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.</p>
<p><img alt="word2vec_full_softmax" src="../../../../_images/word2vec_full_softmax.png" /></p>
<p>where <em>v</em> and <em>v’</em> are target and context vector representations of words and <em>W</em> is vocabulary size.</p>
<p>Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words which is often large (105-107) terms.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss">Noise Contrastive Estimation</a> loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modelling the word distribution, NCE loss can be <a class="reference external" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">simplified</a> to use negative sampling.</p>
<p>The simplified negative sampling objective for a target word is to distinguish the context word from <em>num_ns</em> negative samples drawn from noise distribution <em>Pn(w)</em> of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and <em>num_ns</em> negative samples.</p>
<p>A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the <code class="docutils literal notranslate"><span class="pre">window_size</span></code> neighborhood of the target_word. For the example sentence, these are few potential negative samples (when <code class="docutils literal notranslate"><span class="pre">window_size</span></code> is 2).</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(hot, shimmered)
(wide, hot)
(wide, sun)
</pre></div>
</div>
<p>In the next section, you’ll generate skip-grams and negative samples for a single sentence. You’ll also learn about subsampling techniques and train a classification model for positive and negative training examples later in the tutorial.</p>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dot</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental.preprocessing</span> <span class="kn">import</span> <span class="n">TextVectorization</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the TensorBoard notebook extension</span>
<span class="o">%</span><span class="k">load_ext</span> tensorboard
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">AUTOTUNE</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span>
</pre></div>
</div>
</div>
<div class="section" id="Vectorize-an-example-sentence">
<h3>Vectorize an example sentence<a class="headerlink" href="#Vectorize-an-example-sentence" title="Enlazar permanentemente con este título">¶</a></h3>
<div class="line-block">
<div class="line">Consider the following sentence:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">wide</span> <span class="pre">road</span> <span class="pre">shimmered</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">hot</span> <span class="pre">sun.</span></code></div>
</div>
<p>Tokenize the sentence:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;The wide road shimmered in the hot sun&quot;</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Create a vocabulary to save mappings from tokens to integer indices.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">vocab</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="p">{},</span> <span class="mi">1</span>  <span class="c1"># start indexing from 1</span>
<span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;&lt;pad&gt;&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># add a padding token</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
    <span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Create an inverse vocabulary to save mappings from integer indices to tokens.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">inverse_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Vectorize your sentence.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">example_sequence</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">example_sequence</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generate-skip-grams-from-one-sentence">
<h3>Generate skip-grams from one sentence<a class="headerlink" href="#Generate-skip-grams-from-one-sentence" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.preprocessing.sequence</span></code> module provides useful functions that simplify data preparation for Word2Vec. You can use the <code class="docutils literal notranslate"><span class="pre">tf.keras.preprocessing.sequence.skipgrams</span></code> to generate skip-gram pairs from the <code class="docutils literal notranslate"><span class="pre">example_sequence</span></code> with a given <code class="docutils literal notranslate"><span class="pre">window_size</span></code> from tokens in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">vocab_size)</span></code>.</p>
<p>Note: <code class="docutils literal notranslate"><span class="pre">negative_samples</span></code> is set to <code class="docutils literal notranslate"><span class="pre">0</span></code> here as batching negative samples generated by this function requires a bit of code. You will use another function to perform negative sampling in the next section.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">window_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">positive_skip_grams</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">skipgrams</span><span class="p">(</span>
      <span class="n">example_sequence</span><span class="p">,</span>
      <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
      <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span>
      <span class="n">negative_samples</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">positive_skip_grams</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Take a look at few positive skip-grams.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">target</span><span class="p">,</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">positive_skip_grams</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">): (</span><span class="si">{</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">target</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">context</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Negative-sampling-for-one-skip-gram">
<h3>Negative sampling for one skip-gram<a class="headerlink" href="#Negative-sampling-for-one-skip-gram" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">skipgrams</span></code> function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the <code class="docutils literal notranslate"><span class="pre">tf.random.log_uniform_candidate_sampler</span></code> function to sample <code class="docutils literal notranslate"><span class="pre">num_ns</span></code> number of negative samples for a given target word in a window. You can call the function on one skip-grams’s target word and pass the context word as true class to exclude it
from being sampled.</p>
<p>Key point: <em>num_ns</em> (number of negative samples per positive context word) between [5, 20] is <a class="reference external" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">shown to work</a> best for smaller datasets, while <em>num_ns</em> between [2,5] suffices for larger datasets.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Get target and context words for one positive skip-gram.</span>
<span class="n">target_word</span><span class="p">,</span> <span class="n">context_word</span> <span class="o">=</span> <span class="n">positive_skip_grams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Set the number of negative samples per positive context.</span>
<span class="n">num_ns</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">context_class</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">context_word</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">negative_sampling_candidates</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">log_uniform_candidate_sampler</span><span class="p">(</span>
    <span class="n">true_classes</span><span class="o">=</span><span class="n">context_class</span><span class="p">,</span>  <span class="c1"># class that should be sampled as &#39;positive&#39;</span>
    <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># each positive skip-gram has 1 positive context class</span>
    <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_ns</span><span class="p">,</span>  <span class="c1"># number of negative context words to sample</span>
    <span class="n">unique</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># all the negative samples should be unique</span>
    <span class="n">range_max</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>  <span class="c1"># pick index of the samples from [0, vocab_size]</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span>  <span class="c1"># seed for reproducibility</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;negative_sampling&quot;</span>  <span class="c1"># name of this operation</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">negative_sampling_candidates</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">index</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">negative_sampling_candidates</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Construct-one-training-example">
<h3>Construct one training example<a class="headerlink" href="#Construct-one-training-example" title="Enlazar permanentemente con este título">¶</a></h3>
<p>For a given positive <code class="docutils literal notranslate"><span class="pre">(target_word,</span> <span class="pre">context_word)</span></code> skip-gram, you now also have <code class="docutils literal notranslate"><span class="pre">num_ns</span></code> negative sampled context words that do not appear in the window size neighborhood of <code class="docutils literal notranslate"><span class="pre">target_word</span></code>. Batch the <code class="docutils literal notranslate"><span class="pre">1</span></code> positive <code class="docutils literal notranslate"><span class="pre">context_word</span></code> and <code class="docutils literal notranslate"><span class="pre">num_ns</span></code> negative context words into one tensor. This produces a set of positive skip-grams (labelled as <code class="docutils literal notranslate"><span class="pre">1</span></code>) and negative samples (labelled as <code class="docutils literal notranslate"><span class="pre">0</span></code>) for each target word.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Add a dimension so you can use concatenation (on the next step).</span>
<span class="n">negative_sampling_candidates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">negative_sampling_candidates</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Concat positive context word with negative sampled words.</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">context_class</span><span class="p">,</span> <span class="n">negative_sampling_candidates</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Label first context word as 1 (positive) followed by num_ns 0s (negative).</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">num_ns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="c1"># Reshape target to shape (1,) and context and label to (num_ns+1,).</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">target_word</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Take a look at the context and the corresponding labels for the target word from the skip-gram example above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;target_index    : </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;target_word     : </span><span class="si">{</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">target_word</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;context_indices : </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;context_words   : </span><span class="si">{</span><span class="p">[</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">context</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;label           : </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A tuple of <code class="docutils literal notranslate"><span class="pre">(target,</span> <span class="pre">context,</span> <span class="pre">label)</span></code> tensors constitutes one training example for training your skip-gram negative sampling Word2Vec model. Notice that the target is of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code> while the context and label are of shape <code class="docutils literal notranslate"><span class="pre">(1+num_ns,)</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;target  :&quot;</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;context :&quot;</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label   :&quot;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Summary">
<h3>Summary<a class="headerlink" href="#Summary" title="Enlazar permanentemente con este título">¶</a></h3>
<p>This picture summarizes the procedure of generating training example from a sentence.</p>
<p><img alt="word2vec_negative_sampling" src="../../../../_images/word2vec_negative_sampling.png" /></p>
</div>
</div>
<div class="section" id="Compile-all-steps-into-one-function">
<h2>Compile all steps into one function<a class="headerlink" href="#Compile-all-steps-into-one-function" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="section" id="Skip-gram-Sampling-table">
<h3>Skip-gram Sampling table<a class="headerlink" href="#Skip-gram-Sampling-table" title="Enlazar permanentemente con este título">¶</a></h3>
<p>A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as <code class="docutils literal notranslate"><span class="pre">the</span></code>, <code class="docutils literal notranslate"><span class="pre">is</span></code>, <code class="docutils literal notranslate"><span class="pre">on</span></code>) don’t add much useful information for the model to learn from. <a class="reference external" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al.</a> suggest subsampling of frequent words as a helpful practice to improve embedding quality.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.keras.preprocessing.sequence.skipgrams</span></code> function accepts a sampling table argument to encode probabilities of sampling any token. You can use the <code class="docutils literal notranslate"><span class="pre">tf.keras.preprocessing.sequence.make_sampling_table</span></code> to generate a word-frequency rank based probabilistic sampling table and pass it to <code class="docutils literal notranslate"><span class="pre">skipgrams</span></code> function. Take a look at the sampling probabilities for a <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> of 10.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sampling_table</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">make_sampling_table</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sampling_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">sampling_table[i]</span></code> denotes the probability of sampling the i-th most common word in a dataset. The function assumes a <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf’s distribution</a> of the word frequencies for sampling.</p>
<p>Key point: The <code class="docutils literal notranslate"><span class="pre">tf.random.log_uniform_candidate_sampler</span></code> already assumes that the vocabulary frequency follows a log-uniform (Zipf’s) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective.</p>
</div>
<div class="section" id="Generate-training-data">
<h3>Generate training data<a class="headerlink" href="#Generate-training-data" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Generates skip-gram pairs with negative sampling for a list of sequences</span>
<span class="c1"># (int-encoded sentences) based on window size, number of negative samples</span>
<span class="c1"># and vocabulary size.</span>
<span class="k">def</span> <span class="nf">generate_training_data</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">num_ns</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="c1"># Elements of each training example are appended to these lists.</span>
  <span class="n">targets</span><span class="p">,</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

  <span class="c1"># Build the sampling table for vocab_size tokens.</span>
  <span class="n">sampling_table</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">make_sampling_table</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>

  <span class="c1"># Iterate over all sequences (sentences) in dataset.</span>
  <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>

    <span class="c1"># Generate positive skip-gram pairs for a sequence (sentence).</span>
    <span class="n">positive_skip_grams</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">skipgrams</span><span class="p">(</span>
          <span class="n">sequence</span><span class="p">,</span>
          <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
          <span class="n">sampling_table</span><span class="o">=</span><span class="n">sampling_table</span><span class="p">,</span>
          <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span>
          <span class="n">negative_samples</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Iterate over each positive skip-gram pair to produce training examples</span>
    <span class="c1"># with positive context word and negative samples.</span>
    <span class="k">for</span> <span class="n">target_word</span><span class="p">,</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="n">positive_skip_grams</span><span class="p">:</span>
      <span class="n">context_class</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">context_word</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">negative_sampling_candidates</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">log_uniform_candidate_sampler</span><span class="p">(</span>
          <span class="n">true_classes</span><span class="o">=</span><span class="n">context_class</span><span class="p">,</span>
          <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_ns</span><span class="p">,</span>
          <span class="n">unique</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">range_max</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
          <span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s2">&quot;negative_sampling&quot;</span><span class="p">)</span>

      <span class="c1"># Build context and label vectors (for one target word)</span>
      <span class="n">negative_sampling_candidates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
          <span class="n">negative_sampling_candidates</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

      <span class="n">context</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">context_class</span><span class="p">,</span> <span class="n">negative_sampling_candidates</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">num_ns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

      <span class="c1"># Append each element from the training example to global lists.</span>
      <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target_word</span><span class="p">)</span>
      <span class="n">contexts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
      <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">targets</span><span class="p">,</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">labels</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Prepare-training-data-for-Word2Vec">
<h2>Prepare training data for Word2Vec<a class="headerlink" href="#Prepare-training-data-for-Word2Vec" title="Enlazar permanentemente con este título">¶</a></h2>
<p>With an understanding of how to work with one sentence for a skip-gram negative sampling based Word2Vec model, you can proceed to generate training examples from a larger list of sentences!</p>
<div class="section" id="Download-text-corpus">
<h3>Download text corpus<a class="headerlink" href="#Download-text-corpus" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You will use a text file of Shakespeare’s writing for this tutorial. Change the following line to run this code on your own data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">path_to_file</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s1">&#39;shakespeare.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Read text from the file and take a look at the first few lines.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path_to_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
  <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[:</span><span class="mi">20</span><span class="p">]:</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Use the non empty lines to construct a <code class="docutils literal notranslate"><span class="pre">tf.data.TextLineDataset</span></code> object for next steps.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">text_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TextLineDataset</span><span class="p">(</span><span class="n">path_to_file</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">bool</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Vectorize-sentences-from-the-corpus">
<h3>Vectorize sentences from the corpus<a class="headerlink" href="#Vectorize-sentences-from-the-corpus" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> layer to vectorize sentences from the corpus. Learn more about using this layer in this <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/text_classification">Text Classification</a> tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a <code class="docutils literal notranslate"><span class="pre">custom_standardization</span> <span class="pre">function</span></code> that can be used in the TextVectorization layer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Now, create a custom standardization function to lowercase the text and</span>
<span class="c1"># remove punctuation.</span>
<span class="k">def</span> <span class="nf">custom_standardization</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
  <span class="n">lowercase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">regex_replace</span><span class="p">(</span><span class="n">lowercase</span><span class="p">,</span>
                                  <span class="s1">&#39;[</span><span class="si">%s</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>


<span class="c1"># Define the vocabulary size and number of words in a sequence.</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Use the text vectorization layer to normalize, split, and map strings to</span>
<span class="c1"># integers. Set output_sequence_length length to pad all samples to same length.</span>
<span class="n">vectorize_layer</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">standardize</span><span class="o">=</span><span class="n">custom_standardization</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Call <code class="docutils literal notranslate"><span class="pre">adapt</span></code> on the text dataset to create vocabulary.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">vectorize_layer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">text_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1024</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with <code class="docutils literal notranslate"><span class="pre">get_vocabulary()</span></code>. This function returns a list of all vocabulary tokens sorted (descending) by their frequency.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Save the created vocabulary for reference.</span>
<span class="n">inverse_vocab</span> <span class="o">=</span> <span class="n">vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>The vectorize_layer can now be used to generate vectors for each element in the <code class="docutils literal notranslate"><span class="pre">text_ds</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Vectorize the data in text_ds.</span>
<span class="n">text_vector_ds</span> <span class="o">=</span> <span class="n">text_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">vectorize_layer</span><span class="p">)</span><span class="o">.</span><span class="n">unbatch</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Obtain-sequences-from-the-dataset">
<h3>Obtain sequences from the dataset<a class="headerlink" href="#Obtain-sequences-from-the-dataset" title="Enlazar permanentemente con este título">¶</a></h3>
<p>You now have a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> of integer encoded sentences. To prepare the dataset for training a Word2Vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples.</p>
<p>Note: Since the <code class="docutils literal notranslate"><span class="pre">generate_training_data()</span></code> defined earlier uses non-TF python/numpy functions, you could also use a <code class="docutils literal notranslate"><span class="pre">tf.py_function</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.numpy_function</span></code> with <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.map()</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sequences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">text_vector_ds</span><span class="o">.</span><span class="n">as_numpy_iterator</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Take a look at few examples from <code class="docutils literal notranslate"><span class="pre">sequences</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">seq</span><span class="si">}</span><span class="s2"> =&gt; </span><span class="si">{</span><span class="p">[</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generate-training-examples-from-sequences">
<h3>Generate training examples from sequences<a class="headerlink" href="#Generate-training-examples-from-sequences" title="Enlazar permanentemente con este título">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">sequences</span></code> is now a list of int encoded sentences. Just call the <code class="docutils literal notranslate"><span class="pre">generate_training_data()</span></code> function defined earlier to generate training examples for the Word2Vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be same, representing the total number of training examples.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">targets</span><span class="p">,</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_training_data</span><span class="p">(</span>
    <span class="n">sequences</span><span class="o">=</span><span class="n">sequences</span><span class="p">,</span>
    <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_ns</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">contexts</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Configure-the-dataset-for-performance">
<h3>Configure the dataset for performance<a class="headerlink" href="#Configure-the-dataset-for-performance" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To perform efficient batching for the potentially large number of training examples, use the <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> API. After this step, you would have a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> object of <code class="docutils literal notranslate"><span class="pre">(target_word,</span> <span class="pre">context_word),</span> <span class="pre">(label)</span></code> elements to train your Word2Vec model!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(((</span><span class="n">targets</span><span class="p">,</span> <span class="n">contexts</span><span class="p">),</span> <span class="n">labels</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Add <code class="docutils literal notranslate"><span class="pre">cache()</span></code> and <code class="docutils literal notranslate"><span class="pre">prefetch()</span></code> to improve performance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Model-and-Training">
<h2>Model and Training<a class="headerlink" href="#Model-and-Training" title="Enlazar permanentemente con este título">¶</a></h2>
<p>The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset.</p>
<div class="section" id="Subclassed-Word2Vec-Model">
<h3>Subclassed Word2Vec Model<a class="headerlink" href="#Subclassed-Word2Vec-Model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Use the <a class="reference external" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Keras Subclassing API</a> to define your Word2Vec model with the following layers:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target_embedding</span></code>: A <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Embedding</span></code> layer which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are <code class="docutils literal notranslate"><span class="pre">(vocab_size</span> <span class="pre">*</span> <span class="pre">embedding_dim)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context_embedding</span></code>: Another <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Embedding</span></code> layer which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in <code class="docutils literal notranslate"><span class="pre">target_embedding</span></code>, i.e. <code class="docutils literal notranslate"><span class="pre">(vocab_size</span> <span class="pre">*</span> <span class="pre">embedding_dim)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dots</span></code>: A <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dot</span></code> layer that computes the dot product of target and context embeddings from a training pair.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flatten</span></code>: A <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Flatten</span></code> layer to flatten the results of <code class="docutils literal notranslate"><span class="pre">dots</span></code> layer into logits.</p></li>
</ul>
<p>With the subclassed model, you can define the <code class="docutils literal notranslate"><span class="pre">call()</span></code> function that accepts <code class="docutils literal notranslate"><span class="pre">(target,</span> <span class="pre">context)</span></code> pairs which can then be passed into their corresponding embedding layer. Reshape the <code class="docutils literal notranslate"><span class="pre">context_embedding</span></code> to perform a dot product with <code class="docutils literal notranslate"><span class="pre">target_embedding</span></code> and return the flattened result.</p>
<p>Key point: The <code class="docutils literal notranslate"><span class="pre">target_embedding</span></code> and <code class="docutils literal notranslate"><span class="pre">context_embedding</span></code> layers can be shared as well. You could also use a concatenation of both embeddings as the final Word2Vec embedding.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">target_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span>
                                      <span class="n">embedding_dim</span><span class="p">,</span>
                                      <span class="n">input_length</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">name</span><span class="o">=</span><span class="s2">&quot;w2v_embedding&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span>
                                       <span class="n">embedding_dim</span><span class="p">,</span>
                                       <span class="n">input_length</span><span class="o">=</span><span class="n">num_ns</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dots</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">):</span>
    <span class="n">target</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="n">pair</span>
    <span class="n">word_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_embedding</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">context_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_embedding</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
    <span class="n">dots</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dots</span><span class="p">([</span><span class="n">context_emb</span><span class="p">,</span> <span class="n">word_emb</span><span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">dots</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-loss-function-and-compile-model">
<h3>Define loss function and compile model<a class="headerlink" href="#Define-loss-function-and-compile-model" title="Enlazar permanentemente con este título">¶</a></h3>
<p>For simplicity, you can use <code class="docutils literal notranslate"><span class="pre">tf.keras.losses.CategoricalCrossEntropy</span></code> as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">custom_loss</span><span class="p">(</span><span class="n">x_logit</span><span class="p">,</span> <span class="n">y_true</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">x_logit</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y_true</span><span class="p">)</span>
</pre></div>
</div>
<p>It’s time to build your model! Instantiate your Word2Vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code> optimizer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">word2vec</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
<span class="n">word2vec</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                 <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Also define a callback to log training statistics for tensorboard.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="s2">&quot;logs&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Train the model with <code class="docutils literal notranslate"><span class="pre">dataset</span></code> prepared above for some number of epochs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">word2vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>Tensorboard now shows the Word2Vec model’s accuracy and loss.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#docs_infra: no_execute</span>
<span class="o">%</span><span class="k">tensorboard</span> --logdir logs
</pre></div>
</div>
</div>
<!-- <img class="tfo-display-only-on-site" src="images/word2vec_tensorboard.png"/> --></div>
</div>
<div class="section" id="Embedding-lookup-and-analysis">
<h2>Embedding lookup and analysis<a class="headerlink" href="#Embedding-lookup-and-analysis" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Obtain the weights from the model using <code class="docutils literal notranslate"><span class="pre">get_layer()</span></code> and <code class="docutils literal notranslate"><span class="pre">get_weights()</span></code>. The <code class="docutils literal notranslate"><span class="pre">get_vocabulary()</span></code> function provides the vocabulary to build a metadata file with one token per line.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">&#39;w2v_embedding&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">vectorize_layer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Create and save the vectors and metadata file.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">out_v</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;vectors.tsv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">out_m</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;metadata.tsv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">continue</span>  <span class="c1"># skip 0, it&#39;s padding.</span>
  <span class="n">vec</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
  <span class="n">out_v</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vec</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="n">out_m</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">word</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">out_v</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">out_m</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Download the <code class="docutils literal notranslate"><span class="pre">vectors.tsv</span></code> and <code class="docutils literal notranslate"><span class="pre">metadata.tsv</span></code> to analyze the obtained embeddings in the <a class="reference external" href="https://projector.tensorflow.org/">Embedding Projector</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">try</span><span class="p">:</span>
  <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
  <span class="n">files</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;vectors.tsv&#39;</span><span class="p">)</span>
  <span class="n">files</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;metadata.tsv&#39;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
  <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Next-steps">
<h2>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título">¶</a></h2>
<p>This tutorial has shown you how to implement a skip-gram Word2Vec model with negative sampling from scratch and visualize the obtained word embeddings.</p>
<ul class="simple">
<li><p>To learn more about word vectors and their mathematical representations, refer to these <a class="reference external" href="https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf">notes</a>.</p></li>
<li><p>To learn more about advanced text processing, read the <a class="reference external" href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding</a> tutorial.</p></li>
<li><p>If you’re interested in pre-trained embedding models, you may also be interested in <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/cord_19_embeddings_keras">Exploring the TF-Hub CORD-19 Swivel Embeddings</a>, or the <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder">Multilingual Universal Sentence Encoder</a></p></li>
<li><p>You may also like to train the model on a new dataset (there are many available in <a class="reference external" href="https://www.tensorflow.org/datasets">TensorFlow Datasets</a>).</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>